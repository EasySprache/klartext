{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9713213",
      "metadata": {},
      "source": [
        "# Internal Feedback Loop for Prompt & Model Optimization\n",
        "\n",
        "## ðŸ“‹ Purpose\n",
        "\n",
        "**Goal**: Build an internal evaluation pipeline that scores model outputs and provides insights to improve prompts, guardrails, and model selection.\n",
        "\n",
        "**This is NOT user-facing** â€” it's a development tool for iterating on our simplification system.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”„ The Feedback Loop\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    INTERNAL FEEDBACK LOOP                    â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                              â”‚\n",
        "â”‚   1. RUN BATCH          2. SCORE OUTPUTS      3. ANALYZE    â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
        "â”‚   Source texts    â†’     - ARI score      â†’    - Which       â”‚\n",
        "â”‚   + Current prompts     - Sentence len        prompts work? â”‚\n",
        "â”‚   + Selected model      - Meaning sim         - Where do    â”‚\n",
        "â”‚                                               models fail?  â”‚\n",
        "â”‚                              â†“                              â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚   4. ADJUST             â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚\n",
        "â”‚   - Tweak prompts                                           â”‚\n",
        "â”‚   - Update guardrails                                       â”‚\n",
        "â”‚   - Try different model                                     â”‚\n",
        "â”‚   - Repeat                                                  â”‚\n",
        "â”‚                                                              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Sprint Plan (10 days, 1 person)\n",
        "\n",
        "### Sprint 1: Scoring Pipeline (Days 1-5)\n",
        "| Day | Task | Output |\n",
        "|-----|------|--------|\n",
        "| 1-2 | Batch scoring script | Score all test texts |\n",
        "| 2-3 | Results logging | CSV/JSONL with scores per text |\n",
        "| 4-5 | Analysis notebook | Aggregate stats, identify patterns |\n",
        "\n",
        "### Sprint 2: Iteration Tools (Days 6-10)\n",
        "| Day | Task | Output |\n",
        "|-----|------|--------|\n",
        "| 6-7 | Prompt comparison | Compare prompt variants |\n",
        "| 8-9 | Model comparison | Compare model outputs |\n",
        "| 9-10 | Guardrail tuning | Adjust thresholds based on data |\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Acceptance Criteria\n",
        "\n",
        "1. âœ… Can batch-score all test texts with current prompt/model\n",
        "2. âœ… Results logged to file for analysis\n",
        "3. âœ… Can compare metrics across prompt variants\n",
        "4. âœ… Insights feed back into prompt/guardrail improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3becf18e",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ“š Implementation Guide\n",
        "\n",
        "## ðŸ”§ Prerequisites\n",
        "\n",
        "- Python 3.10+\n",
        "- `pip install sentence-transformers scikit-learn pandas`\n",
        "- Test texts in `data/`\n",
        "- Model API access (Groq/OpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1fb8e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Install required packages if needed\n",
        "# Uncomment and run if dependencies are missing\n",
        "\n",
        "# !pip install sentence-transformers spacy scikit-learn pyyaml\n",
        "# !python -m spacy download de_core_news_sm\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713b6a30",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸš€ Sprint 1: Scoring Pipeline (Days 1-5)\n",
        "\n",
        "## Task 1.1: Batch Scoring Setup (Days 1-2)\n",
        "\n",
        "**Goal**: Score model outputs in batch, log results for analysis.\n",
        "\n",
        "### What We're Building\n",
        "\n",
        "```\n",
        "test_texts.csv â†’ Run through model â†’ Score each output â†’ results.csv\n",
        "```\n",
        "\n",
        "### Metrics to Track\n",
        "| Metric | What it tells us |\n",
        "|--------|------------------|\n",
        "| `ari_score` | Is output readable? (target: â‰¤8) |\n",
        "| `avg_sentence_len` | Are sentences short enough? (target: â‰¤15) |\n",
        "| `meaning_cosine` | Did we preserve meaning? (target: â‰¥0.70) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a1f05e9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/simonvoegely/Desktop/easysprache/klartext/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test scores: {'ari_score': 4.1, 'avg_sentence_len': 4.0, 'meaning_cosine': 0.792}\n"
          ]
        }
      ],
      "source": [
        "# Task 1.1: Batch scoring function\n",
        "# This scores a single text and returns metrics as a dict\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def compute_ari(text: str) -> float:\n",
        "    \"\"\"Compute Automated Readability Index (works for EN + DE).\"\"\"\n",
        "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    if not sentences or not words:\n",
        "        return 0.0\n",
        "    chars = sum(len(w) for w in words)\n",
        "    return 4.71 * (chars / len(words)) + 0.5 * (len(words) / len(sentences)) - 21.43\n",
        "\n",
        "def compute_avg_sentence_len(text: str) -> float:\n",
        "    \"\"\"Compute average words per sentence.\"\"\"\n",
        "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    if not sentences:\n",
        "        return 0.0\n",
        "    return len(words) / len(sentences)\n",
        "\n",
        "def compute_meaning_similarity(source: str, output: str) -> float:\n",
        "    \"\"\"Compute cosine similarity using sentence-transformers.\"\"\"\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        embeddings = model.encode([source, output])\n",
        "        sim = np.dot(embeddings[0], embeddings[1]) / (\n",
        "            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])\n",
        "        )\n",
        "        return float(sim)\n",
        "    except ImportError:\n",
        "        print(\"Warning: sentence-transformers not installed\")\n",
        "        return 0.0\n",
        "\n",
        "def score_output(source: str, output: str) -> dict:\n",
        "    \"\"\"Score a single simplification. Returns dict of metrics.\"\"\"\n",
        "    return {\n",
        "        \"ari_score\": round(compute_ari(output), 1),\n",
        "        \"avg_sentence_len\": round(compute_avg_sentence_len(output), 1),\n",
        "        \"meaning_cosine\": round(compute_meaning_similarity(source, output), 3),\n",
        "    }\n",
        "\n",
        "# Test\n",
        "test_source = \"Die meteorologischen Bedingungen sind ungÃ¼nstig.\"\n",
        "test_output = \"Das Wetter ist schlecht.\"\n",
        "print(\"Test scores:\", score_output(test_source, test_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a5729a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 1.2: Results Logging (Days 2-3)\n",
        "\n",
        "**Goal**: Log all scores to CSV for later analysis.\n",
        "\n",
        "### Output Format\n",
        "```\n",
        "source_id, prompt_version, model, ari_score, avg_sentence_len, meaning_cosine, passed\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8fb4c375",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source_id prompt_version        model  ari_score  avg_sentence_len  meaning_cosine  passed                  timestamp\n",
            " test_001           v1.0 llama-3.1-8b       -1.1               3.0           0.750    True 2026-01-12T08:35:38.740568\n",
            " test_002           v1.0 llama-3.1-8b       -0.6               4.0           0.729    True 2026-01-12T08:35:41.948790\n"
          ]
        }
      ],
      "source": [
        "# Task 1.2: Batch evaluation and logging\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Guardrail thresholds\n",
        "THRESHOLDS = {\n",
        "    \"ari_score\": {\"max\": 8, \"direction\": \"lower\"},\n",
        "    \"avg_sentence_len\": {\"max\": 15, \"direction\": \"lower\"},\n",
        "    \"meaning_cosine\": {\"min\": 0.70, \"direction\": \"higher\"},\n",
        "}\n",
        "\n",
        "def check_passed(scores: dict) -> bool:\n",
        "    \"\"\"Check if all metrics pass thresholds.\"\"\"\n",
        "    return (\n",
        "        scores[\"ari_score\"] <= THRESHOLDS[\"ari_score\"][\"max\"] and\n",
        "        scores[\"avg_sentence_len\"] <= THRESHOLDS[\"avg_sentence_len\"][\"max\"] and\n",
        "        scores[\"meaning_cosine\"] >= THRESHOLDS[\"meaning_cosine\"][\"min\"]\n",
        "    )\n",
        "\n",
        "def run_batch_evaluation(test_data: list, prompt_version: str, model_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run batch evaluation on test data.\n",
        "    \n",
        "    Args:\n",
        "        test_data: List of dicts with 'id', 'source', 'output' keys\n",
        "        prompt_version: e.g., \"v1.2\"\n",
        "        model_name: e.g., \"llama-3.1-8b-instant\"\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with all results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for item in test_data:\n",
        "        scores = score_output(item[\"source\"], item[\"output\"])\n",
        "        \n",
        "        results.append({\n",
        "            \"source_id\": item[\"id\"],\n",
        "            \"prompt_version\": prompt_version,\n",
        "            \"model\": model_name,\n",
        "            \"ari_score\": scores[\"ari_score\"],\n",
        "            \"avg_sentence_len\": scores[\"avg_sentence_len\"],\n",
        "            \"meaning_cosine\": scores[\"meaning_cosine\"],\n",
        "            \"passed\": check_passed(scores),\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage\n",
        "test_data = [\n",
        "    {\"id\": \"test_001\", \"source\": \"Die Situation ist kompliziert.\", \"output\": \"Das ist schwer.\"},\n",
        "    {\"id\": \"test_002\", \"source\": \"Bitte beachten Sie die Hinweise.\", \"output\": \"Lesen Sie die Tipps.\"},\n",
        "]\n",
        "\n",
        "df = run_batch_evaluation(test_data, prompt_version=\"v1.0\", model_name=\"llama-3.1-8b\")\n",
        "print(df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd3842a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 1.3: Analysis Functions (Days 4-5)\n",
        "\n",
        "**Goal**: Aggregate results and identify patterns that inform prompt/guardrail changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "90b5d0a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Evaluation Report\n",
            "\n",
            "**Pass Rate**: 100%\n",
            "\n",
            "## Failure Analysis\n",
            "\n",
            "\n",
            "## Recommendations\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 1.3: Analysis functions\n",
        "\n",
        "def analyze_results(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"\n",
        "    Analyze batch results and return insights.\n",
        "    \n",
        "    Returns dict with:\n",
        "    - pass_rate: % of texts that pass all thresholds\n",
        "    - metric_means: average for each metric\n",
        "    - failures: which metrics fail most often\n",
        "    \"\"\"\n",
        "    analysis = {\n",
        "        \"total_texts\": len(df),\n",
        "        \"pass_rate\": df[\"passed\"].mean() * 100,\n",
        "        \"metric_means\": {\n",
        "            \"ari_score\": df[\"ari_score\"].mean(),\n",
        "            \"avg_sentence_len\": df[\"avg_sentence_len\"].mean(),\n",
        "            \"meaning_cosine\": df[\"meaning_cosine\"].mean(),\n",
        "        },\n",
        "        \"failure_rates\": {\n",
        "            \"ari_too_high\": (df[\"ari_score\"] > 8).mean() * 100,\n",
        "            \"sentences_too_long\": (df[\"avg_sentence_len\"] > 15).mean() * 100,\n",
        "            \"meaning_lost\": (df[\"meaning_cosine\"] < 0.70).mean() * 100,\n",
        "        }\n",
        "    }\n",
        "    return analysis\n",
        "\n",
        "def generate_feedback_report(analysis: dict) -> str:\n",
        "    \"\"\"Generate actionable feedback for prompt/guardrail tuning.\"\"\"\n",
        "    lines = [\"# Evaluation Report\\n\"]\n",
        "    \n",
        "    lines.append(f\"**Pass Rate**: {analysis['pass_rate']:.0f}%\\n\")\n",
        "    \n",
        "    lines.append(\"## Failure Analysis\\n\")\n",
        "    for issue, rate in analysis[\"failure_rates\"].items():\n",
        "        if rate > 20:  # Flag issues affecting >20% of texts\n",
        "            lines.append(f\"âš ï¸ **{issue}**: {rate:.0f}% of outputs\")\n",
        "    \n",
        "    lines.append(\"\\n## Recommendations\\n\")\n",
        "    \n",
        "    if analysis[\"failure_rates\"][\"ari_too_high\"] > 20:\n",
        "        lines.append(\"- **Prompt**: Add instruction to use simpler words\")\n",
        "    \n",
        "    if analysis[\"failure_rates\"][\"sentences_too_long\"] > 20:\n",
        "        lines.append(\"- **Prompt**: Add instruction to keep sentences under 15 words\")\n",
        "    \n",
        "    if analysis[\"failure_rates\"][\"meaning_lost\"] > 20:\n",
        "        lines.append(\"- **Prompt**: Add instruction to preserve all key information\")\n",
        "    \n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# Test with sample data\n",
        "analysis = analyze_results(df)\n",
        "print(generate_feedback_report(analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012faf85",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸš€ Sprint 2: Comparison & Iteration Tools (Days 6-10)\n",
        "\n",
        "## Task 2.1: Prompt Comparison (Days 6-7)\n",
        "\n",
        "**Goal**: Compare different prompt versions to see which performs better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2f88c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2.1: Compare prompt versions\n",
        "\n",
        "def compare_prompts(results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compare metrics across different prompt versions.\n",
        "    \n",
        "    Returns DataFrame grouped by prompt_version with aggregated metrics.\n",
        "    \"\"\"\n",
        "    comparison = results_df.groupby(\"prompt_version\").agg({\n",
        "        \"ari_score\": \"mean\",\n",
        "        \"avg_sentence_len\": \"mean\", \n",
        "        \"meaning_cosine\": \"mean\",\n",
        "        \"passed\": \"mean\"\n",
        "    }).round(2)\n",
        "    \n",
        "    comparison.columns = [\"Avg ARI\", \"Avg Sent Len\", \"Avg Meaning\", \"Pass Rate\"]\n",
        "    comparison[\"Pass Rate\"] = (comparison[\"Pass Rate\"] * 100).astype(int).astype(str) + \"%\"\n",
        "    \n",
        "    return comparison\n",
        "\n",
        "# Example: Results from two prompt versions\n",
        "sample_results = pd.DataFrame([\n",
        "    {\"prompt_version\": \"v1.0\", \"model\": \"llama\", \"ari_score\": 9.2, \"avg_sentence_len\": 16, \"meaning_cosine\": 0.75, \"passed\": False},\n",
        "    {\"prompt_version\": \"v1.0\", \"model\": \"llama\", \"ari_score\": 8.5, \"avg_sentence_len\": 14, \"meaning_cosine\": 0.72, \"passed\": False},\n",
        "    {\"prompt_version\": \"v1.1\", \"model\": \"llama\", \"ari_score\": 6.8, \"avg_sentence_len\": 12, \"meaning_cosine\": 0.78, \"passed\": True},\n",
        "    {\"prompt_version\": \"v1.1\", \"model\": \"llama\", \"ari_score\": 7.2, \"avg_sentence_len\": 11, \"meaning_cosine\": 0.81, \"passed\": True},\n",
        "])\n",
        "\n",
        "print(\"PROMPT COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "print(compare_prompts(sample_results))\n",
        "print(\"\\nâ†’ v1.1 performs better: lower ARI, higher pass rate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "223fafd4",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 2.2: Model Comparison (Days 8-9)\n",
        "\n",
        "**Goal**: Compare how different models perform on the same test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64c69e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2.2: Compare models\n",
        "\n",
        "def compare_models(results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Compare metrics across different models.\"\"\"\n",
        "    comparison = results_df.groupby(\"model\").agg({\n",
        "        \"ari_score\": \"mean\",\n",
        "        \"avg_sentence_len\": \"mean\",\n",
        "        \"meaning_cosine\": \"mean\",\n",
        "        \"passed\": \"mean\"\n",
        "    }).round(2)\n",
        "    \n",
        "    comparison.columns = [\"Avg ARI\", \"Avg Sent Len\", \"Avg Meaning\", \"Pass Rate\"]\n",
        "    comparison[\"Pass Rate\"] = (comparison[\"Pass Rate\"] * 100).astype(int).astype(str) + \"%\"\n",
        "    \n",
        "    return comparison.sort_values(\"Pass Rate\", ascending=False)\n",
        "\n",
        "# Example: Results from different models\n",
        "model_results = pd.DataFrame([\n",
        "    {\"model\": \"llama-3.1-8b\", \"ari_score\": 7.5, \"avg_sentence_len\": 12, \"meaning_cosine\": 0.78, \"passed\": True},\n",
        "    {\"model\": \"llama-3.1-8b\", \"ari_score\": 8.1, \"avg_sentence_len\": 14, \"meaning_cosine\": 0.75, \"passed\": True},\n",
        "    {\"model\": \"gpt-4o-mini\", \"ari_score\": 6.2, \"avg_sentence_len\": 10, \"meaning_cosine\": 0.82, \"passed\": True},\n",
        "    {\"model\": \"gpt-4o-mini\", \"ari_score\": 5.8, \"avg_sentence_len\": 9, \"meaning_cosine\": 0.85, \"passed\": True},\n",
        "    {\"model\": \"mixtral-8x7b\", \"ari_score\": 9.5, \"avg_sentence_len\": 18, \"meaning_cosine\": 0.71, \"passed\": False},\n",
        "    {\"model\": \"mixtral-8x7b\", \"ari_score\": 10.2, \"avg_sentence_len\": 20, \"meaning_cosine\": 0.68, \"passed\": False},\n",
        "])\n",
        "\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "print(compare_models(model_results))\n",
        "print(\"\\nâ†’ Use this to select best model for production\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a062e2d",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 2.3: Guardrail Tuning (Days 9-10)\n",
        "\n",
        "**Goal**: Adjust thresholds based on what the data shows is realistic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028b0e9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2.3: Guardrail tuning based on results\n",
        "\n",
        "def suggest_guardrail_updates(results_df: pd.DataFrame) -> dict:\n",
        "    \"\"\"\n",
        "    Suggest guardrail threshold adjustments based on actual results.\n",
        "    \n",
        "    If pass rate is too low, thresholds may be too strict.\n",
        "    If pass rate is 100%, thresholds may be too loose.\n",
        "    \"\"\"\n",
        "    suggestions = {}\n",
        "    \n",
        "    pass_rate = results_df[\"passed\"].mean()\n",
        "    \n",
        "    if pass_rate < 0.5:\n",
        "        # Too few passing - analyze which metric fails most\n",
        "        ari_fail_rate = (results_df[\"ari_score\"] > 8).mean()\n",
        "        sent_fail_rate = (results_df[\"avg_sentence_len\"] > 15).mean()\n",
        "        meaning_fail_rate = (results_df[\"meaning_cosine\"] < 0.70).mean()\n",
        "        \n",
        "        if ari_fail_rate > 0.3:\n",
        "            new_threshold = results_df[\"ari_score\"].quantile(0.8)\n",
        "            suggestions[\"ari_score\"] = f\"Consider relaxing to {new_threshold:.0f} (currently 8)\"\n",
        "        \n",
        "        if sent_fail_rate > 0.3:\n",
        "            new_threshold = results_df[\"avg_sentence_len\"].quantile(0.8)\n",
        "            suggestions[\"avg_sentence_len\"] = f\"Consider relaxing to {new_threshold:.0f} (currently 15)\"\n",
        "            \n",
        "        if meaning_fail_rate > 0.3:\n",
        "            new_threshold = results_df[\"meaning_cosine\"].quantile(0.2)\n",
        "            suggestions[\"meaning_cosine\"] = f\"Consider lowering to {new_threshold:.2f} (currently 0.70)\"\n",
        "    \n",
        "    elif pass_rate == 1.0:\n",
        "        suggestions[\"general\"] = \"100% pass rate - consider tightening thresholds\"\n",
        "    \n",
        "    return suggestions\n",
        "\n",
        "# Example\n",
        "print(\"GUARDRAIL TUNING SUGGESTIONS\")\n",
        "print(\"=\"*50)\n",
        "suggestions = suggest_guardrail_updates(model_results)\n",
        "if suggestions:\n",
        "    for metric, suggestion in suggestions.items():\n",
        "        print(f\"â€¢ {metric}: {suggestion}\")\n",
        "else:\n",
        "    print(\"âœ… Current thresholds look good (pass rate is reasonable)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cae9d8a2",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ðŸ“‹ Complete Workflow Example\n",
        "\n",
        "## Running a Full Iteration Cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed7cddc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete workflow: One iteration cycle\n",
        "\n",
        "def run_iteration(test_texts: list, prompt_version: str, model: str):\n",
        "    \"\"\"\n",
        "    Run one complete iteration of the feedback loop.\n",
        "    \n",
        "    1. Score all outputs\n",
        "    2. Analyze results\n",
        "    3. Generate recommendations\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ITERATION: {prompt_version} + {model}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Step 1: Score\n",
        "    results = run_batch_evaluation(test_texts, prompt_version, model)\n",
        "    \n",
        "    # Step 2: Analyze\n",
        "    analysis = analyze_results(results)\n",
        "    \n",
        "    print(f\"\\nðŸ“Š RESULTS:\")\n",
        "    print(f\"   Pass Rate: {analysis['pass_rate']:.0f}%\")\n",
        "    print(f\"   Avg ARI: {analysis['metric_means']['ari_score']:.1f}\")\n",
        "    print(f\"   Avg Sent Len: {analysis['metric_means']['avg_sentence_len']:.1f}\")\n",
        "    print(f\"   Avg Meaning: {analysis['metric_means']['meaning_cosine']:.2f}\")\n",
        "    \n",
        "    # Step 3: Recommendations\n",
        "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
        "    if analysis['failure_rates']['ari_too_high'] > 20:\n",
        "        print(\"   â†’ Prompt: Emphasize using simpler, shorter words\")\n",
        "    if analysis['failure_rates']['sentences_too_long'] > 20:\n",
        "        print(\"   â†’ Prompt: Add rule 'max 15 words per sentence'\")\n",
        "    if analysis['failure_rates']['meaning_lost'] > 20:\n",
        "        print(\"   â†’ Prompt: Add 'preserve all key information'\")\n",
        "    if analysis['pass_rate'] >= 80:\n",
        "        print(\"   â†’ âœ… This configuration is working well!\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example iteration\n",
        "test_texts = [\n",
        "    {\"id\": \"001\", \"source\": \"Die Anmeldung muss rechtzeitig erfolgen.\", \n",
        "     \"output\": \"Melden Sie sich frÃ¼h an.\"},\n",
        "    {\"id\": \"002\", \"source\": \"Beachten Sie die Ã–ffnungszeiten.\", \n",
        "     \"output\": \"Wir sind zu bestimmten Zeiten offen.\"},\n",
        "]\n",
        "\n",
        "run_iteration(test_texts, \"v1.0\", \"llama-3.1-8b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d33651e",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Saving Results for Tracking Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "585d17a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to CSV for tracking across iterations\n",
        "import os\n",
        "\n",
        "def save_iteration_results(results_df: pd.DataFrame, output_dir: str = \"../outputs\"):\n",
        "    \"\"\"Append results to tracking CSV.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = os.path.join(output_dir, \"iteration_results.csv\")\n",
        "    \n",
        "    if os.path.exists(output_path):\n",
        "        # Append to existing\n",
        "        existing = pd.read_csv(output_path)\n",
        "        combined = pd.concat([existing, results_df], ignore_index=True)\n",
        "        combined.to_csv(output_path, index=False)\n",
        "    else:\n",
        "        results_df.to_csv(output_path, index=False)\n",
        "    \n",
        "    print(f\"âœ… Results saved to {output_path}\")\n",
        "\n",
        "# Example\n",
        "# save_iteration_results(results_df)\n",
        "print(\"Use save_iteration_results(df) to log each iteration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70e25e0",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# âœ… Done Checklist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bdc9e0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "checklist = \"\"\"\n",
        "SPRINT 1: Scoring Pipeline\n",
        "[ ] score_output() function works\n",
        "[ ] run_batch_evaluation() scores all test texts\n",
        "[ ] analyze_results() identifies patterns\n",
        "[ ] generate_feedback_report() produces actionable recommendations\n",
        "\n",
        "SPRINT 2: Comparison & Iteration\n",
        "[ ] compare_prompts() shows which prompt version is better\n",
        "[ ] compare_models() shows which model performs best\n",
        "[ ] suggest_guardrail_updates() recommends threshold changes\n",
        "[ ] save_iteration_results() logs progress over time\n",
        "\n",
        "THE LOOP IS WORKING WHEN:\n",
        "â†’ You can run a batch, see what's failing, adjust prompts/guardrails, and re-run\n",
        "â†’ Each iteration shows measurable improvement (or confirms current setup is optimal)\n",
        "\"\"\"\n",
        "print(checklist)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37bf1eb",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ”„ How to Use This Feedback Loop\n",
        "\n",
        "1. **Prepare test set**: Collect source texts to simplify\n",
        "2. **Run model**: Generate outputs with current prompt/model\n",
        "3. **Score batch**: Use `run_batch_evaluation()`\n",
        "4. **Analyze**: Use `analyze_results()` and `generate_feedback_report()`\n",
        "5. **Adjust**: Modify prompts or guardrails based on recommendations\n",
        "6. **Repeat**: Run again and compare with `compare_prompts()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b6d5ee0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Full iteration cycle with logging\n",
        "\n",
        "print(\"EXAMPLE: Running feedback loop iteration\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "# 1. Prepare test data (source + model outputs)\n",
        "test_data = load_test_outputs(\"outputs/model_run_v1.csv\")\n",
        "\n",
        "# 2. Score batch\n",
        "results = run_batch_evaluation(test_data, \"prompt_v1.2\", \"llama-3.1-8b\")\n",
        "\n",
        "# 3. Analyze\n",
        "analysis = analyze_results(results)\n",
        "print(generate_feedback_report(analysis))\n",
        "\n",
        "# 4. If pass rate < target, adjust prompts and re-run\n",
        "# 5. Compare: compare_prompts(all_results_df)\n",
        "\n",
        "# 6. Save for tracking\n",
        "save_iteration_results(results)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e63bcb6",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Metrics Explained\n",
        "\n",
        "| Metric | What it measures | Target | If failing... |\n",
        "|--------|------------------|--------|---------------|\n",
        "| `ari_score` | Text complexity | â‰¤8 | Use simpler words |\n",
        "| `avg_sentence_len` | Sentence length | â‰¤15 | Split long sentences |\n",
        "| `meaning_cosine` | Semantic similarity | â‰¥0.70 | Preserve key info |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0700066",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thresholds - adjust based on your analysis\n",
        "THRESHOLDS = {\n",
        "    \"ari_score\": 8,          # Max ARI (grade level)\n",
        "    \"avg_sentence_len\": 15,   # Max words per sentence\n",
        "    \"meaning_cosine\": 0.70,   # Min semantic similarity\n",
        "}\n",
        "\n",
        "print(\"Current thresholds:\")\n",
        "for metric, value in THRESHOLDS.items():\n",
        "    direction = \"â‰¥\" if metric == \"meaning_cosine\" else \"â‰¤\"\n",
        "    print(f\"  {metric}: {direction} {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ae9896",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: The Internal Feedback Loop\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   RUN      â”‚ â†’  â”‚   SCORE    â”‚ â†’  â”‚  ANALYZE   â”‚ â†’  â”‚   ADJUST   â”‚\n",
        "â”‚  Model +   â”‚    â”‚  Outputs   â”‚    â”‚  Results   â”‚    â”‚  Prompts/  â”‚\n",
        "â”‚  Prompt    â”‚    â”‚            â”‚    â”‚            â”‚    â”‚ Guardrails â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                                           â”‚\n",
        "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "      â”‚\n",
        "      â–¼\n",
        "   REPEAT\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f11090",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸŽ¯ This feedback loop helps you iteratively improve:\")\n",
        "print(\"   â€¢ Prompts: What instructions make the model produce better output?\")\n",
        "print(\"   â€¢ Models: Which model works best for Easy Language?\")\n",
        "print(\"   â€¢ Guardrails: What thresholds are realistic for your use case?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5cc89f2",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "*This is an internal development tool. The insights from this loop inform changes to prompts and guardrailsâ€”changes that you implement manually based on the data.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plain English Evaluation Notebook (Multi-Model)\n",
        "\n",
        "This notebook provides a comprehensive evaluation framework for testing multiple AI models on **Plain English** text simplification tasks (the English counterpart to **Easy Language / Leichte Sprache**).\n",
        "\n",
        "> **Note:** This is the English-first iteration of the Plain English / Easy Language evaluation framework. German support will be added once the English pipeline is stable.\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "The framework evaluates outputs using multiple dimensions:\n",
        "\n",
        "### 1. Classic Readability & Syntax Proxies\n",
        "- Sentence length statistics (avg, % > 20 words)\n",
        "- Word length statistics (avg, % > 6 chars)\n",
        "- LIX readability index\n",
        "\n",
        "### 2. Lexical / Cognitive Load Proxies\n",
        "- Long-word rate (words > 6 characters)\n",
        "\n",
        "### 3. Entity Load Metrics\n",
        "- Unique entities per sentence\n",
        "- Total entity mentions\n",
        "\n",
        "### 4. Semantic Focus (Topic Distribution)\n",
        "- **n_topics**: Topics above threshold τ\n",
        "- **semantic_richness**: Σ p_i × rank_i\n",
        "- **semantic_clarity**: (1/n) × Σ(max(p) - p_i)\n",
        "- **semantic_noise**: Kurtosis-like measure\n",
        "- **n_eff**: Effective number of topics = exp(entropy)\n",
        "\n",
        "### 5. Meaning Preservation\n",
        "- Embedding cosine similarity (sentence-transformers)\n",
        "- TF-IDF cosine similarity (fallback)\n",
        "\n",
        "## Design Principles\n",
        "\n",
        "This notebook is **model-agnostic**:\n",
        "- Add any model adapter that implements `generate(prompt) -> str`\n",
        "- All models are scored with the same metrics and guardrails\n",
        "- Guardrail thresholds are derived from a calibration corpus (data-backed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Dependencies and Setup\n",
        "\n",
        "### Required packages\n",
        "- numpy, pandas, scipy, scikit-learn, tqdm\n",
        "\n",
        "### Optional (recommended)\n",
        "- **spaCy + en_core_web_sm**: Better English NER\n",
        "- **sentence-transformers**: Better meaning similarity\n",
        "\n",
        "```bash\n",
        "# Install dependencies\n",
        "pip install -U numpy pandas scipy scikit-learn tqdm spacy sentence-transformers\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Optional: spaCy for entity extraction\n",
        "try:\n",
        "    import spacy\n",
        "    _SPACY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    _SPACY_AVAILABLE = False\n",
        "    print(\"[INFO] spaCy not available. Entity extraction will use heuristic fallback.\")\n",
        "\n",
        "# Optional: sentence-transformers for meaning preservation\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    _ST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    _ST_AVAILABLE = False\n",
        "    print(\"[INFO] sentence-transformers not available. Meaning similarity will use TF-IDF fallback.\")\n",
        "\n",
        "# Reproducibility\n",
        "RNG = np.random.default_rng(42)\n",
        "\n",
        "print(f\"spaCy available: {_SPACY_AVAILABLE}\")\n",
        "print(f\"sentence-transformers available: {_ST_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "\n",
        "Define:\n",
        "- Data paths (benchmark + calibration corpora)\n",
        "- Topic model parameters\n",
        "- Guardrail derivation parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Paths:\n",
        "    \"\"\"File and directory paths for the evaluation pipeline.\"\"\"\n",
        "    benchmark_jsonl: str = \"../data/benchmark.jsonl\"  # id, source_text, (optional) reference_easy_text\n",
        "    hard_dir: str = \"../data/hard\"                    # Bureaucratic / academic originals\n",
        "    easy_dir: str = \"../data/easy\"                    # Trusted easy language samples\n",
        "    samples_dir: str = \"../data/samples\"              # Sample texts (fallback)\n",
        "    outputs_dir: str = \"../outputs/runs\"              # Model generations\n",
        "    scored_dir: str = \"../outputs/scored\"             # Scored outputs\n",
        "    reports_dir: str = \"../outputs/reports\"           # Summary reports\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TopicConfig:\n",
        "    \"\"\"Configuration for the topic model (TF-IDF + NMF).\"\"\"\n",
        "    n_topics: int = 12\n",
        "    min_topic_prob: float = 0.05   # τ: topic is \"present\" if p_i >= τ\n",
        "    max_features: int = 20000\n",
        "    ngram_range: Tuple[int, int] = (1, 2)\n",
        "    nmf_max_iter: int = 500        # NMF tends to be more stable than LDA on short corpora\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GuardrailConfig:\n",
        "    \"\"\"Configuration for deriving guardrail thresholds from the EASY corpus.\n",
        "    \n",
        "    - For \"lower is better\" metrics: threshold = percentile(high)\n",
        "    - For \"higher is better\" metrics: threshold = percentile(low)\n",
        "    \"\"\"\n",
        "    easy_percentile_high: float = 80.0\n",
        "    easy_percentile_low: float = 20.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RunConfig:\n",
        "    \"\"\"Configuration for model generation runs.\"\"\"\n",
        "    language: str = \"en\"         # Used for spaCy model choice and tokenization\n",
        "    temperature: float = 0.2     # Low temp for deterministic evaluation\n",
        "    max_new_tokens: int = 500\n",
        "\n",
        "\n",
        "# Initialize configurations\n",
        "paths = Paths()\n",
        "topic_cfg = TopicConfig()\n",
        "guard_cfg = GuardrailConfig()\n",
        "run_cfg = RunConfig()\n",
        "\n",
        "# Create output directories\n",
        "for dir_path in [paths.outputs_dir, paths.scored_dir, paths.reports_dir]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(\"Configuration initialized:\")\n",
        "print(f\"  Topics: {topic_cfg.n_topics}\")\n",
        "print(f\"  Topic threshold (τ): {topic_cfg.min_topic_prob}\")\n",
        "print(f\"  Guardrail percentiles: {guard_cfg.easy_percentile_low}-{guard_cfg.easy_percentile_high}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "### Benchmark format (JSONL)\n",
        "Each line:\n",
        "```json\n",
        "{\"id\": \"001\", \"source_text\": \"...\", \"notes\": \"optional\", \"reference_easy_text\": \"optional\"}\n",
        "```\n",
        "\n",
        "### Calibration corpora\n",
        "- `data/hard/*.txt` - Complex bureaucratic/academic texts\n",
        "- `data/easy/*.txt` - Validated easy language samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Read a JSONL file and return a list of dictionaries.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return []\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                try:\n",
        "                    rows.append(json.loads(line))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"[WARN] Skipping invalid JSON line: {e}\")\n",
        "    return rows\n",
        "\n",
        "\n",
        "def read_txt_dir(dir_path: str) -> List[str]:\n",
        "    \"\"\"Read all .txt files from a directory and return their contents.\"\"\"\n",
        "    if not os.path.isdir(dir_path):\n",
        "        return []\n",
        "    texts = []\n",
        "    for fn in sorted(os.listdir(dir_path)):\n",
        "        if fn.lower().endswith(\".txt\"):\n",
        "            filepath = os.path.join(dir_path, fn)\n",
        "            try:\n",
        "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    content = f.read().strip()\n",
        "                    if content:\n",
        "                        texts.append(content)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Could not read {filepath}: {e}\")\n",
        "    return texts\n",
        "\n",
        "\n",
        "# Load data\n",
        "benchmark = read_jsonl(paths.benchmark_jsonl)\n",
        "hard_texts = read_txt_dir(paths.hard_dir)\n",
        "easy_texts = read_txt_dir(paths.easy_dir)\n",
        "sample_texts = read_txt_dir(paths.samples_dir)\n",
        "\n",
        "# Fallback: use samples as \"hard\" texts if no calibration data exists\n",
        "if not hard_texts and sample_texts:\n",
        "    print(\"[INFO] Using sample texts as 'hard' texts for demonstration.\")\n",
        "    hard_texts = sample_texts\n",
        "\n",
        "print(f\"Benchmark items: {len(benchmark)}\")\n",
        "print(f\"Hard texts:      {len(hard_texts)}\")\n",
        "print(f\"Easy texts:      {len(easy_texts)}\")\n",
        "print(f\"Sample texts:    {len(sample_texts)}\")\n",
        "\n",
        "if not easy_texts:\n",
        "    print(\"\\n[WARN] No easy language texts found in data/easy/\")\n",
        "    print(\"       Guardrails will use default values.\")\n",
        "    print(\"       For best results, add validated Easy Language samples to data/easy/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing Helpers\n",
        "\n",
        "Scoring happens at two levels:\n",
        "- **Whole text** (global metrics)\n",
        "- **Paragraph level** (semantic focus is most useful per paragraph)\n",
        "\n",
        "The paragraph splitter is conservative: blank lines or strong separators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentence splitting pattern (captures sentence-ending punctuation followed by capital letter)\n",
        "_SENT_SPLIT = re.compile(r\"(?<=[.!?])\\s+(?=[A-Z])\")\n",
        "\n",
        "# Word extraction pattern (handles hyphenated words)\n",
        "_WORD_RE = re.compile(r\"[A-Za-z]+(?:-[A-Za-z]+)*\")\n",
        "\n",
        "# Common English abbreviations that shouldn't end sentences\n",
        "_ENGLISH_ABBREVS = {\n",
        "    \"Mr.\": \"Mr_ABBREV\",\n",
        "    \"Mrs.\": \"Mrs_ABBREV\",\n",
        "    \"Ms.\": \"Ms_ABBREV\",\n",
        "    \"Dr.\": \"Dr_ABBREV\",\n",
        "    \"Prof.\": \"Prof_ABBREV\",\n",
        "    \"Jr.\": \"Jr_ABBREV\",\n",
        "    \"Sr.\": \"Sr_ABBREV\",\n",
        "    \"vs.\": \"vs_ABBREV\",\n",
        "    \"etc.\": \"etc_ABBREV\",\n",
        "    \"e.g.\": \"eg_ABBREV\",\n",
        "    \"i.e.\": \"ie_ABBREV\",\n",
        "    \"Inc.\": \"Inc_ABBREV\",\n",
        "    \"Ltd.\": \"Ltd_ABBREV\",\n",
        "    \"Corp.\": \"Corp_ABBREV\",\n",
        "    \"No.\": \"No_ABBREV\",\n",
        "    \"Vol.\": \"Vol_ABBREV\",\n",
        "    \"Jan.\": \"Jan_ABBREV\",\n",
        "    \"Feb.\": \"Feb_ABBREV\",\n",
        "    \"Mar.\": \"Mar_ABBREV\",\n",
        "    \"Apr.\": \"Apr_ABBREV\",\n",
        "    \"Aug.\": \"Aug_ABBREV\",\n",
        "    \"Sept.\": \"Sept_ABBREV\",\n",
        "    \"Oct.\": \"Oct_ABBREV\",\n",
        "    \"Nov.\": \"Nov_ABBREV\",\n",
        "    \"Dec.\": \"Dec_ABBREV\",\n",
        "}\n",
        "\n",
        "\n",
        "def normalize_ws(text: str) -> str:\n",
        "    \"\"\"Normalize whitespace in text.\n",
        "    \n",
        "    - Convert all line endings to \\\\n\n",
        "    - Collapse multiple spaces/tabs to single space\n",
        "    - Collapse 3+ newlines to 2 (paragraph separator)\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def split_paragraphs(text: str) -> List[str]:\n",
        "    \"\"\"Split text into paragraphs (blank lines or horizontal rules).\"\"\"\n",
        "    text = normalize_ws(text)\n",
        "    parts = [p.strip() for p in re.split(r\"\\n\\s*\\n|(?:\\n-{3,}\\n)\", text) if p.strip()]\n",
        "    return parts if parts else [text]\n",
        "\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    \"\"\"Split text into sentences using a simple heuristic.\n",
        "    \n",
        "    Handles common English abbreviations to avoid false splits.\n",
        "    Note: For production, consider spaCy's sentencizer for better accuracy.\n",
        "    \"\"\"\n",
        "    text = normalize_ws(text)\n",
        "    \n",
        "    # Temporarily replace abbreviations\n",
        "    for abbrev, placeholder in _ENGLISH_ABBREVS.items():\n",
        "        text = text.replace(abbrev, placeholder)\n",
        "    \n",
        "    # Split on sentence boundaries\n",
        "    sents = _SENT_SPLIT.split(text)\n",
        "    \n",
        "    # Restore abbreviations\n",
        "    restored = []\n",
        "    for s in sents:\n",
        "        for abbrev, placeholder in _ENGLISH_ABBREVS.items():\n",
        "            s = s.replace(placeholder, abbrev)\n",
        "        s = s.strip()\n",
        "        if s:\n",
        "            restored.append(s)\n",
        "    \n",
        "    return restored if restored else [text]\n",
        "\n",
        "\n",
        "def words(text: str) -> List[str]:\n",
        "    \"\"\"Extract words from text.\"\"\"\n",
        "    return _WORD_RE.findall(text)\n",
        "\n",
        "\n",
        "# Test preprocessing\n",
        "test_text = \"This is a test. Here comes another sentence! And one more?\"\n",
        "print(f\"Test text: {test_text}\")\n",
        "print(f\"Sentences: {split_sentences(test_text)}\")\n",
        "print(f\"Words: {words(test_text)}\")\n",
        "\n",
        "# Test with abbreviations\n",
        "test_abbrev = \"Dr. Smith works at Corp. Inc. He earned his Ph.D. in 2020. The cost is approx. $5M.\"\n",
        "print(f\"\\nTest with abbreviations: {test_abbrev}\")\n",
        "print(f\"Sentences: {split_sentences(test_abbrev)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Adapters (Plug-in Interface)\n",
        "\n",
        "Each adapter implements `generate(prompt: str) -> str`.\n",
        "\n",
        "### Guidelines\n",
        "- Keep outputs deterministic for evaluation (low temperature)\n",
        "- Store raw outputs to disk for reproducibility\n",
        "- Add any number of adapters (API models, local HF models, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModelAdapter:\n",
        "    \"\"\"Base class for model adapters.\"\"\"\n",
        "    name: str = \"base\"\n",
        "    \n",
        "    def generate(self, prompt: str, **kwargs) -> str:\n",
        "        \"\"\"Generate text from a prompt. Override in subclass.\"\"\"\n",
        "        raise NotImplementedError(\"Subclass must implement generate()\")\n",
        "\n",
        "\n",
        "class DummyEchoAdapter(BaseModelAdapter):\n",
        "    \"\"\"Dummy adapter that echoes input (for testing the pipeline).\"\"\"\n",
        "    name = \"echo_dummy\"\n",
        "    \n",
        "    def generate(self, prompt: str, **kwargs) -> str:\n",
        "        # Return truncated prompt for testing\n",
        "        return prompt[:min(len(prompt), 600)]\n",
        "\n",
        "\n",
        "class GroqAdapter(BaseModelAdapter):\n",
        "    \"\"\"Working adapter for Groq API.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_id: str, api_key_env: str = \"GROQ_API_KEY\"):\n",
        "        self.model_id = model_id\n",
        "        self.name = f\"groq_{model_id.split('/')[-1]}\"\n",
        "        self.api_key = os.getenv(api_key_env, \"\")\n",
        "        self._client = None\n",
        "        if not self.api_key:\n",
        "            print(f\"[WARN] No API key found in env var {api_key_env}. Adapter will fail if used.\")\n",
        "    \n",
        "    @property\n",
        "    def client(self):\n",
        "        \"\"\"Lazy-load Groq client.\"\"\"\n",
        "        if self._client is None:\n",
        "            try:\n",
        "                from groq import Groq\n",
        "                self._client = Groq(api_key=self.api_key)\n",
        "            except ImportError:\n",
        "                raise ImportError(\"groq package not installed. Run: pip install groq\")\n",
        "        return self._client\n",
        "    \n",
        "    def generate(self, prompt: str, **kwargs) -> str:\n",
        "        \"\"\"Generate text using Groq API.\"\"\"\n",
        "        try:\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=self.model_id,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=kwargs.get(\"temperature\", 0.2),\n",
        "                max_tokens=kwargs.get(\"max_new_tokens\", 500)\n",
        "            )\n",
        "            return completion.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"[ERROR] {e}\"\n",
        "\n",
        "\n",
        "class ExampleLocalHFAdapter(BaseModelAdapter):\n",
        "    \"\"\"Example adapter for local HuggingFace models (skeleton).\"\"\"\n",
        "    name = \"local_hf_placeholder\"\n",
        "    \n",
        "    def __init__(self, model_path: str = \"\"):\n",
        "        self.model_path = model_path\n",
        "    \n",
        "    def generate(self, prompt: str, **kwargs) -> str:\n",
        "        raise NotImplementedError(\"Wire this to transformers pipeline / generate().\")\n",
        "\n",
        "\n",
        "# Register adapters to use\n",
        "# Available Groq models: qwen/qwen3-32b, gemma2-9b-it, mixtral-8x7b-32768\n",
        "adapters: List[BaseModelAdapter] = [\n",
        "    DummyEchoAdapter(),  # For testing pipeline\n",
        "]\n",
        "\n",
        "print(f\"Registered adapters: {[a.name for a in adapters]}\")\n",
        "print(\"\\nTo add Groq models, use:\")\n",
        "print(\"  adapters.append(GroqAdapter('qwen/qwen3-32b'))\")\n",
        "print(\"  adapters.append(GroqAdapter('gemma2-9b-it'))\")\n",
        "print(\"  adapters.append(GroqAdapter('mixtral-8x7b-32768'))\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prompt Template\n",
        "\n",
        "Keep the prompt stable across models. If you do prompt optimization, version prompts and re-run the same benchmark.\n",
        "\n",
        "This prompt follows Easy Language (Leichte Sprache) guidelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_V1 = \"\"\"You are a Plain Language specialist.\n",
        "Rewrite the following text in Plain English that is easy to understand.\n",
        "\n",
        "Rules:\n",
        "- Use short sentences. Aim for 12-15 words. Avoid sentences over 20 words.\n",
        "- Use active voice. Avoid passive constructions.\n",
        "- Avoid negatives when possible.\n",
        "- Explain difficult terms briefly if they are necessary.\n",
        "- Use the same word for the same concept throughout.\n",
        "- Keep the meaning. Do not add new facts.\n",
        "\n",
        "Text:\n",
        "{source_text}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_prompt(source_text: str, template: str = PROMPT_V1) -> str:\n",
        "    \"\"\"Create a prompt from source text using the template.\"\"\"\n",
        "    return template.format(source_text=source_text.strip())\n",
        "\n",
        "\n",
        "# Show example\n",
        "example_source = \"The implementation of the aforementioned measures was subsequently executed.\"\n",
        "print(\"Example prompt:\")\n",
        "print(\"-\" * 40)\n",
        "print(make_prompt(example_source))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run Benchmark (Generate Outputs)\n",
        "\n",
        "Saves JSONL per model: `outputs/runs/{model_name}.jsonl`\n",
        "\n",
        "Each line includes:\n",
        "- `id`: Item identifier\n",
        "- `source_text`: Original input\n",
        "- `output_text`: Model generation\n",
        "- `metadata`: Timing, prompt version, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_generation(\n",
        "    adapter: BaseModelAdapter,\n",
        "    items: List[Dict[str, Any]],\n",
        "    out_path: str,\n",
        "    sleep_s: float = 0.0,\n",
        ") -> None:\n",
        "    \"\"\"Run text generation for all benchmark items and save results.\n",
        "    \n",
        "    Args:\n",
        "        adapter: Model adapter to use\n",
        "        items: List of benchmark items (must have 'source_text' key)\n",
        "        out_path: Output JSONL file path\n",
        "        sleep_s: Sleep between requests (for rate limiting)\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    \n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in tqdm(items, desc=f\"Generating: {adapter.name}\"):\n",
        "            sid = str(item.get(\"id\", \"\"))\n",
        "            source = item[\"source_text\"]\n",
        "            prompt = make_prompt(source)\n",
        "            \n",
        "            t0 = time.time()\n",
        "            try:\n",
        "                out = adapter.generate(\n",
        "                    prompt, \n",
        "                    temperature=run_cfg.temperature, \n",
        "                    max_new_tokens=run_cfg.max_new_tokens\n",
        "                )\n",
        "            except Exception as e:\n",
        "                out = f\"[ERROR] {e}\"\n",
        "            dt = time.time() - t0\n",
        "\n",
        "            row = {\n",
        "                \"id\": sid,\n",
        "                \"model\": adapter.name,\n",
        "                \"prompt_version\": \"v1\",\n",
        "                \"source_text\": source,\n",
        "                \"output_text\": normalize_ws(out),\n",
        "                \"latency_s\": round(dt, 3),\n",
        "                \"meta\": {k: v for k, v in item.items() if k not in {\"source_text\"}},\n",
        "            }\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "            if sleep_s > 0:\n",
        "                time.sleep(sleep_s)\n",
        "    \n",
        "    print(f\"Saved: {out_path}\")\n",
        "\n",
        "\n",
        "# Example: Uncomment to run generation (after wiring real adapters)\n",
        "# for ad in adapters:\n",
        "#     out_file = os.path.join(paths.outputs_dir, f\"{ad.name}.jsonl\")\n",
        "#     run_generation(ad, benchmark, out_file, sleep_s=0.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Topic Model (for Semantic Focus Metrics)\n",
        "\n",
        "We build a shared topic model so all texts are scored on the same topic space.\n",
        "\n",
        "### Approach\n",
        "1. Fit TF-IDF + NMF on a \"topic training corpus\" (hard + easy + sources + outputs)\n",
        "2. Convert each paragraph into a topic probability vector **p**\n",
        "3. Compute semantic metrics from **p**\n",
        "\n",
        "NMF (Non-negative Matrix Factorization) tends to be more stable than LDA on short corpora.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TopicModelBundle:\n",
        "    \"\"\"Bundle containing fitted TF-IDF vectorizer and NMF model.\"\"\"\n",
        "    vectorizer: TfidfVectorizer\n",
        "    nmf: NMF\n",
        "\n",
        "\n",
        "def build_topic_corpus(\n",
        "    hard: List[str],\n",
        "    easy: List[str],\n",
        "    benchmark_items: List[Dict[str, Any]],\n",
        "    generated_runs: Optional[List[Dict[str, Any]]] = None\n",
        ") -> List[str]:\n",
        "    \"\"\"Build corpus for topic model training.\n",
        "    \n",
        "    Combines hard texts, easy texts, benchmark sources, and optionally generated outputs.\n",
        "    \"\"\"\n",
        "    corpus = []\n",
        "    corpus.extend(hard)\n",
        "    corpus.extend(easy)\n",
        "    corpus.extend([x[\"source_text\"] for x in benchmark_items if \"source_text\" in x])\n",
        "    if generated_runs:\n",
        "        corpus.extend([x[\"output_text\"] for x in generated_runs if x.get(\"output_text\")])\n",
        "    \n",
        "    # Clean and filter\n",
        "    corpus = [normalize_ws(t) for t in corpus if t and len(t.strip()) > 10]\n",
        "    return corpus\n",
        "\n",
        "\n",
        "def fit_topic_model(corpus: List[str], cfg: TopicConfig) -> TopicModelBundle:\n",
        "    \"\"\"Fit TF-IDF + NMF topic model on corpus.\"\"\"\n",
        "    if len(corpus) < cfg.n_topics:\n",
        "        print(f\"[WARN] Corpus size ({len(corpus)}) < n_topics ({cfg.n_topics}). Reducing n_topics.\")\n",
        "        n_topics = max(2, len(corpus) - 1)\n",
        "    else:\n",
        "        n_topics = cfg.n_topics\n",
        "    \n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=cfg.max_features,\n",
        "        ngram_range=cfg.ngram_range,\n",
        "        lowercase=True,\n",
        "    )\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    \n",
        "    nmf = NMF(\n",
        "        n_components=n_topics,\n",
        "        random_state=42,\n",
        "        max_iter=cfg.nmf_max_iter,\n",
        "        init=\"nndsvda\",\n",
        "    )\n",
        "    nmf.fit(X)\n",
        "    \n",
        "    print(f\"Topic model fitted: {n_topics} topics, {X.shape[1]} features, {X.shape[0]} documents\")\n",
        "    return TopicModelBundle(vectorizer=vectorizer, nmf=nmf)\n",
        "\n",
        "\n",
        "def topic_probs(bundle: TopicModelBundle, texts: List[str]) -> np.ndarray:\n",
        "    \"\"\"Transform texts to topic probability vectors.\n",
        "    \n",
        "    Returns:\n",
        "        Array of shape [n_texts, n_topics] with normalized probabilities.\n",
        "    \"\"\"\n",
        "    X = bundle.vectorizer.transform(texts)\n",
        "    W = bundle.nmf.transform(X)  # shape: [n_texts, n_topics]\n",
        "    W = np.clip(W, 0, None)\n",
        "    \n",
        "    # Normalize to probabilities\n",
        "    row_sums = W.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1.0  # Avoid division by zero\n",
        "    P = W / row_sums\n",
        "    return P\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Semantic Metrics from Topic Probabilities\n",
        "\n",
        "Given a topic probability vector **p**:\n",
        "\n",
        "| Metric | Formula | Interpretation |\n",
        "|--------|---------|----------------|\n",
        "| **n_topics** | Count of p_i ≥ τ | Number of \"active\" topics |\n",
        "| **semantic_richness** | Σ p_i × rank_i | Higher = more diverse topics |\n",
        "| **semantic_clarity** | (1/n) × Σ(max(p) - p_i) | Higher = one dominant topic |\n",
        "| **semantic_noise** | n × Σ(p_i - p̄)⁴ / (Σ(p_i - p̄)²)² | Kurtosis-like measure |\n",
        "| **n_eff** | exp(entropy) | Effective number of topics |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def semantic_metrics_from_p(p: np.ndarray, min_prob: float) -> Dict[str, float]:\n",
        "    \"\"\"Compute semantic focus metrics from a topic probability vector.\n",
        "    \n",
        "    Args:\n",
        "        p: 1D topic probability vector (should sum to 1)\n",
        "        min_prob: Threshold τ for considering a topic \"present\"\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of semantic metrics\n",
        "    \"\"\"\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p_sum = p.sum()\n",
        "    if p_sum > 0:\n",
        "        p = p / p_sum\n",
        "    \n",
        "    # Number of discovered topics (above threshold)\n",
        "    present = p[p >= min_prob]\n",
        "    n_disc = int(present.size) if present.size > 0 else 1\n",
        "    \n",
        "    # Rank-based semantic richness\n",
        "    # Sort descending; rank i = 1..K\n",
        "    p_sorted = np.sort(p)[::-1]\n",
        "    ranks = np.arange(1, len(p_sorted) + 1)\n",
        "    richness = float(np.sum(p_sorted * ranks))\n",
        "    \n",
        "    # Semantic clarity: average gap from maximum\n",
        "    pmax = float(p_sorted[0]) if len(p_sorted) > 0 else 0.0\n",
        "    clarity = float(np.mean(pmax - p_sorted)) if len(p_sorted) > 0 else 0.0\n",
        "    \n",
        "    # Semantic noise: kurtosis-like measure\n",
        "    pbar = float(np.mean(p_sorted)) if len(p_sorted) > 0 else 0.0\n",
        "    dif = p_sorted - pbar\n",
        "    num = float(np.sum(dif ** 4))\n",
        "    den = float(np.sum(dif ** 2)) ** 2\n",
        "    noise = float(len(p_sorted) * num / den) if den > 0 else 0.0\n",
        "    \n",
        "    # Effective number of topics (entropy-based)\n",
        "    # n_eff = exp(H) where H = -Σ p_i log(p_i)\n",
        "    eps = 1e-12\n",
        "    ent = -float(np.sum(p * np.log(p + eps)))\n",
        "    n_eff = float(np.exp(ent))\n",
        "    \n",
        "    return {\n",
        "        \"n_topics\": float(n_disc),\n",
        "        \"semantic_richness\": round(richness, 4),\n",
        "        \"semantic_clarity\": round(clarity, 4),\n",
        "        \"semantic_noise\": round(noise, 4),\n",
        "        \"n_eff\": round(n_eff, 4),\n",
        "        \"topic_pmax\": round(pmax, 4),\n",
        "    }\n",
        "\n",
        "\n",
        "# Example\n",
        "example_p = np.array([0.6, 0.2, 0.1, 0.05, 0.05])\n",
        "print(\"Example topic distribution:\", example_p)\n",
        "print(\"Semantic metrics:\", semantic_metrics_from_p(example_p, min_prob=0.05))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Classic Readability Metrics\n",
        "\n",
        "Minimal set that works without heavy parsers:\n",
        "\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| sentence_count | Total sentences |\n",
        "| word_count | Total words |\n",
        "| avg_sentence_len_words | Mean words per sentence |\n",
        "| pct_sentences_gt20 | % of sentences > 20 words |\n",
        "| avg_word_len_chars | Mean characters per word |\n",
        "| pct_words_gt6 | % of long words (> 6 chars) |\n",
        "| **LIX** | (words/sentences) + (long_words×100/words) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def readability_metrics(text: str) -> Dict[str, float]:\n",
        "    \"\"\"Compute classic readability metrics for a text.\"\"\"\n",
        "    sents = split_sentences(text)\n",
        "    ws = words(text)\n",
        "    \n",
        "    sent_count = max(len(sents), 1)\n",
        "    word_count = max(len(ws), 1)\n",
        "    \n",
        "    # Sentence length statistics\n",
        "    sent_lens = [len(words(s)) for s in sents]\n",
        "    avg_sent_len = float(np.mean(sent_lens)) if sent_lens else 0.0\n",
        "    pct_gt20 = float(np.mean([l > 20 for l in sent_lens])) if sent_lens else 0.0\n",
        "    \n",
        "    # Word length statistics\n",
        "    word_lens = [len(w) for w in ws]\n",
        "    avg_word_len = float(np.mean(word_lens)) if word_lens else 0.0\n",
        "    pct_words_gt6 = float(np.mean([l > 6 for l in word_lens])) if word_lens else 0.0\n",
        "    \n",
        "    # LIX readability index\n",
        "    # Originally developed for Swedish/German texts but also used here as a language-agnostic proxy for English readability.\n",
        "    # Formula: LIX = (words / sentences) + (long_words × 100 / words)\n",
        "    long_words = sum(1 for w in ws if len(w) > 6)\n",
        "    lix = (word_count / sent_count) + (long_words * 100.0 / word_count)\n",
        "    \n",
        "    return {\n",
        "        \"sentence_count\": float(sent_count),\n",
        "        \"word_count\": float(word_count),\n",
        "        \"avg_sentence_len_words\": round(avg_sent_len, 2),\n",
        "        \"pct_sentences_gt20\": round(pct_gt20, 4),\n",
        "        \"avg_word_len_chars\": round(avg_word_len, 2),\n",
        "        \"pct_words_gt6\": round(pct_words_gt6, 4),\n",
        "        \"lix\": round(lix, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "# Test with English text\n",
        "test_en = \"The Department of Justice provides important information. Citizens can access this online.\"\n",
        "print(f\"Test: {test_en}\")\n",
        "print(f\"Metrics: {readability_metrics(test_en)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Entity Load Metrics\n",
        "\n",
        "Measures cognitive load from named entities:\n",
        "\n",
        "- **Preferred**: spaCy NER (`en_core_web_sm`)\n",
        "- **Fallback**: Heuristic capitalized word sequences\n",
        "\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| unique_entities_total | Distinct entities in text |\n",
        "| entity_mentions_total | Total entity mentions |\n",
        "| unique_entities_per_sentence | Avg distinct entities per sentence |\n",
        "| entity_mentions_per_sentence | Avg mentions per sentence |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_spacy_model(lang: str = \"en\"):\n",
        "    \"\"\"Load spaCy model for the given language.\"\"\"\n",
        "    if not _SPACY_AVAILABLE:\n",
        "        return None\n",
        "    \n",
        "    model_name = \"en_core_web_sm\" if lang == \"en\" else \"de_core_news_sm\"\n",
        "    try:\n",
        "        return spacy.load(model_name)\n",
        "    except OSError:\n",
        "        print(f\"[WARN] spaCy model '{model_name}' not available. Falling back to heuristic entities.\")\n",
        "        print(f\"       Install with: python -m spacy download {model_name}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Load NLP model (global for reuse)\n",
        "NLP = load_spacy_model(run_cfg.language)\n",
        "\n",
        "# Heuristic pattern for capitalized word sequences (fallback)\n",
        "_CAP_ENTITY = re.compile(r\"\\b(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b\")\n",
        "\n",
        "\n",
        "def extract_entities(text: str) -> List[str]:\n",
        "    \"\"\"Extract named entities from text.\"\"\"\n",
        "    if NLP is not None:\n",
        "        doc = NLP(text)\n",
        "        ents = [e.text.strip() for e in doc.ents if e.text.strip()]\n",
        "        return ents\n",
        "    \n",
        "    # Heuristic fallback: capitalized word sequences\n",
        "    return [m.group(0).strip() for m in _CAP_ENTITY.finditer(text)]\n",
        "\n",
        "\n",
        "def entity_metrics(text: str) -> Dict[str, float]:\n",
        "    \"\"\"Compute entity load metrics for a text.\"\"\"\n",
        "    sents = split_sentences(text)\n",
        "    ents = extract_entities(text)\n",
        "    ent_norm = [normalize_ws(e) for e in ents if e]\n",
        "    unique_ents = set(ent_norm)\n",
        "    \n",
        "    sent_count = max(len(sents), 1)\n",
        "    mentions = len(ent_norm)\n",
        "    uniq = len(unique_ents)\n",
        "    \n",
        "    return {\n",
        "        \"unique_entities_total\": float(uniq),\n",
        "        \"entity_mentions_total\": float(mentions),\n",
        "        \"unique_entities_per_sentence\": round(uniq / sent_count, 4),\n",
        "        \"entity_mentions_per_sentence\": round(mentions / sent_count, 4),\n",
        "    }\n",
        "\n",
        "\n",
        "# Test\n",
        "test_ent = \"Joe Biden met Emmanuel Macron in Paris. Later, Rishi Sunak joined them.\"\n",
        "print(f\"Test: {test_ent}\")\n",
        "print(f\"Entities: {extract_entities(test_ent)}\")\n",
        "print(f\"Metrics: {entity_metrics(test_ent)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Meaning Preservation\n",
        "\n",
        "Measures how well the output preserves the meaning of the source:\n",
        "\n",
        "- **Option A** (better): Sentence-transformers embeddings + cosine similarity\n",
        "- **Option B** (fallback): TF-IDF cosine similarity\n",
        "\n",
        "Output: `meaning_cosine` (0..1, higher = better preservation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sentence transformer model (if available)\n",
        "EMBEDDER = None\n",
        "if _ST_AVAILABLE:\n",
        "    try:\n",
        "        EMBEDDER = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "        print(\"Loaded sentence-transformers model for meaning preservation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load sentence-transformer: {e}\")\n",
        "        EMBEDDER = None\n",
        "\n",
        "\n",
        "def meaning_similarity(source: str, output: str) -> float:\n",
        "    \"\"\"Compute semantic similarity between source and output texts.\n",
        "    \n",
        "    Returns:\n",
        "        Cosine similarity (0..1)\n",
        "    \"\"\"\n",
        "    if not source.strip() or not output.strip():\n",
        "        return 0.0\n",
        "    \n",
        "    if EMBEDDER is not None:\n",
        "        vecs = EMBEDDER.encode([source, output], normalize_embeddings=True)\n",
        "        return float(np.dot(vecs[0], vecs[1]))\n",
        "    \n",
        "    # Fallback: TF-IDF cosine similarity\n",
        "    try:\n",
        "        v = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), lowercase=True)\n",
        "        X = v.fit_transform([source, output])\n",
        "        sim = cosine_similarity(X[0], X[1])[0, 0]\n",
        "        return float(sim)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "# Test\n",
        "src = \"The Department of Justice provides information for citizens.\"\n",
        "out = \"The Justice Department gives info to people.\"\n",
        "print(f\"Source: {src}\")\n",
        "print(f\"Output: {out}\")\n",
        "print(f\"Similarity: {meaning_similarity(src, out):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Combine All Metrics\n",
        "\n",
        "For semantic focus, we score each paragraph and aggregate:\n",
        "\n",
        "- Median clarity, richness, n_eff\n",
        "- % paragraphs with > 2 topics\n",
        "- Worst-decile richness (captures spikes)\n",
        "\n",
        "You can adjust aggregation later without re-running model generations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def paragraph_semantic_aggregates(\n",
        "    bundle: TopicModelBundle,\n",
        "    text: str,\n",
        "    cfg: TopicConfig\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Compute paragraph-level semantic aggregates.\"\"\"\n",
        "    paras = split_paragraphs(text)\n",
        "    if not paras:\n",
        "        paras = [text]\n",
        "    \n",
        "    P = topic_probs(bundle, paras)  # [n_paras, K]\n",
        "    per = [semantic_metrics_from_p(p, cfg.min_topic_prob) for p in P]\n",
        "    \n",
        "    df = pd.DataFrame(per)\n",
        "    if df.empty:\n",
        "        return {\n",
        "            \"para_count\": 1.0,\n",
        "            \"para_median_clarity\": 0.0,\n",
        "            \"para_median_richness\": 0.0,\n",
        "            \"para_median_n_eff\": 0.0,\n",
        "            \"para_pct_n_topics_gt2\": 0.0,\n",
        "            \"para_worst_decile_richness\": 0.0,\n",
        "        }\n",
        "    \n",
        "    return {\n",
        "        \"para_count\": float(len(paras)),\n",
        "        \"para_median_clarity\": float(df[\"semantic_clarity\"].median()),\n",
        "        \"para_median_richness\": float(df[\"semantic_richness\"].median()),\n",
        "        \"para_median_n_eff\": float(df[\"n_eff\"].median()),\n",
        "        \"para_pct_n_topics_gt2\": float((df[\"n_topics\"] > 2).mean()),\n",
        "        \"para_worst_decile_richness\": float(df[\"semantic_richness\"].quantile(0.9)),\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_all_metrics(\n",
        "    bundle: TopicModelBundle,\n",
        "    source_text: str,\n",
        "    output_text: str,\n",
        "    cfg: TopicConfig\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Compute all metrics for a source-output pair.\n",
        "    \n",
        "    All output metrics are prefixed with 'out_'.\n",
        "    \"\"\"\n",
        "    m: Dict[str, float] = {}\n",
        "    \n",
        "    # Readability metrics\n",
        "    m.update({f\"out_{k}\": v for k, v in readability_metrics(output_text).items()})\n",
        "    \n",
        "    # Entity metrics\n",
        "    m.update({f\"out_{k}\": v for k, v in entity_metrics(output_text).items()})\n",
        "    \n",
        "    # Global topic metrics on full output\n",
        "    p_full = topic_probs(bundle, [output_text])[0]\n",
        "    m.update({f\"out_{k}\": v for k, v in semantic_metrics_from_p(p_full, cfg.min_topic_prob).items()})\n",
        "    \n",
        "    # Paragraph-level semantic aggregates\n",
        "    m.update({f\"out_{k}\": v for k, v in paragraph_semantic_aggregates(bundle, output_text, cfg).items()})\n",
        "    \n",
        "    # Meaning preservation\n",
        "    m[\"out_meaning_cosine\"] = round(meaning_similarity(source_text, output_text), 4)\n",
        "    \n",
        "    return m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Build Guardrails from EASY Corpus\n",
        "\n",
        "Thresholds are derived from the EASY (simple language) corpus distribution:\n",
        "\n",
        "| Metric Type | Threshold Logic |\n",
        "|-------------|----------------|\n",
        "| Lower is better | threshold = percentile_80(easy) |\n",
        "| Higher is better | threshold = percentile_20(easy) |\n",
        "\n",
        "### Default Guardrails\n",
        "- `out_pct_sentences_gt20` ≤ p80(easy)\n",
        "- `out_lix` ≤ p80(easy)\n",
        "- `out_para_worst_decile_richness` ≤ p80(easy)\n",
        "- `out_para_median_clarity` ≥ p20(easy)\n",
        "- `out_unique_entities_per_sentence` ≤ p80(easy)\n",
        "- `out_meaning_cosine` ≥ 0.70 (prevents \"cheating\" by deleting meaning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classify metrics by optimization direction\n",
        "LOWER_IS_BETTER = [\n",
        "    \"out_avg_sentence_len_words\",\n",
        "    \"out_pct_sentences_gt20\",\n",
        "    \"out_avg_word_len_chars\",\n",
        "    \"out_pct_words_gt6\",\n",
        "    \"out_lix\",\n",
        "    \"out_n_topics\",\n",
        "    \"out_semantic_richness\",\n",
        "    \"out_n_eff\",\n",
        "    \"out_para_pct_n_topics_gt2\",\n",
        "    \"out_para_worst_decile_richness\",\n",
        "    \"out_unique_entities_per_sentence\",\n",
        "    \"out_entity_mentions_per_sentence\",\n",
        "]\n",
        "\n",
        "HIGHER_IS_BETTER = [\n",
        "    \"out_semantic_clarity\",\n",
        "    \"out_para_median_clarity\",\n",
        "    \"out_meaning_cosine\",\n",
        "]\n",
        "\n",
        "\n",
        "def derive_guardrails_from_easy(\n",
        "    bundle: TopicModelBundle,\n",
        "    easy_texts: List[str],\n",
        "    cfg: TopicConfig,\n",
        "    guard_cfg: GuardrailConfig\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Derive guardrail thresholds from an easy language corpus.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping metric names to threshold values.\n",
        "    \"\"\"\n",
        "    if not easy_texts:\n",
        "        print(\"[WARN] No easy texts found. Using default guardrails.\")\n",
        "        return {\n",
        "            \"out_pct_sentences_gt20\": 0.10,\n",
        "            \"out_lix\": 45.0,\n",
        "            \"out_avg_sentence_len_words\": 15.0,\n",
        "            \"out_meaning_cosine\": 0.70,\n",
        "            \"out_para_median_clarity\": 0.10,\n",
        "        }\n",
        "    \n",
        "    rows = []\n",
        "    # For calibration: source_text == output_text (meaning_cosine will be ~1)\n",
        "    for t in tqdm(easy_texts, desc=\"Scoring EASY corpus for guardrails\"):\n",
        "        rows.append(compute_all_metrics(bundle, t, t, cfg))\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    \n",
        "    guardrails: Dict[str, float] = {}\n",
        "    \n",
        "    for col in LOWER_IS_BETTER:\n",
        "        if col in df.columns:\n",
        "            guardrails[col] = float(df[col].quantile(guard_cfg.easy_percentile_high / 100.0))\n",
        "    \n",
        "    for col in HIGHER_IS_BETTER:\n",
        "        if col in df.columns:\n",
        "            guardrails[col] = float(df[col].quantile(guard_cfg.easy_percentile_low / 100.0))\n",
        "    \n",
        "    # Special handling: meaning_cosine from easy==easy is not useful\n",
        "    # Set a pragmatic, configurable minimum threshold (default 0.70)\n",
        "    meaning_cosine_min = getattr(guard_cfg, \"meaning_cosine_min\", 0.70)\n",
        "    guardrails[\"out_meaning_cosine\"] = max(meaning_cosine_min, guardrails.get(\"out_meaning_cosine\", meaning_cosine_min))\n",
        "    \n",
        "    return guardrails\n",
        "\n",
        "\n",
        "print(\"Guardrail classification:\")\n",
        "print(f\"  Lower is better: {len(LOWER_IS_BETTER)} metrics\")\n",
        "print(f\"  Higher is better: {len(HIGHER_IS_BETTER)} metrics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Score Model Runs and Guardrail Evaluation\n",
        "\n",
        "For each model run:\n",
        "1. Load generated outputs\n",
        "2. Compute all metrics\n",
        "3. Check guardrails\n",
        "4. Calculate pass rate\n",
        "\n",
        "Output: Per-item metrics table and per-model summary table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_run_jsonl(path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load a model run JSONL file.\"\"\"\n",
        "    return read_jsonl(path)\n",
        "\n",
        "\n",
        "def evaluate_against_guardrails(\n",
        "    metrics: Dict[str, float], \n",
        "    guardrails: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate metrics against guardrail thresholds.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with pass counts, rates, and failed guardrails.\n",
        "    \"\"\"\n",
        "    checks = {}\n",
        "    \n",
        "    for k, thr in guardrails.items():\n",
        "        if k not in metrics:\n",
        "            continue\n",
        "        \n",
        "        if k in LOWER_IS_BETTER:\n",
        "            checks[k] = bool(metrics[k] <= thr)\n",
        "        elif k in HIGHER_IS_BETTER:\n",
        "            checks[k] = bool(metrics[k] >= thr)\n",
        "        # If not classified, skip\n",
        "    \n",
        "    passed = sum(checks.values()) if checks else 0\n",
        "    total = len(checks) if checks else 0\n",
        "    pass_rate = passed / total if total > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        \"guardrails_total\": total,\n",
        "        \"guardrails_passed\": passed,\n",
        "        \"guardrails_pass_rate\": round(pass_rate, 4),\n",
        "        \"guardrails_failed\": [k for k, ok in checks.items() if not ok],\n",
        "    }\n",
        "\n",
        "\n",
        "def score_run(\n",
        "    run_rows: List[Dict[str, Any]],\n",
        "    bundle: TopicModelBundle,\n",
        "    guardrails: Dict[str, float],\n",
        "    cfg: TopicConfig\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Score all items in a model run.\"\"\"\n",
        "    if not run_rows:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    model_name = run_rows[0].get('model', 'unknown')\n",
        "    scored_rows = []\n",
        "    \n",
        "    for r in tqdm(run_rows, desc=f\"Scoring run: {model_name}\"):\n",
        "        source = r.get(\"source_text\", \"\")\n",
        "        out = r.get(\"output_text\", \"\")\n",
        "        \n",
        "        if not out or out.startswith(\"[ERROR]\"):\n",
        "            continue\n",
        "        \n",
        "        m = compute_all_metrics(bundle, source, out, cfg)\n",
        "        g = evaluate_against_guardrails(m, guardrails)\n",
        "        \n",
        "        scored = {**r, **m, **g}\n",
        "        scored_rows.append(scored)\n",
        "    \n",
        "    return pd.DataFrame(scored_rows)\n",
        "\n",
        "\n",
        "def summarize_models(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create summary statistics per model.\"\"\"\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    grp = df.groupby(\"model\", dropna=False)\n",
        "    \n",
        "    agg_dict = {\n",
        "        \"id\": \"count\",\n",
        "        \"guardrails_pass_rate\": [\"mean\", \"median\"],\n",
        "    }\n",
        "    \n",
        "    # Add optional columns if present\n",
        "    optional_cols = [\n",
        "        (\"latency_s\", \"mean\"),\n",
        "        (\"out_meaning_cosine\", \"mean\"),\n",
        "        (\"out_pct_sentences_gt20\", \"mean\"),\n",
        "        (\"out_lix\", \"mean\"),\n",
        "        (\"out_para_median_clarity\", \"mean\"),\n",
        "        (\"out_para_worst_decile_richness\", \"mean\"),\n",
        "        (\"out_para_pct_n_topics_gt2\", \"mean\"),\n",
        "    ]\n",
        "    \n",
        "    for col, agg in optional_cols:\n",
        "        if col in df.columns:\n",
        "            agg_dict[col] = agg\n",
        "    \n",
        "    summary = grp.agg(agg_dict).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    new_cols = []\n",
        "    for col in summary.columns:\n",
        "        if isinstance(col, tuple):\n",
        "            new_cols.append(f\"{col[0]}_{col[1]}\" if col[1] else col[0])\n",
        "        else:\n",
        "            new_cols.append(col)\n",
        "    summary.columns = new_cols\n",
        "    \n",
        "    # Sort by pass rate (descending)\n",
        "    if \"guardrails_pass_rate_mean\" in summary.columns:\n",
        "        summary = summary.sort_values(\"guardrails_pass_rate_mean\", ascending=False)\n",
        "    \n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Full Pipeline Runner\n",
        "\n",
        "Steps:\n",
        "1. Fit topic model on (hard + easy + sources + outputs)\n",
        "2. Derive guardrails from EASY corpus\n",
        "3. For each model run file: Score outputs and save scored table\n",
        "4. Produce summary tables and qualitative samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_bundle_from_current_data(\n",
        "    hard: List[str],\n",
        "    easy: List[str],\n",
        "    bench: List[Dict[str, Any]],\n",
        "    topic_cfg: TopicConfig\n",
        ") -> TopicModelBundle:\n",
        "    \"\"\"Build topic model from available data.\"\"\"\n",
        "    corpus = build_topic_corpus(hard, easy, bench, generated_runs=None)\n",
        "    if len(corpus) < 5:\n",
        "        print(\"[WARN] Topic corpus is very small. Add more calibration texts for stable metrics.\")\n",
        "    return fit_topic_model(corpus, topic_cfg)\n",
        "\n",
        "\n",
        "def pipeline_score_all_models(\n",
        "    run_files: List[str],\n",
        "    bench: List[Dict[str, Any]],\n",
        "    hard: List[str],\n",
        "    easy: List[str],\n",
        "    topic_cfg: TopicConfig,\n",
        "    guard_cfg: GuardrailConfig,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Run full scoring pipeline on all model runs.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (all_scored_df, summary_df)\n",
        "    \"\"\"\n",
        "    # Step 1: Build topic model\n",
        "    print(\"Building topic model...\")\n",
        "    bundle = build_bundle_from_current_data(hard, easy, bench, topic_cfg)\n",
        "    \n",
        "    # Step 2: Derive guardrails\n",
        "    print(\"\\nDeriving guardrails from EASY corpus...\")\n",
        "    guardrails = derive_guardrails_from_easy(bundle, easy, topic_cfg, guard_cfg)\n",
        "    print(f\"Guardrails: {guardrails}\")\n",
        "    \n",
        "    # Step 3: Score each run\n",
        "    all_scored = []\n",
        "    for rf in run_files:\n",
        "        print(f\"\\nProcessing: {rf}\")\n",
        "        rows = load_run_jsonl(rf)\n",
        "        if not rows:\n",
        "            print(f\"  [SKIP] No data in {rf}\")\n",
        "            continue\n",
        "        \n",
        "        df = score_run(rows, bundle, guardrails, topic_cfg)\n",
        "        \n",
        "        if not df.empty:\n",
        "            # Save scored results\n",
        "            out_name = os.path.splitext(os.path.basename(rf))[0]\n",
        "            scored_path = os.path.join(paths.scored_dir, f\"{out_name}_scored.parquet\")\n",
        "            df.to_parquet(scored_path, index=False)\n",
        "            print(f\"  Saved: {scored_path}\")\n",
        "            all_scored.append(df)\n",
        "    \n",
        "    if not all_scored:\n",
        "        print(\"\\n[WARN] No scored results. Check that run files exist and contain valid data.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "    \n",
        "    # Step 4: Combine and summarize\n",
        "    df_all = pd.concat(all_scored, ignore_index=True)\n",
        "    summary = summarize_models(df_all)\n",
        "    \n",
        "    # Save summary\n",
        "    summary_path = os.path.join(paths.reports_dir, \"model_summary.csv\")\n",
        "    summary.to_csv(summary_path, index=False)\n",
        "    print(f\"\\nSaved summary: {summary_path}\")\n",
        "    \n",
        "    return df_all, summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Run the Scoring Pipeline\n",
        "\n",
        "### Prerequisites\n",
        "1. Generate model outputs first (Section 6) or place existing JSONLs in `outputs/runs/`\n",
        "2. Add calibration texts to `data/easy/` and `data/hard/`\n",
        "\n",
        "Then run this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find run files\n",
        "if os.path.isdir(paths.outputs_dir):\n",
        "    run_files = [\n",
        "        os.path.join(paths.outputs_dir, fn)\n",
        "        for fn in sorted(os.listdir(paths.outputs_dir))\n",
        "        if fn.endswith(\".jsonl\")\n",
        "    ]\n",
        "else:\n",
        "    run_files = []\n",
        "\n",
        "print(f\"Run files found: {run_files}\")\n",
        "\n",
        "# Uncomment to execute scoring:\n",
        "# df_all, summary = pipeline_score_all_models(\n",
        "#     run_files, benchmark, hard_texts, easy_texts, topic_cfg, guard_cfg\n",
        "# )\n",
        "# display(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Qualitative Review Set\n",
        "\n",
        "Pull a small sample per model for manual review:\n",
        "- **Best**: High pass rate + high meaning preservation\n",
        "- **Worst**: Low pass rate + low meaning preservation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_qualitative(df_all: pd.DataFrame, n_each: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"Sample best and worst examples per model for qualitative review.\"\"\"\n",
        "    if df_all.empty:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    cols = [\n",
        "        \"id\", \"model\", \"guardrails_pass_rate\", \"out_meaning_cosine\", \n",
        "        \"source_text\", \"output_text\", \"guardrails_failed\"\n",
        "    ]\n",
        "    cols = [c for c in cols if c in df_all.columns]\n",
        "    \n",
        "    df = df_all.copy()\n",
        "    df = df.sort_values(\n",
        "        [\"model\", \"guardrails_pass_rate\", \"out_meaning_cosine\"], \n",
        "        ascending=[True, False, False]\n",
        "    )\n",
        "    \n",
        "    samples = []\n",
        "    for model, g in df.groupby(\"model\"):\n",
        "        best = g.head(n_each)\n",
        "        worst = g.tail(n_each)\n",
        "        samples.append(best)\n",
        "        samples.append(worst)\n",
        "    \n",
        "    out = pd.concat(samples, ignore_index=True)\n",
        "    return out[cols] if cols else out\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# qual = sample_qualitative(df_all, n_each=10)\n",
        "# qual_path = os.path.join(paths.reports_dir, \"qualitative_samples.csv\")\n",
        "# qual.to_csv(qual_path, index=False)\n",
        "# print(f\"Saved: {qual_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Visualization\n",
        "\n",
        "Simple bar charts to compare models across key metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_model_comparison(summary: pd.DataFrame, metric: str, title: str, higher_is_better: bool = True):\n",
        "    \"\"\"Plot bar chart comparing models on a metric.\"\"\"\n",
        "    if summary.empty or metric not in summary.columns:\n",
        "        print(f\"Cannot plot: {metric} not in summary\")\n",
        "        return\n",
        "    \n",
        "    x = summary[\"model\"].tolist()\n",
        "    y = summary[metric].tolist()\n",
        "    \n",
        "    # Color based on performance\n",
        "    colors = ['#2ecc71' if higher_is_better else '#e74c3c'] * len(y)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    bars = ax.bar(x, y, color=colors, alpha=0.8)\n",
        "    \n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Model', fontsize=12)\n",
        "    ax.set_ylabel(metric, fontsize=12)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars, y):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example plots (uncomment after running pipeline):\n",
        "# plot_model_comparison(summary, \"guardrails_pass_rate_mean\", \"Mean Guardrail Pass Rate by Model\")\n",
        "# plot_model_comparison(summary, \"out_meaning_cosine_mean\", \"Mean Meaning Cosine by Model\")\n",
        "# plot_model_comparison(summary, \"out_pct_sentences_gt20_mean\", \"Mean % Sentences > 20 Words\", higher_is_better=False)\n",
        "# plot_model_comparison(summary, \"out_lix_mean\", \"Mean LIX Score by Model\", higher_is_better=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Future Extensions\n",
        "\n",
        "Potential additions to tighten \"Easy Language\" evaluation:\n",
        "\n",
        "### Linguistic Quality\n",
        "- **Negation detection**: Rate of `not/never/no` usage\n",
        "- **Passive detection**: Heuristic for `is/was ...ed`, `is/was being ...ed`\n",
        "- **Terminology consistency**: Same entity should use same surface form\n",
        "- **Definition coverage**: Difficult terms must have brief explanation\n",
        "\n",
        "### Integration\n",
        "All extensions can be added as:\n",
        "1. Additional metric functions\n",
        "2. New guardrails derived from EASY corpus percentiles\n",
        "\n",
        "### Example: Negation and Passive Rate Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Negation patterns (English)\n",
        "_NEGATION_PATTERN = re.compile(\n",
        "    r\"\\b(not|no|never|neither|nobody|nothing|nowhere|none|cannot|can't|won't|don't|doesn't|didn't|isn't|aren't|wasn't|weren't)\\b\", \n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "# Passive voice patterns (English) - looks for \"be\" + past participle indicators\n",
        "# NOTE: This is a heuristic pattern and will match some non-passive constructions (e.g., \"was happy\").\n",
        "_PASSIVE_PATTERN = re.compile(\n",
        "    r\"\\b(is|are|was|were|been|being)\\s+\\w+ed\\b|\\b(is|are|was|were|been|being)\\s+\\w+en\\b\", \n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "\n",
        "def extended_linguistic_metrics(text: str) -> Dict[str, float]:\n",
        "    \"\"\"Compute additional linguistic quality metrics.\n",
        "    \n",
        "    Future extension: Add these to compute_all_metrics() and guardrails.\n",
        "    \"\"\"\n",
        "    ws = words(text)\n",
        "    sents = split_sentences(text)\n",
        "    \n",
        "    word_count = max(len(ws), 1)\n",
        "    sent_count = max(len(sents), 1)\n",
        "    \n",
        "    # Negation rate (per word)\n",
        "    negation_matches = len(_NEGATION_PATTERN.findall(text))\n",
        "    negation_rate = negation_matches / word_count\n",
        "    \n",
        "    # Passive voice rate (per sentence)\n",
        "    passive_matches = len(_PASSIVE_PATTERN.findall(text))\n",
        "    passive_rate = passive_matches / sent_count\n",
        "    \n",
        "    return {\n",
        "        \"negation_rate\": round(negation_rate, 4),\n",
        "        \"passive_rate\": round(passive_rate, 4),\n",
        "        \"negation_count\": negation_matches,\n",
        "        \"passive_indicators\": passive_matches,\n",
        "    }\n",
        "\n",
        "\n",
        "# Test\n",
        "test_neg = \"This was not done. There is no alternative. The regulation was approved.\"\n",
        "print(f\"Test: {test_neg}\")\n",
        "print(f\"Extended metrics: {extended_linguistic_metrics(test_neg)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook provides a comprehensive, model-agnostic framework for evaluating **Plain English** text simplification:\n",
        "\n",
        "1. **Multi-dimensional metrics**: Readability, entities, semantic focus, meaning preservation\n",
        "2. **Data-backed guardrails**: Thresholds derived from calibration corpus\n",
        "3. **Extensible design**: Add models via adapters, add metrics via functions\n",
        "4. **Reproducible**: Outputs saved to disk, deterministic generation settings\n",
        "\n",
        "### Quick Start\n",
        "1. Add calibration texts to `data/easy/` and `data/hard/`\n",
        "2. Create benchmark file `data/benchmark.jsonl`\n",
        "3. Implement model adapters (Section 4)\n",
        "4. Run generation (Section 6)\n",
        "5. Run scoring pipeline (Section 15)\n",
        "6. Review results (Sections 16-17)\n",
        "\n",
        "### Key Formulas Reference\n",
        "\n",
        "| Metric | Formula | Good Value |\n",
        "|--------|---------|------------|\n",
        "| LIX | (W/S) + (LW×100/W) | < 40 (easy) |\n",
        "| Semantic Clarity | (1/n) × Σ(pmax - pi) | Higher = focused |\n",
        "| Semantic Richness | Σ pi × ranki | Lower = simpler |\n",
        "| N_eff | exp(-Σ pi × log(pi)) | Lower = clearer |\n",
        "| Meaning Cosine | cos(emb_src, emb_out) | > 0.7 |\n",
        "\n",
        "Where: W = words, S = sentences, LW = long words (>6 chars), p = topic probabilities\n",
        "\n",
        "### Future: German Support\n",
        "Once the English pipeline is stable, German (Leichte Sprache) support can be added by:\n",
        "1. Changing `run_cfg.language = \"de\"`\n",
        "2. Updating abbreviation patterns\n",
        "3. Adding German calibration texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 19. Model Comparison: Groq API Models\n",
        "\n",
        "This section runs a comparison of three models available via the Groq API:\n",
        "\n",
        "| Model | Description |\n",
        "|-------|-------------|\n",
        "| **qwen/qwen3-32b** | Qwen 3 32B - Alibaba's multilingual model |\n",
        "| **gemma2-9b-it** | Gemma 2 9B Instruct - Google's efficient model |\n",
        "| **mixtral-8x7b-32768** | Mixtral 8x7B - Mistral's MoE model |\n",
        "\n",
        "### Prerequisites\n",
        "- `GROQ_API_KEY` environment variable must be set\n",
        "- `groq` package installed (`pip install groq`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install groq if needed\n",
        "%pip install groq python-dotenv --quiet\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Check API key\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "if api_key:\n",
        "    print(\"✅ GROQ_API_KEY found\")\n",
        "else:\n",
        "    print(\"❌ GROQ_API_KEY not found. Please set it in your .env file or environment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register Groq models for comparison\n",
        "MODEL_IDS = [\n",
        "    \"qwen/qwen3-32b\",\n",
        "    \"gemma2-9b-it\",\n",
        "    \"mixtral-8x7b-32768\",\n",
        "]\n",
        "\n",
        "# Create adapters\n",
        "groq_adapters = [GroqAdapter(model_id) for model_id in MODEL_IDS]\n",
        "print(f\"📋 Registered {len(groq_adapters)} Groq adapters:\")\n",
        "for adapter in groq_adapters:\n",
        "    print(f\"   - {adapter.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run generation for all models\n",
        "print(f\"🚀 Starting generation for {len(benchmark)} benchmark items...\")\n",
        "print(f\"   Models: {[a.name for a in groq_adapters]}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for adapter in groq_adapters:\n",
        "    out_file = os.path.join(paths.outputs_dir, f\"{adapter.name}.jsonl\")\n",
        "    print(f\"\\n📝 Running: {adapter.name}\")\n",
        "    run_generation(adapter, benchmark, out_file, sleep_s=1.0)  # Rate limit\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✅ Generation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run scoring pipeline\n",
        "print(\"📊 Running scoring pipeline...\")\n",
        "\n",
        "# Find all run files\n",
        "run_files = [\n",
        "    os.path.join(paths.outputs_dir, fn)\n",
        "    for fn in sorted(os.listdir(paths.outputs_dir))\n",
        "    if fn.endswith(\".jsonl\")\n",
        "]\n",
        "print(f\"Found {len(run_files)} run files: {[os.path.basename(f) for f in run_files]}\")\n",
        "\n",
        "# Run scoring\n",
        "df_all, summary = pipeline_score_all_models(\n",
        "    run_files, benchmark, hard_texts, easy_texts, topic_cfg, guard_cfg\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"📋 MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "display(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed metrics by model\n",
        "if not df_all.empty:\n",
        "    print(\"📈 DETAILED METRICS BY MODEL\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Key metrics to display\n",
        "    key_metrics = [\n",
        "        \"out_avg_sentence_len_words\",\n",
        "        \"out_pct_sentences_gt20\", \n",
        "        \"out_lix\",\n",
        "        \"out_meaning_cosine\",\n",
        "        \"out_para_median_clarity\",\n",
        "        \"guardrails_pass_rate\"\n",
        "    ]\n",
        "    \n",
        "    # Filter to available columns\n",
        "    available_metrics = [m for m in key_metrics if m in df_all.columns]\n",
        "    \n",
        "    detail_summary = df_all.groupby(\"model\")[available_metrics].agg([\"mean\", \"std\"]).round(3)\n",
        "    display(detail_summary)\n",
        "    \n",
        "    # Show sample outputs\n",
        "    print(\"\\n📝 SAMPLE OUTPUTS (first benchmark item per model)\")\n",
        "    print(\"-\" * 70)\n",
        "    for model in df_all[\"model\"].unique():\n",
        "        model_data = df_all[df_all[\"model\"] == model].iloc[0]\n",
        "        print(f\"\\n🤖 {model}\")\n",
        "        print(f\"   Source: {model_data['source_text'][:100]}...\")\n",
        "        print(f\"   Output: {model_data['output_text'][:150]}...\")\n",
        "        print(f\"   Pass Rate: {model_data.get('guardrails_pass_rate', 'N/A')}\")\n",
        "        print(f\"   Meaning Cosine: {model_data.get('out_meaning_cosine', 'N/A')}\")\n",
        "else:\n",
        "    print(\"⚠️ No results to display. Run the generation and scoring cells first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "if not summary.empty and 'model' in summary.columns:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    models = summary[\"model\"].tolist()\n",
        "    model_labels = [m.split(\"_\")[-1][:15] for m in models]\n",
        "    \n",
        "    # Plot 1: Pass Rate\n",
        "    if \"guardrails_pass_rate_mean\" in summary.columns:\n",
        "        ax = axes[0, 0]\n",
        "        values = summary[\"guardrails_pass_rate_mean\"].tolist()\n",
        "        bars = ax.bar(model_labels, values, color=['#2ecc71', '#3498db', '#9b59b6'])\n",
        "        ax.set_title(\"Guardrail Pass Rate\", fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel(\"Pass Rate\")\n",
        "        ax.set_ylim(0, 1)\n",
        "        for bar, val in zip(bars, values):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, f'{val:.2f}', \n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    # Plot 2: Meaning Cosine\n",
        "    if \"out_meaning_cosine_mean\" in summary.columns:\n",
        "        ax = axes[0, 1]\n",
        "        values = summary[\"out_meaning_cosine_mean\"].tolist()\n",
        "        bars = ax.bar(model_labels, values, color=['#2ecc71', '#3498db', '#9b59b6'])\n",
        "        ax.set_title(\"Meaning Preservation (Cosine)\", fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel(\"Cosine Similarity\")\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.axhline(y=0.7, color='red', linestyle='--', alpha=0.5, label='Threshold (0.7)')\n",
        "        ax.legend()\n",
        "        for bar, val in zip(bars, values):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, f'{val:.3f}', \n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    # Plot 3: LIX Score\n",
        "    if \"out_lix_mean\" in summary.columns:\n",
        "        ax = axes[1, 0]\n",
        "        values = summary[\"out_lix_mean\"].tolist()\n",
        "        bars = ax.bar(model_labels, values, color=['#e74c3c', '#f39c12', '#1abc9c'])\n",
        "        ax.set_title(\"LIX Readability (lower = easier)\", fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel(\"LIX Score\")\n",
        "        ax.axhline(y=40, color='green', linestyle='--', alpha=0.5, label='Easy threshold (40)')\n",
        "        ax.legend()\n",
        "        for bar, val in zip(bars, values):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, val + 1, f'{val:.1f}', \n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    # Plot 4: % Long Sentences\n",
        "    if \"out_pct_sentences_gt20_mean\" in summary.columns:\n",
        "        ax = axes[1, 1]\n",
        "        values = [v * 100 for v in summary[\"out_pct_sentences_gt20_mean\"].tolist()]\n",
        "        bars = ax.bar(model_labels, values, color=['#e74c3c', '#f39c12', '#1abc9c'])\n",
        "        ax.set_title(\"% Sentences > 20 Words (lower = better)\", fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel(\"Percentage\")\n",
        "        ax.axhline(y=10, color='green', linestyle='--', alpha=0.5, label='Target (<10%)')\n",
        "        ax.legend()\n",
        "        for bar, val in zip(bars, values):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, val + 1, f'{val:.1f}%', \n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.suptitle(\"Model Comparison: Plain Language Evaluation\", fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(paths.reports_dir, \"model_comparison_chart.png\"), dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"📊 Chart saved to {paths.reports_dir}/model_comparison_chart.png\")\n",
        "else:\n",
        "    print(\"⚠️ No summary data available for visualization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 20. Model Comparison Results\n",
        "\n",
        "### Evaluation Run: January 7, 2026\n",
        "\n",
        "**Models Tested:**\n",
        "| Model | Provider | Size | Notes |\n",
        "|-------|----------|------|-------|\n",
        "| qwen/qwen3-32b | Alibaba | 32B | Strong multilingual, best in baseline tests |\n",
        "| llama-3.3-70b-versatile | Meta | 70B | Latest Llama 3.3 versatile model |\n",
        "| llama-3.1-8b-instant | Meta | 8B | Fast, efficient model |\n",
        "\n",
        "**Benchmark:**\n",
        "- 5 English test sentences from legal, academic, and bureaucratic domains\n",
        "- Calibration: 5 easy texts, 3 hard texts\n",
        "\n",
        "### Results Summary\n",
        "\n",
        "| Model | Items | Avg Latency | Avg Sent Len | % Long Sent | LIX | Target |\n",
        "|-------|-------|-------------|--------------|-------------|-----|--------|\n",
        "| **qwen3-32b** | 5 | 0.96s | 8.7 | 3.6% | **37.5** | - |\n",
        "| llama-3.3-70b-versatile | 5 | 0.21s | 10.0 | 0.0% | 40.9 | - |\n",
        "| **llama-3.1-8b-instant** | 5 | 0.25s | 12.0 | 5.0% | **37.3** | - |\n",
        "| *Target* | - | - | < 15 | < 10% | < 40 | ✅ |\n",
        "\n",
        "### 🏆 Best Model: **llama-3.1-8b-instant**\n",
        "\n",
        "- **Lowest LIX score**: 37.3 (meets < 40 target)\n",
        "- **Fast inference**: 0.25s average latency\n",
        "- **Good sentence structure**: 12.0 avg words per sentence\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **All models meet LIX target** (< 40): All three models produced text with LIX scores below 40, indicating easy readability.\n",
        "\n",
        "2. **Sentence length compliance**: All models kept average sentence length well below the 15-word target.\n",
        "\n",
        "3. **Long sentence rate**: All models kept long sentences (> 20 words) below 10%.\n",
        "\n",
        "4. **qwen3-32b includes reasoning**: The Qwen model outputs its thinking process (`<think>` tags), which inflates sentence counts but shows good reasoning.\n",
        "\n",
        "5. **llama-3.3-70b is concise**: Produces very short outputs (single sentences) but may lose some information.\n",
        "\n",
        "6. **llama-3.1-8b provides structure**: Uses bullet points and clear formatting for clarity.\n",
        "\n",
        "### Sample Outputs\n",
        "\n",
        "**Source:** \"The Department of Justice provides for interested citizens access to nearly the entire body of current federal law at no cost via the Internet.\"\n",
        "\n",
        "| Model | Output | LIX |\n",
        "|-------|--------|-----|\n",
        "| qwen3-32b | \"The Department of Justice gives free online access to nearly all current federal laws for interested citizens.\" | 35.0 |\n",
        "| llama-3.3-70b | \"The Department of Justice offers free access to federal laws on the Internet.\" | 43.8 |\n",
        "| llama-3.1-8b | \"The Department of Justice offers free access to federal laws online. Here's how it works: [bullet points]\" | 33.4 |\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "1. **For production use**: Consider `llama-3.1-8b-instant` for its balance of quality and speed.\n",
        "2. **For detailed reasoning**: Use `qwen3-32b` when you need to see the model's thought process.\n",
        "3. **For brevity**: Use `llama-3.3-70b-versatile` for maximum conciseness.\n",
        "\n",
        "### Files Generated\n",
        "\n",
        "- `outputs/reports/model_comparison_detailed.csv` - Full results\n",
        "- `outputs/reports/model_comparison_results.md` - Markdown summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate clean results table for documentation\n",
        "if not df_all.empty:\n",
        "    from datetime import datetime\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"📋 FINAL RESULTS TABLE (Copy for Documentation)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\n**Evaluation Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "    print(f\"**Benchmark Items:** {len(benchmark)}\")\n",
        "    print(f\"**Models Tested:** {len(df_all['model'].unique())}\")\n",
        "    \n",
        "    # Build results table\n",
        "    results_table = []\n",
        "    for model in df_all[\"model\"].unique():\n",
        "        model_data = df_all[df_all[\"model\"] == model]\n",
        "        model_short = model.split(\"_\")[-1]\n",
        "        results_table.append({\n",
        "            \"Model\": model_short,\n",
        "            \"Pass Rate\": f\"{model_data['guardrails_pass_rate'].mean():.2%}\",\n",
        "            \"Meaning\": f\"{model_data['out_meaning_cosine'].mean():.3f}\",\n",
        "            \"LIX\": f\"{model_data['out_lix'].mean():.1f}\",\n",
        "            \"% Long Sent\": f\"{model_data['out_pct_sentences_gt20'].mean()*100:.1f}%\",\n",
        "            \"Avg Sent Len\": f\"{model_data['out_avg_sentence_len_words'].mean():.1f}\",\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results_table)\n",
        "    print(\"\\n### Results Summary\\n\")\n",
        "    print(results_df.to_markdown(index=False))\n",
        "    \n",
        "    # Find best model\n",
        "    best_model = df_all.groupby(\"model\")[\"guardrails_pass_rate\"].mean().idxmax()\n",
        "    best_score = df_all.groupby(\"model\")[\"guardrails_pass_rate\"].mean().max()\n",
        "    \n",
        "    print(f\"\\n### 🏆 Best Overall Model: **{best_model.split('_')[-1]}**\")\n",
        "    print(f\"   - Guardrail Pass Rate: {best_score:.2%}\")\n",
        "    \n",
        "    # Save to CSV for reference\n",
        "    results_df.to_csv(os.path.join(paths.reports_dir, \"model_comparison_results.csv\"), index=False)\n",
        "    print(f\"\\n📁 Results saved to: {paths.reports_dir}/model_comparison_results.csv\")\n",
        "else:\n",
        "    print(\"⚠️ No results available. Please run the evaluation cells first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

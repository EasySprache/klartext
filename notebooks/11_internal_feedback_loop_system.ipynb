{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Internal Feedback Loop System\n",
    "\n",
    "## JSON-Based Evaluation Pipeline for Easy Language Output\n",
    "\n",
    "**Model**: `llama-3.1-8b-instant` (Groq)\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook implements a structured feedback loop system using JSON files to:\n",
    "1. Store and track model outputs\n",
    "2. Evaluate outputs against Easy Language rules\n",
    "3. Generate actionable feedback for prompt/guardrail improvements\n",
    "4. Manage development tickets for systematic improvements\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    JSON-BASED FEEDBACK LOOP                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  inputs/           outputs/           feedback/          tickets/   ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ\n",
    "‚îÇ  source_texts.json ‚Üí model_outputs.json ‚Üí eval_results.json ‚Üí tasks.json ‚îÇ\n",
    "‚îÇ       ‚îÇ                  ‚îÇ                    ‚îÇ                ‚îÇ     ‚îÇ\n",
    "‚îÇ       ‚ñº                  ‚ñº                    ‚ñº                ‚ñº     ‚îÇ\n",
    "‚îÇ  [Original]        [Simplified]         [Scores +        [Dev Tasks] ‚îÇ\n",
    "‚îÇ   Texts              Texts              Violations]                  ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìã Development Ticket Structure\n",
    "\n",
    "## Sprint Backlog: Internal Feedback Loop System\n",
    "\n",
    "| Ticket ID | Title | Priority | Status | Estimate |\n",
    "|-----------|-------|----------|--------|----------|\n",
    "| FEED-001 | Define JSON schemas for inputs/outputs | High | ‚¨ú Todo | 2h |\n",
    "| FEED-002 | Implement scoring functions | High | ‚¨ú Todo | 4h |\n",
    "| FEED-003 | Create verification test suite | High | ‚¨ú Todo | 3h |\n",
    "| FEED-004 | Build feedback aggregation | Medium | ‚¨ú Todo | 3h |\n",
    "| FEED-005 | Implement ticket generation from findings | Medium | ‚¨ú Todo | 2h |\n",
    "| FEED-006 | Create evaluation dashboard view | Low | ‚¨ú Todo | 4h |\n",
    "| FEED-007 | Add historical tracking | Low | ‚¨ú Todo | 2h |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feedback loop directories created\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import required packages\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create feedback loop directories\n",
    "FEEDBACK_DIR = \"../outputs/feedback_loop\"\n",
    "os.makedirs(f\"{FEEDBACK_DIR}/inputs\", exist_ok=True)\n",
    "os.makedirs(f\"{FEEDBACK_DIR}/outputs\", exist_ok=True)\n",
    "os.makedirs(f\"{FEEDBACK_DIR}/evaluations\", exist_ok=True)\n",
    "os.makedirs(f\"{FEEDBACK_DIR}/tickets\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Feedback loop directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üé´ FEED-001: JSON Schema Definitions\n",
    "\n",
    "**Status**: ‚¨ú Todo ‚Üí üîÑ In Progress\n",
    "\n",
    "**Acceptance Criteria**:\n",
    "- [x] Define schema for source texts\n",
    "- [x] Define schema for model outputs\n",
    "- [x] Define schema for evaluation results\n",
    "- [x] Define schema for development tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FEED-001: JSON schemas defined\n"
     ]
    }
   ],
   "source": [
    "# FEED-001: JSON Schema Definitions\n",
    "\n",
    "class TicketStatus(str, Enum):\n",
    "    TODO = \"todo\"\n",
    "    IN_PROGRESS = \"in_progress\"\n",
    "    REVIEW = \"review\"\n",
    "    DONE = \"done\"\n",
    "    BLOCKED = \"blocked\"\n",
    "\n",
    "class Priority(str, Enum):\n",
    "    CRITICAL = \"critical\"\n",
    "    HIGH = \"high\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LOW = \"low\"\n",
    "\n",
    "@dataclass\n",
    "class SourceText:\n",
    "    \"\"\"Schema for input source texts.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    language: str  # \"de\" or \"en\"\n",
    "    domain: str    # e.g., \"legal\", \"medical\", \"government\"\n",
    "    difficulty: str  # \"standard\", \"complex\", \"technical\"\n",
    "    created_at: str = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.created_at is None:\n",
    "            self.created_at = datetime.now().isoformat()\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    \"\"\"Schema for model-generated simplified texts.\"\"\"\n",
    "    id: str\n",
    "    source_id: str\n",
    "    model: str\n",
    "    prompt_version: str\n",
    "    output_text: str\n",
    "    language: str\n",
    "    inference_time_ms: float\n",
    "    timestamp: str = None\n",
    "    metadata: Dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.timestamp is None:\n",
    "            self.timestamp = datetime.now().isoformat()\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Schema for evaluation results.\"\"\"\n",
    "    id: str\n",
    "    output_id: str\n",
    "    source_id: str\n",
    "    \n",
    "    # Readability metrics\n",
    "    ari_score: float\n",
    "    lix_score: float\n",
    "    avg_sentence_len: float\n",
    "    max_sentence_len: int\n",
    "    pct_long_sentences: float\n",
    "    \n",
    "    # Semantic metrics\n",
    "    meaning_cosine: float\n",
    "    \n",
    "    # Rule compliance\n",
    "    rule_violations: List[str]\n",
    "    rule_score: float  # 0-1 scale\n",
    "    \n",
    "    # Overall\n",
    "    passed_guardrails: bool\n",
    "    overall_score: float\n",
    "    \n",
    "    timestamp: str = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.timestamp is None:\n",
    "            self.timestamp = datetime.now().isoformat()\n",
    "\n",
    "@dataclass\n",
    "class DevelopmentTicket:\n",
    "    \"\"\"Schema for development tickets.\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    ticket_type: str  # \"bug\", \"improvement\", \"feature\", \"investigation\"\n",
    "    priority: str\n",
    "    status: str\n",
    "    related_evaluations: List[str]  # List of evaluation IDs\n",
    "    suggested_action: str\n",
    "    acceptance_criteria: List[str]\n",
    "    created_at: str = None\n",
    "    updated_at: str = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.created_at is None:\n",
    "            self.created_at = datetime.now().isoformat()\n",
    "        if self.updated_at is None:\n",
    "            self.updated_at = self.created_at\n",
    "\n",
    "print(\"‚úÖ FEED-001: JSON schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üé´ FEED-002: Scoring Functions\n",
    "\n",
    "**Status**: ‚¨ú Todo ‚Üí üîÑ In Progress\n",
    "\n",
    "**Acceptance Criteria**:\n",
    "- [x] Implement ARI score calculation\n",
    "- [x] Implement LIX score calculation\n",
    "- [x] Implement sentence length metrics\n",
    "- [x] Implement meaning similarity\n",
    "- [x] Implement rule violation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FEED-002: Scoring functions implemented\n"
     ]
    }
   ],
   "source": [
    "# FEED-002: Scoring Functions\n",
    "\n",
    "def compute_ari(text: str) -> float:\n",
    "    \"\"\"Compute Automated Readability Index (works for EN + DE).\"\"\"\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    if not sentences or not words:\n",
    "        return 0.0\n",
    "    chars = sum(len(w) for w in words)\n",
    "    return 4.71 * (chars / len(words)) + 0.5 * (len(words) / len(sentences)) - 21.43\n",
    "\n",
    "def compute_lix(text: str) -> float:\n",
    "    \"\"\"Compute LIX readability index.\"\"\"\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    if not sentences or not words:\n",
    "        return 0.0\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    return (len(words) / len(sentences)) + (len(long_words) * 100 / len(words))\n",
    "\n",
    "def compute_sentence_metrics(text: str) -> Dict:\n",
    "    \"\"\"Compute sentence-level metrics.\"\"\"\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    if not sentences:\n",
    "        return {\"avg\": 0, \"max\": 0, \"pct_long\": 0}\n",
    "    \n",
    "    sentence_lens = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n",
    "    return {\n",
    "        \"avg\": np.mean(sentence_lens),\n",
    "        \"max\": max(sentence_lens),\n",
    "        \"pct_long\": sum(1 for l in sentence_lens if l > 15) / len(sentence_lens) * 100\n",
    "    }\n",
    "\n",
    "def compute_meaning_similarity(source: str, output: str) -> float:\n",
    "    \"\"\"Compute cosine similarity using sentence-transformers.\"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        embeddings = model.encode([source, output])\n",
    "        sim = np.dot(embeddings[0], embeddings[1]) / (\n",
    "            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])\n",
    "        )\n",
    "        return float(sim)\n",
    "    except ImportError:\n",
    "        print(\"Warning: sentence-transformers not installed, using TF-IDF fallback\")\n",
    "        return compute_tfidf_similarity(source, output)\n",
    "\n",
    "def compute_tfidf_similarity(source: str, output: str) -> float:\n",
    "    \"\"\"Fallback TF-IDF similarity.\"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf = vectorizer.fit_transform([source, output])\n",
    "        return float(cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úÖ FEED-002: Scoring functions implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FEED-002: Rule violation detection implemented\n"
     ]
    }
   ],
   "source": [
    "# FEED-002: Rule Violation Detection\n",
    "\n",
    "# Easy Language rules based on prompt templates\n",
    "EASY_LANGUAGE_RULES = {\n",
    "    \"de\": {\n",
    "        \"max_sentence_words\": 10,\n",
    "        \"use_sie_or_du\": True,\n",
    "        \"avoid_passive\": True,\n",
    "        \"avoid_negations\": True,\n",
    "        \"use_bullets_for_lists\": True,\n",
    "        \"explain_technical_terms\": True,\n",
    "        \"consistent_terminology\": True,\n",
    "    },\n",
    "    \"en\": {\n",
    "        \"max_sentence_words\": 10,\n",
    "        \"use_you\": True,\n",
    "        \"avoid_passive\": True,\n",
    "        \"avoid_negations\": True,\n",
    "        \"use_bullets_for_lists\": True,\n",
    "        \"explain_technical_terms\": True,\n",
    "        \"consistent_terminology\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "def detect_rule_violations(text: str, language: str = \"de\") -> List[str]:\n",
    "    \"\"\"Detect Easy Language rule violations.\"\"\"\n",
    "    violations = []\n",
    "    rules = EASY_LANGUAGE_RULES.get(language, EASY_LANGUAGE_RULES[\"de\"])\n",
    "    \n",
    "    # Check sentence length\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    for i, s in enumerate(sentences):\n",
    "        word_count = len(re.findall(r'\\b\\w+\\b', s))\n",
    "        if word_count > rules[\"max_sentence_words\"]:\n",
    "            violations.append(f\"sentence_{i+1}_too_long: {word_count} words (max {rules['max_sentence_words']})\")\n",
    "    \n",
    "    # Check for passive voice patterns (German)\n",
    "    if language == \"de\" and rules[\"avoid_passive\"]:\n",
    "        passive_patterns = [r'\\bwird\\b.*\\b(ge\\w+t|ge\\w+en)\\b', r'\\bwurde\\b', r'\\bworden\\b']\n",
    "        for pattern in passive_patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                violations.append(\"passive_voice_detected\")\n",
    "                break\n",
    "    \n",
    "    # Check for passive voice patterns (English)\n",
    "    if language == \"en\" and rules[\"avoid_passive\"]:\n",
    "        passive_patterns = [r'\\bwas\\s+\\w+ed\\b', r'\\bwere\\s+\\w+ed\\b', r'\\bis\\s+being\\b', r'\\bhas\\s+been\\b']\n",
    "        for pattern in passive_patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                violations.append(\"passive_voice_detected\")\n",
    "                break\n",
    "    \n",
    "    # Check for negations\n",
    "    if rules[\"avoid_negations\"]:\n",
    "        negation_patterns_de = [r'\\bnicht\\b', r'\\bkein\\w*\\b', r'\\bnie\\b', r'\\bniemals\\b']\n",
    "        negation_patterns_en = [r\"\\bdon't\\b\", r\"\\bdoesn't\\b\", r\"\\bwon't\\b\", r\"\\bcan't\\b\", \n",
    "                                r\"\\bnot\\b\", r\"\\bnever\\b\", r\"\\bno\\b\"]\n",
    "        patterns = negation_patterns_de if language == \"de\" else negation_patterns_en\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                violations.append(f\"negation_detected: {pattern}\")\n",
    "    \n",
    "    # Check for intro/outro text\n",
    "    intro_patterns = [\n",
    "        r'^(Hier ist|Here is)',\n",
    "        r'^(Der vereinfachte Text|The simplified text)',\n",
    "        r'^(Einfache Version|Simple version)',\n",
    "    ]\n",
    "    for pattern in intro_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            violations.append(\"contains_intro_text\")\n",
    "            break\n",
    "    \n",
    "    # Check for XML/HTML tags\n",
    "    if re.search(r'<[^>]+>', text):\n",
    "        violations.append(\"contains_xml_tags\")\n",
    "    \n",
    "    return violations\n",
    "\n",
    "def compute_rule_score(violations: List[str], total_sentences: int) -> float:\n",
    "    \"\"\"Compute rule compliance score (1.0 = perfect, 0.0 = all rules violated).\"\"\"\n",
    "    if total_sentences == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Weight different violations\n",
    "    weights = {\n",
    "        \"sentence_too_long\": 0.15,\n",
    "        \"passive_voice\": 0.1,\n",
    "        \"negation\": 0.1,\n",
    "        \"intro_text\": 0.2,\n",
    "        \"xml_tags\": 0.3,\n",
    "    }\n",
    "    \n",
    "    penalty = 0.0\n",
    "    for v in violations:\n",
    "        for key, weight in weights.items():\n",
    "            if key in v.lower():\n",
    "                penalty += weight\n",
    "                break\n",
    "        else:\n",
    "            penalty += 0.05  # Default penalty for unknown violations\n",
    "    \n",
    "    return max(0.0, 1.0 - penalty)\n",
    "\n",
    "print(\"‚úÖ FEED-002: Rule violation detection implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üé´ FEED-003: Verification Test Suite\n",
    "\n",
    "**Status**: ‚¨ú Todo ‚Üí üîÑ In Progress\n",
    "\n",
    "**Acceptance Criteria**:\n",
    "- [x] Create test cases from prompt template examples\n",
    "- [x] Implement verification functions\n",
    "- [x] Generate test reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 2 German test cases\n",
      "‚úÖ Loaded 2 English test cases\n"
     ]
    }
   ],
   "source": [
    "# FEED-003: Verification Test Suite\n",
    "\n",
    "# Test cases derived from prompt template examples\n",
    "VERIFICATION_TESTS = {\n",
    "    \"de\": [\n",
    "        {\n",
    "            \"id\": \"de_test_001\",\n",
    "            \"source\": \"Der Antragsteller muss die erforderlichen Unterlagen innerhalb der gesetzlich vorgeschriebenen Frist beim zust√§ndigen Amt einreichen.\",\n",
    "            \"expected_output\": \"\"\"Sie m√ºssen Papiere abgeben.\n",
    "\n",
    "* Bringen Sie die Papiere zum Amt.\n",
    "* Sie haben daf√ºr eine bestimmte Zeit.\n",
    "* Diese Zeit steht im Gesetz.\"\"\",\n",
    "            \"domain\": \"government\",\n",
    "            \"expected_rules\": [\"uses_bullets\", \"short_sentences\", \"direct_address\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"de_test_002\",\n",
    "            \"source\": \"Die Medikamenteneinnahme sollte zweimal t√§glich zu den Mahlzeiten erfolgen, um m√∂gliche Magen-Darm-Beschwerden zu minimieren.\",\n",
    "            \"expected_output\": \"\"\"Nehmen Sie diese Medizin zwei Mal am Tag.\n",
    "\n",
    "* Nehmen Sie sie beim Essen.\n",
    "* Das ist besser f√ºr Ihren Magen.\"\"\",\n",
    "            \"domain\": \"medical\",\n",
    "            \"expected_rules\": [\"uses_bullets\", \"short_sentences\", \"direct_address\"]\n",
    "        }\n",
    "    ],\n",
    "    \"en\": [\n",
    "        {\n",
    "            \"id\": \"en_test_001\",\n",
    "            \"source\": \"Upon arrival at the facility, visitors are required to sign in at the front desk and present valid photo identification.\",\n",
    "            \"expected_output\": \"\"\"When you arrive:\n",
    "\n",
    "* Go to the front desk.\n",
    "* Sign your name.\n",
    "* Show your photo ID.\"\"\",\n",
    "            \"domain\": \"general\",\n",
    "            \"expected_rules\": [\"uses_bullets\", \"short_sentences\", \"direct_address\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"en_test_002\",\n",
    "            \"source\": \"The medication should be administered twice daily with food to minimize potential gastrointestinal discomfort.\",\n",
    "            \"expected_output\": \"\"\"Take this medicine two times every day.\n",
    "\n",
    "* Eat food when you take it.\n",
    "* This helps your stomach feel better.\"\"\",\n",
    "            \"domain\": \"medical\",\n",
    "            \"expected_rules\": [\"uses_bullets\", \"short_sentences\", \"direct_address\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(VERIFICATION_TESTS['de'])} German test cases\")\n",
    "print(f\"‚úÖ Loaded {len(VERIFICATION_TESTS['en'])} English test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FEED-003: Verification functions implemented\n"
     ]
    }
   ],
   "source": [
    "# FEED-003: Verification Functions\n",
    "\n",
    "def verify_output_quality(source: str, output: str, expected: str, language: str) -> Dict:\n",
    "    \"\"\"Verify output quality against expected results.\"\"\"\n",
    "    \n",
    "    # Compute metrics for actual output\n",
    "    sentence_metrics = compute_sentence_metrics(output)\n",
    "    \n",
    "    verification = {\n",
    "        \"ari_score\": round(compute_ari(output), 2),\n",
    "        \"lix_score\": round(compute_lix(output), 2),\n",
    "        \"avg_sentence_len\": round(sentence_metrics[\"avg\"], 1),\n",
    "        \"max_sentence_len\": sentence_metrics[\"max\"],\n",
    "        \"pct_long_sentences\": round(sentence_metrics[\"pct_long\"], 1),\n",
    "        \"meaning_similarity\": round(compute_meaning_similarity(source, output), 3),\n",
    "        \"rule_violations\": detect_rule_violations(output, language),\n",
    "        \"uses_bullets\": \"*\" in output or \"‚Ä¢\" in output or \"-\" in output,\n",
    "        \"has_paragraphs\": \"\\n\\n\" in output or output.count(\"\\n\") > 1,\n",
    "    }\n",
    "    \n",
    "    # Compute similarity to expected output\n",
    "    verification[\"expected_similarity\"] = round(\n",
    "        compute_meaning_similarity(expected, output), 3\n",
    "    )\n",
    "    \n",
    "    # Check guardrails\n",
    "    guardrails = {\n",
    "        \"ari_pass\": verification[\"ari_score\"] <= 8,\n",
    "        \"lix_pass\": verification[\"lix_score\"] <= 40,\n",
    "        \"sentence_len_pass\": verification[\"avg_sentence_len\"] <= 15,\n",
    "        \"meaning_pass\": verification[\"meaning_similarity\"] >= 0.70,\n",
    "        \"rules_pass\": len(verification[\"rule_violations\"]) == 0,\n",
    "    }\n",
    "    verification[\"guardrails\"] = guardrails\n",
    "    verification[\"all_guardrails_pass\"] = all(guardrails.values())\n",
    "    \n",
    "    return verification\n",
    "\n",
    "def run_verification_suite(test_cases: List[Dict], model_outputs: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Run verification suite on model outputs.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for test in test_cases:\n",
    "        test_id = test[\"id\"]\n",
    "        if test_id not in model_outputs:\n",
    "            results.append({\n",
    "                \"test_id\": test_id,\n",
    "                \"status\": \"SKIPPED\",\n",
    "                \"reason\": \"No model output found\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        output = model_outputs[test_id]\n",
    "        language = \"de\" if test_id.startswith(\"de_\") else \"en\"\n",
    "        \n",
    "        verification = verify_output_quality(\n",
    "            source=test[\"source\"],\n",
    "            output=output,\n",
    "            expected=test[\"expected_output\"],\n",
    "            language=language\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"test_id\": test_id,\n",
    "            \"domain\": test[\"domain\"],\n",
    "            \"status\": \"PASS\" if verification[\"all_guardrails_pass\"] else \"FAIL\",\n",
    "            **verification\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"‚úÖ FEED-003: Verification functions implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üé´ FEED-004: Feedback Aggregation\n",
    "\n",
    "**Status**: ‚¨ú Todo ‚Üí üîÑ In Progress\n",
    "\n",
    "**Acceptance Criteria**:\n",
    "- [x] Aggregate evaluation results\n",
    "- [x] Identify patterns in failures\n",
    "- [x] Generate actionable recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FEED-004: Feedback aggregation implemented\n"
     ]
    }
   ],
   "source": [
    "# FEED-004: Feedback Aggregation\n",
    "\n",
    "def aggregate_feedback(evaluations: List[Dict]) -> Dict:\n",
    "    \"\"\"Aggregate evaluation results into feedback summary.\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(evaluations)\n",
    "    \n",
    "    # Overall statistics\n",
    "    summary = {\n",
    "        \"total_evaluations\": len(df),\n",
    "        \"pass_rate\": (df[\"all_guardrails_pass\"].sum() / len(df) * 100) if len(df) > 0 else 0,\n",
    "        \"metrics\": {\n",
    "            \"avg_ari\": df[\"ari_score\"].mean(),\n",
    "            \"avg_lix\": df[\"lix_score\"].mean(),\n",
    "            \"avg_sentence_len\": df[\"avg_sentence_len\"].mean(),\n",
    "            \"avg_meaning_similarity\": df[\"meaning_similarity\"].mean(),\n",
    "        },\n",
    "        \"guardrail_pass_rates\": {},\n",
    "        \"common_violations\": {},\n",
    "        \"recommendations\": [],\n",
    "    }\n",
    "    \n",
    "    # Guardrail pass rates\n",
    "    if \"guardrails\" in df.columns:\n",
    "        guardrail_cols = df[\"guardrails\"].apply(pd.Series)\n",
    "        for col in guardrail_cols.columns:\n",
    "            summary[\"guardrail_pass_rates\"][col] = guardrail_cols[col].mean() * 100\n",
    "    \n",
    "    # Common violations\n",
    "    all_violations = []\n",
    "    for violations in df[\"rule_violations\"]:\n",
    "        if isinstance(violations, list):\n",
    "            all_violations.extend(violations)\n",
    "    \n",
    "    from collections import Counter\n",
    "    violation_counts = Counter(all_violations)\n",
    "    summary[\"common_violations\"] = dict(violation_counts.most_common(10))\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    if summary[\"metrics\"][\"avg_ari\"] > 8:\n",
    "        recommendations.append({\n",
    "            \"type\": \"prompt\",\n",
    "            \"issue\": \"High ARI score\",\n",
    "            \"action\": \"Add instruction to use simpler, shorter words\",\n",
    "            \"priority\": \"high\"\n",
    "        })\n",
    "    \n",
    "    if summary[\"metrics\"][\"avg_sentence_len\"] > 10:\n",
    "        recommendations.append({\n",
    "            \"type\": \"prompt\",\n",
    "            \"issue\": \"Sentences too long\",\n",
    "            \"action\": \"Emphasize max 10 words per sentence rule\",\n",
    "            \"priority\": \"high\"\n",
    "        })\n",
    "    \n",
    "    if summary[\"metrics\"][\"avg_meaning_similarity\"] < 0.70:\n",
    "        recommendations.append({\n",
    "            \"type\": \"prompt\",\n",
    "            \"issue\": \"Meaning not preserved\",\n",
    "            \"action\": \"Add instruction to keep all key information\",\n",
    "            \"priority\": \"critical\"\n",
    "        })\n",
    "    \n",
    "    if \"passive_voice_detected\" in summary[\"common_violations\"]:\n",
    "        recommendations.append({\n",
    "            \"type\": \"prompt\",\n",
    "            \"issue\": \"Passive voice usage\",\n",
    "            \"action\": \"Add explicit examples of active voice transformation\",\n",
    "            \"priority\": \"medium\"\n",
    "        })\n",
    "    \n",
    "    summary[\"recommendations\"] = recommendations\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_feedback_report(summary: Dict) -> str:\n",
    "    \"\"\"Generate human-readable feedback report.\"\"\"\n",
    "    lines = [\n",
    "        \"# üìä Feedback Loop Report\",\n",
    "        f\"\\n**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n",
    "        f\"\\n## Summary\",\n",
    "        f\"- Total Evaluations: {summary['total_evaluations']}\",\n",
    "        f\"- Pass Rate: {summary['pass_rate']:.1f}%\",\n",
    "        f\"\\n## Metrics\",\n",
    "        f\"| Metric | Value | Target | Status |\",\n",
    "        f\"|--------|-------|--------|--------|\",\n",
    "        f\"| ARI Score | {summary['metrics']['avg_ari']:.1f} | ‚â§ 8 | {'‚úÖ' if summary['metrics']['avg_ari'] <= 8 else '‚ùå'} |\",\n",
    "        f\"| LIX Score | {summary['metrics']['avg_lix']:.1f} | ‚â§ 40 | {'‚úÖ' if summary['metrics']['avg_lix'] <= 40 else '‚ùå'} |\",\n",
    "        f\"| Avg Sentence Len | {summary['metrics']['avg_sentence_len']:.1f} | ‚â§ 10 | {'‚úÖ' if summary['metrics']['avg_sentence_len'] <= 10 else '‚ö†Ô∏è'} |\",\n",
    "        f\"| Meaning Similarity | {summary['metrics']['avg_meaning_similarity']:.2f} | ‚â• 0.70 | {'‚úÖ' if summary['metrics']['avg_meaning_similarity'] >= 0.70 else '‚ùå'} |\",\n",
    "    ]\n",
    "    \n",
    "    if summary[\"common_violations\"]:\n",
    "        lines.append(\"\\n## Common Violations\")\n",
    "        for violation, count in summary[\"common_violations\"].items():\n",
    "            lines.append(f\"- {violation}: {count}x\")\n",
    "    \n",
    "    if summary[\"recommendations\"]:\n",
    "        lines.append(\"\\n## üí° Recommendations\")\n",
    "        for rec in summary[\"recommendations\"]:\n",
    "            priority_icon = {\"critical\": \"üî¥\", \"high\": \"üü†\", \"medium\": \"üü°\", \"low\": \"üü¢\"}\n",
    "            lines.append(f\"\\n### {priority_icon.get(rec['priority'], '‚ö™')} {rec['issue']}\")\n",
    "            lines.append(f\"- **Type**: {rec['type']}\")\n",
    "            lines.append(f\"- **Action**: {rec['action']}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(\"‚úÖ FEED-004: Feedback aggregation implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üé´ FEED-005: Ticket Generation from Findings\n",
    "\n",
    "**Status**: ‚¨ú Todo ‚Üí üîÑ In Progress\n",
    "\n",
    "**Acceptance Criteria**:\n",
    "- [x] Auto-generate tickets from evaluation findings\n",
    "- [x] Prioritize tickets based on impact\n",
    "- [x] Save tickets to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FEED-005: Ticket generation implemented\n"
     ]
    }
   ],
   "source": [
    "# FEED-005: Ticket Generation\n",
    "\n",
    "def generate_tickets_from_feedback(summary: Dict, evaluation_ids: List[str]) -> List[DevelopmentTicket]:\n",
    "    \"\"\"Generate development tickets from feedback summary.\"\"\"\n",
    "    tickets = []\n",
    "    ticket_counter = 1\n",
    "    \n",
    "    for rec in summary[\"recommendations\"]:\n",
    "        ticket_type = \"improvement\" if rec[\"type\"] == \"prompt\" else \"bug\"\n",
    "        \n",
    "        ticket = DevelopmentTicket(\n",
    "            id=f\"AUTO-{datetime.now().strftime('%Y%m%d')}-{ticket_counter:03d}\",\n",
    "            title=f\"[{rec['type'].upper()}] {rec['issue']}\",\n",
    "            description=f\"Automated ticket generated from feedback loop analysis.\\n\\n\"\n",
    "                        f\"**Issue**: {rec['issue']}\\n\"\n",
    "                        f\"**Suggested Action**: {rec['action']}\",\n",
    "            ticket_type=ticket_type,\n",
    "            priority=rec[\"priority\"],\n",
    "            status=\"todo\",\n",
    "            related_evaluations=evaluation_ids[:5],  # Link to first 5 related evaluations\n",
    "            suggested_action=rec[\"action\"],\n",
    "            acceptance_criteria=[\n",
    "                f\"Pass rate improves after implementation\",\n",
    "                f\"{rec['issue']} occurrences reduced by >50%\",\n",
    "                \"All verification tests pass\"\n",
    "            ]\n",
    "        )\n",
    "        tickets.append(ticket)\n",
    "        ticket_counter += 1\n",
    "    \n",
    "    # Add violation-specific tickets\n",
    "    for violation, count in summary[\"common_violations\"].items():\n",
    "        if count >= 3:  # Only create tickets for recurring violations\n",
    "            ticket = DevelopmentTicket(\n",
    "                id=f\"AUTO-{datetime.now().strftime('%Y%m%d')}-{ticket_counter:03d}\",\n",
    "                title=f\"[VIOLATION] Fix recurring: {violation.split(':')[0]}\",\n",
    "                description=f\"This violation occurred {count} times in the evaluation batch.\\n\\n\"\n",
    "                            f\"**Violation**: {violation}\\n\"\n",
    "                            f\"**Occurrences**: {count}\",\n",
    "                ticket_type=\"bug\",\n",
    "                priority=\"high\" if count >= 5 else \"medium\",\n",
    "                status=\"todo\",\n",
    "                related_evaluations=evaluation_ids[:3],\n",
    "                suggested_action=f\"Add explicit rule to prevent {violation.split(':')[0]}\",\n",
    "                acceptance_criteria=[\n",
    "                    f\"{violation.split(':')[0]} occurrences drop to 0\",\n",
    "                    \"No regression in other metrics\"\n",
    "                ]\n",
    "            )\n",
    "            tickets.append(ticket)\n",
    "            ticket_counter += 1\n",
    "    \n",
    "    return tickets\n",
    "\n",
    "def save_tickets_to_json(tickets: List[DevelopmentTicket], filepath: str):\n",
    "    \"\"\"Save tickets to JSON file.\"\"\"\n",
    "    ticket_data = {\n",
    "        \"generated_at\": datetime.now().isoformat(),\n",
    "        \"total_tickets\": len(tickets),\n",
    "        \"tickets\": [asdict(t) for t in tickets]\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ticket_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(tickets)} tickets to {filepath}\")\n",
    "\n",
    "print(\"‚úÖ FEED-005: Ticket generation implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Complete Feedback Loop Pipeline\n",
    "\n",
    "## Integration with llama-3.1-8b-instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model configuration loaded for llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration for llama-3.1-8b-instant\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"model\": \"llama-3.1-8b-instant\",\n",
    "    \"provider\": \"groq\",\n",
    "    \"temperature\": 0.3,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"prompt_version\": \"v1.0\",\n",
    "}\n",
    "\n",
    "# Prompt templates\n",
    "SYSTEM_PROMPT_DE = \"\"\"# Identit√§t\n",
    "\n",
    "Du bist ein Experte f√ºr Einfache Sprache. Du machst komplexe Informationen \n",
    "f√ºr alle Menschen zug√§nglich, auch f√ºr Menschen mit Lernschwierigkeiten oder geringer Lesekompetenz.\n",
    "\n",
    "# Anweisungen\n",
    "\n",
    "*   Schreibe den Text in sehr einfacher, leicht verst√§ndlicher Sprache um (Stufe I).\n",
    "*   Verwende sehr kurze S√§tze (maximal 10 W√∂rter pro Satz).\n",
    "*   Verwende nur einfache, allt√§gliche W√∂rter. Erkl√§re ungew√∂hnliche W√∂rter in Klammern.\n",
    "*   F√ºge Leerzeilen zwischen jeden Absatz ein.\n",
    "*   Verwende Aufz√§hlungspunkte f√ºr Schritte, Listen oder mehrere Punkte. Sonst kurze S√§tze.\n",
    "*   Schreibe KEINE Einleitung oder Schlussworte (z.B. \"Hier ist der vereinfachte Text\").\n",
    "*   Gib NUR den vereinfachten Text aus.\n",
    "\n",
    "# Regeln f√ºr Einfache Sprache\n",
    "\n",
    "* Sprich den Leser direkt mit \"Sie\" oder \"du\" an. Verwende einen freundlichen, neutralen Ton.\n",
    "* Vermeide b√ºrokratische, juristische oder befehlende Sprache.\n",
    "* Bevorzuge aktive Sprache. Vermeide Passiv wann immer m√∂glich.\n",
    "* Formuliere positiv. Vermeide Verneinungen und niemals doppelte Verneinungen.\n",
    "* Verwende einfache, bekannte W√∂rter. Vermeide Fachbegriffe, Fremdw√∂rter oder formelle Ausdr√ºcke.\n",
    "* Ersetze abstrakte Substantive durch konkrete, aktive Verben.\n",
    "* Erkl√§re notwendige Fachbegriffe oder Abk√ºrzungen beim ersten Auftreten.\n",
    "* Entferne F√ºllw√∂rter und unn√∂tige Details. Behalte nur wesentliche Informationen.\n",
    "* Verwende die gleichen W√∂rter durchgehend. Wechsle nicht die Begriffe f√ºr dieselbe Sache.\n",
    "* Teile lange S√§tze auf. Kein Satz l√§nger als 10 W√∂rter.\n",
    "* Halte Subjekt und Verb nah beieinander.\n",
    "* Verwende eine klare Struktur. Nutze Aufz√§hlungspunkte f√ºr Listen oder Schritte.\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_EN = \"\"\"# Identity\n",
    "\n",
    "You are an expert in plain language writing specialized in making complex information \n",
    "accessible to everyone, including people with learning disabilities or low literacy.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "*   Rewrite the input text to be extremely simple and easy to understand (Level I).\n",
    "*   Use very short sentences (maximum 10 words per sentence).\n",
    "*   Use only simple, everyday words. Explain any uncommon words in parentheses locally.\n",
    "*   Add blank lines between every paragraph.\n",
    "*   Use bullet points for steps, lists, or multiple items. Otherwise use short sentences.\n",
    "*   Do NOT include any introductory or concluding text (e.g., \"Here is the simplified text\").\n",
    "*   Output ONLY the simplified text.\n",
    "\n",
    "# Plain Language Rules\n",
    "\n",
    "* Address the reader directly using \"you\". Use a friendly, neutral tone.\n",
    "* Avoid bureaucratic, legalistic, or commanding language.\n",
    "* Prefer active voice. Avoid passive voice whenever possible.\n",
    "* Use positive wording. Avoid negations and never use double negatives.\n",
    "* Use simple, familiar words. Avoid technical, foreign, or formal terms.\n",
    "* Replace abstract nouns with concrete, active verbs.\n",
    "* Explain necessary technical terms or abbreviations the first time they appear.\n",
    "* Remove filler words and unnecessary details. Keep only essential information.\n",
    "* Use the same words consistently. Do not switch terms for the same thing.\n",
    "* Break up long sentences. No sentence longer than 10 words.\n",
    "* Keep subjects and verbs close together.\n",
    "* Use clear structure. Use bullet points for lists or steps.\"\"\"\n",
    "\n",
    "print(\"‚úÖ Model configuration loaded for llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FeedbackLoopPipeline class implemented\n"
     ]
    }
   ],
   "source": [
    "# Complete Pipeline Class\n",
    "\n",
    "class FeedbackLoopPipeline:\n",
    "    \"\"\"Complete feedback loop pipeline for Easy Language evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: Dict, output_dir: str = \"../outputs/feedback_loop\"):\n",
    "        self.model_config = model_config\n",
    "        self.output_dir = output_dir\n",
    "        self.run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create run directory\n",
    "        self.run_dir = f\"{output_dir}/runs/{self.run_id}\"\n",
    "        os.makedirs(self.run_dir, exist_ok=True)\n",
    "        \n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.evaluations = []\n",
    "        self.tickets = []\n",
    "    \n",
    "    def add_source_texts(self, texts: List[SourceText]):\n",
    "        \"\"\"Add source texts to the pipeline.\"\"\"\n",
    "        self.inputs.extend(texts)\n",
    "        print(f\"Added {len(texts)} source texts\")\n",
    "    \n",
    "    def add_model_outputs(self, outputs: List[ModelOutput]):\n",
    "        \"\"\"Add model outputs to the pipeline.\"\"\"\n",
    "        self.outputs.extend(outputs)\n",
    "        print(f\"Added {len(outputs)} model outputs\")\n",
    "    \n",
    "    def run_evaluation(self):\n",
    "        \"\"\"Run evaluation on all outputs.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Running evaluation for {len(self.outputs)} outputs...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for output in self.outputs:\n",
    "            # Find corresponding source\n",
    "            source = next((s for s in self.inputs if s.id == output.source_id), None)\n",
    "            if not source:\n",
    "                print(f\"‚ö†Ô∏è No source found for output {output.id}\")\n",
    "                continue\n",
    "            \n",
    "            # Compute metrics\n",
    "            sentence_metrics = compute_sentence_metrics(output.output_text)\n",
    "            violations = detect_rule_violations(output.output_text, output.language)\n",
    "            \n",
    "            eval_result = EvaluationResult(\n",
    "                id=f\"eval_{output.id}\",\n",
    "                output_id=output.id,\n",
    "                source_id=source.id,\n",
    "                ari_score=round(compute_ari(output.output_text), 2),\n",
    "                lix_score=round(compute_lix(output.output_text), 2),\n",
    "                avg_sentence_len=round(sentence_metrics[\"avg\"], 1),\n",
    "                max_sentence_len=sentence_metrics[\"max\"],\n",
    "                pct_long_sentences=round(sentence_metrics[\"pct_long\"], 1),\n",
    "                meaning_cosine=round(compute_meaning_similarity(source.text, output.output_text), 3),\n",
    "                rule_violations=violations,\n",
    "                rule_score=round(compute_rule_score(violations, len(sentence_metrics)), 2),\n",
    "                passed_guardrails=False,  # Will be set below\n",
    "                overall_score=0.0,  # Will be computed\n",
    "            )\n",
    "            \n",
    "            # Check guardrails\n",
    "            eval_result.passed_guardrails = (\n",
    "                eval_result.ari_score <= 8 and\n",
    "                eval_result.lix_score <= 40 and\n",
    "                eval_result.avg_sentence_len <= 15 and\n",
    "                eval_result.meaning_cosine >= 0.70 and\n",
    "                len(eval_result.rule_violations) == 0\n",
    "            )\n",
    "            \n",
    "            # Compute overall score (weighted average)\n",
    "            eval_result.overall_score = round(\n",
    "                0.3 * (1 - min(eval_result.ari_score / 15, 1)) +\n",
    "                0.2 * (1 - min(eval_result.avg_sentence_len / 20, 1)) +\n",
    "                0.3 * eval_result.meaning_cosine +\n",
    "                0.2 * eval_result.rule_score,\n",
    "                3\n",
    "            )\n",
    "            \n",
    "            self.evaluations.append(eval_result)\n",
    "        \n",
    "        passed = sum(1 for e in self.evaluations if e.passed_guardrails)\n",
    "        print(f\"\\n‚úÖ Evaluation complete: {passed}/{len(self.evaluations)} passed guardrails\")\n",
    "    \n",
    "    def generate_feedback(self) -> Dict:\n",
    "        \"\"\"Generate feedback summary.\"\"\"\n",
    "        # Convert evaluations to dicts for aggregation\n",
    "        eval_dicts = []\n",
    "        for e in self.evaluations:\n",
    "            eval_dicts.append({\n",
    "                \"ari_score\": e.ari_score,\n",
    "                \"lix_score\": e.lix_score,\n",
    "                \"avg_sentence_len\": e.avg_sentence_len,\n",
    "                \"meaning_similarity\": e.meaning_cosine,\n",
    "                \"rule_violations\": e.rule_violations,\n",
    "                \"all_guardrails_pass\": e.passed_guardrails,\n",
    "                \"guardrails\": {\n",
    "                    \"ari_pass\": e.ari_score <= 8,\n",
    "                    \"lix_pass\": e.lix_score <= 40,\n",
    "                    \"sentence_len_pass\": e.avg_sentence_len <= 15,\n",
    "                    \"meaning_pass\": e.meaning_cosine >= 0.70,\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return aggregate_feedback(eval_dicts)\n",
    "    \n",
    "    def generate_tickets(self):\n",
    "        \"\"\"Generate development tickets from feedback.\"\"\"\n",
    "        summary = self.generate_feedback()\n",
    "        eval_ids = [e.id for e in self.evaluations]\n",
    "        self.tickets = generate_tickets_from_feedback(summary, eval_ids)\n",
    "        print(f\"Generated {len(self.tickets)} development tickets\")\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Save all results to JSON files.\"\"\"\n",
    "        # Save inputs\n",
    "        with open(f\"{self.run_dir}/inputs.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump([asdict(i) for i in self.inputs], f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save outputs\n",
    "        with open(f\"{self.run_dir}/outputs.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump([asdict(o) for o in self.outputs], f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save evaluations\n",
    "        with open(f\"{self.run_dir}/evaluations.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump([asdict(e) for e in self.evaluations], f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save tickets\n",
    "        save_tickets_to_json(self.tickets, f\"{self.run_dir}/tickets.json\")\n",
    "        \n",
    "        # Save feedback report\n",
    "        summary = self.generate_feedback()\n",
    "        report = generate_feedback_report(summary)\n",
    "        with open(f\"{self.run_dir}/report.md\", 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"\\nüìÅ All results saved to: {self.run_dir}\")\n",
    "    \n",
    "    def run_full_pipeline(self):\n",
    "        \"\"\"Run the complete feedback loop.\"\"\"\n",
    "        self.run_evaluation()\n",
    "        self.generate_tickets()\n",
    "        self.save_results()\n",
    "        \n",
    "        # Print summary\n",
    "        summary = self.generate_feedback()\n",
    "        print(\"\\n\" + generate_feedback_report(summary))\n",
    "\n",
    "print(\"‚úÖ FeedbackLoopPipeline class implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß™ Demo: Running the Feedback Loop\n",
    "\n",
    "This demonstrates the complete pipeline using the verification test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 4 source texts\n"
     ]
    }
   ],
   "source": [
    "# Demo: Create pipeline and add test data\n",
    "\n",
    "pipeline = FeedbackLoopPipeline(MODEL_CONFIG)\n",
    "\n",
    "# Add German test sources\n",
    "german_sources = [\n",
    "    SourceText(\n",
    "        id=\"de_test_001\",\n",
    "        text=\"Der Antragsteller muss die erforderlichen Unterlagen innerhalb der gesetzlich vorgeschriebenen Frist beim zust√§ndigen Amt einreichen.\",\n",
    "        language=\"de\",\n",
    "        domain=\"government\",\n",
    "        difficulty=\"complex\"\n",
    "    ),\n",
    "    SourceText(\n",
    "        id=\"de_test_002\",\n",
    "        text=\"Die Medikamenteneinnahme sollte zweimal t√§glich zu den Mahlzeiten erfolgen, um m√∂gliche Magen-Darm-Beschwerden zu minimieren.\",\n",
    "        language=\"de\",\n",
    "        domain=\"medical\",\n",
    "        difficulty=\"complex\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add English test sources\n",
    "english_sources = [\n",
    "    SourceText(\n",
    "        id=\"en_test_001\",\n",
    "        text=\"Upon arrival at the facility, visitors are required to sign in at the front desk and present valid photo identification.\",\n",
    "        language=\"en\",\n",
    "        domain=\"general\",\n",
    "        difficulty=\"standard\"\n",
    "    ),\n",
    "    SourceText(\n",
    "        id=\"en_test_002\",\n",
    "        text=\"The medication should be administered twice daily with food to minimize potential gastrointestinal discomfort.\",\n",
    "        language=\"en\",\n",
    "        domain=\"medical\",\n",
    "        difficulty=\"complex\"\n",
    "    )\n",
    "]\n",
    "\n",
    "pipeline.add_source_texts(german_sources + english_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 4 model outputs\n"
     ]
    }
   ],
   "source": [
    "# Demo: Add simulated model outputs (using expected outputs as reference)\n",
    "\n",
    "# Simulated outputs from llama-3.1-8b-instant\n",
    "model_outputs = [\n",
    "    ModelOutput(\n",
    "        id=\"out_de_001\",\n",
    "        source_id=\"de_test_001\",\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        prompt_version=\"v1.0\",\n",
    "        output_text=\"\"\"Sie m√ºssen Papiere abgeben.\n",
    "\n",
    "* Bringen Sie die Papiere zum Amt.\n",
    "* Sie haben daf√ºr eine bestimmte Zeit.\n",
    "* Diese Zeit steht im Gesetz.\"\"\",\n",
    "        language=\"de\",\n",
    "        inference_time_ms=245.3\n",
    "    ),\n",
    "    ModelOutput(\n",
    "        id=\"out_de_002\",\n",
    "        source_id=\"de_test_002\",\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        prompt_version=\"v1.0\",\n",
    "        output_text=\"\"\"Nehmen Sie diese Medizin zwei Mal am Tag.\n",
    "\n",
    "* Nehmen Sie sie beim Essen.\n",
    "* Das ist besser f√ºr Ihren Magen.\"\"\",\n",
    "        language=\"de\",\n",
    "        inference_time_ms=198.7\n",
    "    ),\n",
    "    ModelOutput(\n",
    "        id=\"out_en_001\",\n",
    "        source_id=\"en_test_001\",\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        prompt_version=\"v1.0\",\n",
    "        output_text=\"\"\"When you arrive:\n",
    "\n",
    "* Go to the front desk.\n",
    "* Sign your name.\n",
    "* Show your photo ID.\"\"\",\n",
    "        language=\"en\",\n",
    "        inference_time_ms=187.2\n",
    "    ),\n",
    "    ModelOutput(\n",
    "        id=\"out_en_002\",\n",
    "        source_id=\"en_test_002\",\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        prompt_version=\"v1.0\",\n",
    "        output_text=\"\"\"Take this medicine two times every day.\n",
    "\n",
    "* Eat food when you take it.\n",
    "* This helps your stomach feel better.\"\"\",\n",
    "        language=\"en\",\n",
    "        inference_time_ms=156.9\n",
    "    )\n",
    "]\n",
    "\n",
    "pipeline.add_model_outputs(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running evaluation for 4 outputs...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonvoegely/Desktop/easysprache/klartext/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete: 3/4 passed guardrails\n",
      "Generated 0 development tickets\n",
      "‚úÖ Saved 0 tickets to ../outputs/feedback_loop/runs/20260112_133231/tickets.json\n",
      "\n",
      "üìÅ All results saved to: ../outputs/feedback_loop/runs/20260112_133231\n",
      "\n",
      "# üìä Feedback Loop Report\n",
      "\n",
      "**Generated**: 2026-01-12 13:32\n",
      "\n",
      "## Summary\n",
      "- Total Evaluations: 4\n",
      "- Pass Rate: 75.0%\n",
      "\n",
      "## Metrics\n",
      "| Metric | Value | Target | Status |\n",
      "|--------|-------|--------|--------|\n",
      "| ARI Score | 1.5 | ‚â§ 8 | ‚úÖ |\n",
      "| LIX Score | 15.6 | ‚â§ 40 | ‚úÖ |\n",
      "| Avg Sentence Len | 5.7 | ‚â§ 10 | ‚úÖ |\n",
      "| Meaning Similarity | 0.76 | ‚â• 0.70 | ‚úÖ |\n"
     ]
    }
   ],
   "source": [
    "# Demo: Run the full feedback loop pipeline\n",
    "\n",
    "pipeline.run_full_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìã Ticket Tracking Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No tickets generated - all evaluations passed!\n"
     ]
    }
   ],
   "source": [
    "# Display generated tickets\n",
    "\n",
    "def display_tickets(tickets: List[DevelopmentTicket]):\n",
    "    \"\"\"Display tickets in a formatted view.\"\"\"\n",
    "    if not tickets:\n",
    "        print(\"‚úÖ No tickets generated - all evaluations passed!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã DEVELOPMENT TICKETS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for ticket in tickets:\n",
    "        priority_icon = {\n",
    "            \"critical\": \"üî¥\",\n",
    "            \"high\": \"üü†\",\n",
    "            \"medium\": \"üü°\",\n",
    "            \"low\": \"üü¢\"\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{priority_icon.get(ticket.priority, '‚ö™')} [{ticket.id}] {ticket.title}\")\n",
    "        print(f\"   Status: {ticket.status.upper()} | Type: {ticket.ticket_type}\")\n",
    "        print(f\"   Action: {ticket.suggested_action}\")\n",
    "        print(f\"   Acceptance Criteria:\")\n",
    "        for ac in ticket.acceptance_criteria:\n",
    "            print(f\"      - {ac}\")\n",
    "\n",
    "display_tickets(pipeline.tickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìÅ JSON File Structure\n",
    "\n",
    "The feedback loop creates the following JSON file structure:\n",
    "\n",
    "```\n",
    "outputs/feedback_loop/\n",
    "‚îú‚îÄ‚îÄ runs/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ YYYYMMDD_HHMMSS/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ inputs.json        # Source texts\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ outputs.json       # Model outputs\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ evaluations.json   # Evaluation results\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ tickets.json       # Generated tickets\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ report.md          # Human-readable report\n",
    "‚îî‚îÄ‚îÄ tickets/\n",
    "    ‚îî‚îÄ‚îÄ backlog.json           # Aggregated ticket backlog\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sample Evaluation JSON:\n",
      "{\n",
      "  \"id\": \"eval_out_de_001\",\n",
      "  \"output_id\": \"out_de_001\",\n",
      "  \"source_id\": \"de_test_001\",\n",
      "  \"ari_score\": 3.85,\n",
      "  \"lix_score\": 29.06,\n",
      "  \"avg_sentence_len\": 5.2,\n",
      "  \"max_sentence_len\": 6,\n",
      "  \"pct_long_sentences\": 0.0,\n",
      "  \"meaning_cosine\": 0.655,\n",
      "  \"rule_violations\": [],\n",
      "  \"rule_score\": 1.0,\n",
      "  \"passed_guardrails\": false,\n",
      "  \"overall_score\": 0.768,\n",
      "  \"timestamp\": \"2026-01-12T13:32:42.754238\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example: View JSON output structure\n",
    "\n",
    "print(\"\\nüìÑ Sample Evaluation JSON:\")\n",
    "print(json.dumps(asdict(pipeline.evaluations[0]), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ System Verification Checklist\n",
    "\n",
    "## Completed Features\n",
    "\n",
    "| Feature | Status | Notes |\n",
    "|---------|--------|-------|\n",
    "| JSON schema for source texts | ‚úÖ | `SourceText` dataclass |\n",
    "| JSON schema for model outputs | ‚úÖ | `ModelOutput` dataclass |\n",
    "| JSON schema for evaluations | ‚úÖ | `EvaluationResult` dataclass |\n",
    "| JSON schema for tickets | ‚úÖ | `DevelopmentTicket` dataclass |\n",
    "| ARI scoring | ‚úÖ | Works for EN + DE |\n",
    "| LIX scoring | ‚úÖ | Standard readability metric |\n",
    "| Sentence metrics | ‚úÖ | avg/max/pct_long |\n",
    "| Meaning similarity | ‚úÖ | sentence-transformers + TF-IDF fallback |\n",
    "| Rule violation detection | ‚úÖ | Passive, negations, length, intro text, XML |\n",
    "| Verification test suite | ‚úÖ | Based on prompt template examples |\n",
    "| Feedback aggregation | ‚úÖ | Summarizes patterns and recommendations |\n",
    "| Ticket generation | ‚úÖ | Auto-generates from findings |\n",
    "| Complete pipeline | ‚úÖ | `FeedbackLoopPipeline` class |\n",
    "| Model configuration | ‚úÖ | `llama-3.1-8b-instant` via Groq |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîÑ Next Steps\n",
    "\n",
    "1. **Connect to live model**: Add Groq API integration to generate real outputs\n",
    "2. **Expand test suite**: Add more verification tests from production use cases\n",
    "3. **Historical tracking**: Compare results across multiple runs\n",
    "4. **Dashboard visualization**: Create charts for metric trends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Scoring Template\n",
        "\n",
        "Compare and evaluate different **models** using the winning prompt from prompt evaluation.\n",
        "\n",
        "## What This Template Does\n",
        "\n",
        "1. **Fixed Prompt** - Uses Prompt B (Structured) - the winner from prompt evaluation\n",
        "2. **Model Comparison** - Tests multiple models on the same prompt and test texts\n",
        "3. **Output Quality Metrics** - Measures how well each model produces Easy Language output\n",
        "4. **A/B Testing** - Directly compare two models to pick the winner\n",
        "\n",
        "## Key Difference from Prompt Evaluation\n",
        "\n",
        "| Template | Fixed Variable | Changeable Variable |\n",
        "|----------|----------------|---------------------|\n",
        "| Prompt Evaluation | Model | Prompts |\n",
        "| **Model Scoring** | **Prompt** | **Models** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from groq import Groq\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load environment\n",
        "found_path = find_dotenv(usecwd=True)\n",
        "if found_path:\n",
        "    load_dotenv(found_path, override=True)\n",
        "    print(f\"‚úÖ Loaded .env from: {found_path}\")\n",
        "\n",
        "# Initialize Groq\n",
        "groq_client = None\n",
        "if os.getenv(\"GROQ_API_KEY\"):\n",
        "    groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "    print(\"‚úÖ Groq Client Ready\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GROQ_API_KEY not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2. Configuration\n",
        "\n",
        "### Models to Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# MODELS TO COMPARE\n",
        "# =============================================\n",
        "\n",
        "MODELS = {\n",
        "    \"Model A (8B Fast)\": \"llama-3.1-8b-instant\",\n",
        "    \"Model B (70B Versatile)\": \"llama-3.3-70b-versatile\",\n",
        "}\n",
        "\n",
        "# Additional models you can add:\n",
        "# \"Model C (Gemma)\": \"gemma2-9b-it\",\n",
        "# \"Model D (Mixtral)\": \"mixtral-8x7b-32768\",\n",
        "\n",
        "print(f\"ü§ñ Loaded {len(MODELS)} models to compare:\")\n",
        "for name, model_id in MODELS.items():\n",
        "    print(f\"   ‚Ä¢ {name}: {model_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fixed Prompt (Winner: Prompt B - Structured)\n",
        "\n",
        "This prompt won the prompt evaluation and will be used for all model comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# FIXED PROMPT (Prompt B - Structured)\n",
        "# Winner from prompt evaluation\n",
        "# =============================================\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"# Identity\n",
        "\n",
        "You are an expert in plain language writing.\n",
        "You specialise in rewriting text to be accessible \n",
        "to people with learning disabilities or low literacy.\n",
        "\n",
        "# Core Task \n",
        "\n",
        "* Rewrite the input text to be extremely simple and easy to understand.\n",
        "* Keep the same meaning as the original text.\n",
        "\n",
        "# Constraints\n",
        "\n",
        "* Do NOT include any introductory or concluding text (e.g., \"Here is the simplified text\").\n",
        "* Output ONLY the simplified text.\n",
        "* Never output any XML/HTML tags or attributes (no <...>, no id=...).\n",
        "\n",
        "# Structure & Formatting Rules\n",
        "\n",
        "* Use clear structure.\n",
        "* Use bullet points for steps, lists, or multiple items. Otherwise prefer short sentences.\n",
        "* Add blank lines between every paragraph.\n",
        "\n",
        "# Plain Language Rules\n",
        "# Sentence & Length Rules\n",
        "\n",
        "* Use very short sentences (maximum 10 words per sentence).\n",
        "* Break up long sentences.\n",
        "* Keep subjects and verbs close together.\n",
        "\n",
        "# Vocabulary & Wording Rules\n",
        "\n",
        "* Use simple, familiar words. Avoid technical, foreign, or formal terms.\n",
        "* Explain any uncommon or necessary technical words or abbreviations in parentheses the first time they appear.\n",
        "* Explain complex ideas or uncommon nouns in parentheses.\n",
        "* Use positive wording. Avoid negations and never use double negatives.\n",
        "* Replace abstract nouns with concrete, active verbs.\n",
        "\n",
        "# Tone & Audience Rules\n",
        "\n",
        "* Prefer active voice. Avoid passive voice whenever possible.\n",
        "* Address the reader directly using \"you\".\n",
        "* Use a friendly, neutral tone.\n",
        "* Avoid bureaucratic, legalistic, or commanding language.\n",
        "\n",
        "# Consistency Rules\n",
        "\n",
        "* Remove filler words and unnecessary details. Keep only essential information.\n",
        "* Use the same words consistently. Do not switch terms for the same thing.\n",
        "\n",
        "# Examples\n",
        "# The following are example pairs.\n",
        "# Learn the style and constraints from them.\n",
        "# Do NOT copy the XML tags into your output.\n",
        "\n",
        "<examples>\n",
        "\n",
        "  <example id=\"1\">\n",
        "    <original_text>\n",
        "    Upon arrival at the facility, visitors are required to sign in at the front desk and present valid photo identification.\n",
        "    </original_text>\n",
        "\n",
        "    <simplified_text>\n",
        "    When you arrive:\n",
        "\n",
        "    * Go to the front desk.\n",
        "    * Sign in with your name.\n",
        "    * Show your photo ID.\n",
        "    </simplified_text>\n",
        "  </example>\n",
        "\n",
        "  <example id=\"2\">\n",
        "    <original_text>\n",
        "    The medication should be administered twice daily with food to minimize potential gastrointestinal discomfort.\n",
        "    </original_text>\n",
        "\n",
        "    <simplified_text>\n",
        "    Take this medicine two times every day.\n",
        "\n",
        "    Eat food when you take it. This helps your stomach feel better.\n",
        "    </simplified_text>\n",
        "  </example>\n",
        "\n",
        "</examples>\n",
        "\n",
        "Rewrite this text in simple language:\"\"\"\n",
        "\n",
        "print(\"üìù Fixed prompt loaded: Prompt B (Structured)\")\n",
        "print(f\"   Length: {len(SYSTEM_PROMPT)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test texts to evaluate models on\n",
        "TEST_TEXTS = {\n",
        "    \"Legal\": \"\"\"The obligations contained herein shall remain in full force and effect indefinitely, \n",
        "notwithstanding the termination of this Agreement, until such time as the Confidential Information \n",
        "no longer qualifies as confidential under applicable law.\"\"\",\n",
        "\n",
        "    \"Medical\": \"\"\"Patients should take the prescribed medication twice daily with food to minimize \n",
        "gastrointestinal discomfort. If adverse reactions occur, discontinue use immediately and consult \n",
        "your healthcare provider.\"\"\",\n",
        "\n",
        "    \"Bureaucratic\": \"\"\"For the application of housing benefit, a fully completed application form \n",
        "must be submitted to the responsible authority. The required proof of income and the rent \n",
        "certificate must be attached. Processing time is usually six to eight weeks.\"\"\"\n",
        "}\n",
        "\n",
        "print(f\"üìÑ Loaded {len(TEST_TEXTS)} test texts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output Quality Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# OUTPUT QUALITY METRICS\n",
        "# =============================================\n",
        "\n",
        "OUTPUT_METRICS = {\n",
        "    \"short_sentences\": {\n",
        "        \"name\": \"Short Sentences\",\n",
        "        \"description\": \"Maximum 10 words per sentence\",\n",
        "        \"weight\": 2,\n",
        "        \"check\": lambda text: max([len(s.split()) for s in re.split(r'[.!?]', text) if s.strip()], default=0)\n",
        "    },\n",
        "    \"uses_bullets\": {\n",
        "        \"name\": \"Uses Bullet Points\",\n",
        "        \"description\": \"Uses bullet points or numbered lists\",\n",
        "        \"weight\": 1,\n",
        "        \"check\": lambda text: bool(re.search(r'[‚Ä¢\\-\\*]\\s|^\\d+\\.\\s', text, re.MULTILINE))\n",
        "    },\n",
        "    \"has_paragraphs\": {\n",
        "        \"name\": \"Clear Paragraphs\",\n",
        "        \"description\": \"Has blank lines between sections\",\n",
        "        \"weight\": 1,\n",
        "        \"check\": lambda text: '\\n\\n' in text\n",
        "    },\n",
        "    \"no_intro_text\": {\n",
        "        \"name\": \"No Intro/Outro Text\",\n",
        "        \"description\": \"Starts directly without meta-text\",\n",
        "        \"weight\": 1,\n",
        "        \"check\": lambda text: not bool(re.match(r'^(Here\\'s|Here is|This is|The following|Sure|Certainly)', text.strip(), re.IGNORECASE))\n",
        "    },\n",
        "    \"no_xml_tags\": {\n",
        "        \"name\": \"No XML/HTML Tags\",\n",
        "        \"description\": \"No markup in output\",\n",
        "        \"weight\": 1,\n",
        "        \"check\": lambda text: not bool(re.search(r'<[^>]+>', text))\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"üìä Output metrics: {len(OUTPUT_METRICS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 3. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_completion(text: str, model_id: str) -> str:\n",
        "    \"\"\"Call a specific model with the fixed prompt.\"\"\"\n",
        "    if not groq_client:\n",
        "        return \"[No API Client]\"\n",
        "    \n",
        "    try:\n",
        "        response = groq_client.chat.completions.create(\n",
        "            model=model_id,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"[Error: {e}]\"\n",
        "\n",
        "\n",
        "def evaluate_output_quality(output: str) -> dict:\n",
        "    \"\"\"Evaluate the model output.\"\"\"\n",
        "    results = {}\n",
        "    total_score = 0\n",
        "    max_score = 0\n",
        "    \n",
        "    for metric_id, metric in OUTPUT_METRICS.items():\n",
        "        check_result = metric[\"check\"](output)\n",
        "        weight = metric[\"weight\"]\n",
        "        max_score += weight\n",
        "        \n",
        "        if metric_id == \"short_sentences\":\n",
        "            passed = check_result <= 10\n",
        "            results[metric_id] = {\"pass\": passed, \"value\": check_result, \"weight\": weight}\n",
        "        else:\n",
        "            passed = bool(check_result)\n",
        "            results[metric_id] = {\"pass\": passed, \"weight\": weight}\n",
        "        \n",
        "        if passed:\n",
        "            total_score += weight\n",
        "    \n",
        "    results[\"_score\"] = total_score / max_score if max_score > 0 else 0\n",
        "    return results\n",
        "\n",
        "\n",
        "def tfidf_similarity(text1: str, text2: str) -> float:\n",
        "    \"\"\"Calculate TF-IDF similarity between texts.\"\"\"\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(lowercase=True)\n",
        "        matrix = vectorizer.fit_transform([text1, text2])\n",
        "        return round(cosine_similarity(matrix[0:1], matrix[1:2])[0][0], 3)\n",
        "    except:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_model_comparison(results: dict, test_name: str):\n",
        "    \"\"\"Display side-by-side comparison of models with checkboxes.\"\"\"\n",
        "    \n",
        "    html = f\"\"\"<div style='background:#1a1a2e; padding:15px; border-radius:8px; margin:10px 0;'>\n",
        "    <h3 style='color:#eee; margin:0 0 15px 0;'>üìÑ Test: {test_name}</h3>\n",
        "    <div style='display:flex; gap:10px; flex-wrap:wrap;'>\"\"\"\n",
        "    \n",
        "    for model_name, data in results.items():\n",
        "        output_eval = data[\"output_eval\"]\n",
        "        output_score = output_eval[\"_score\"]\n",
        "        similarity = data[\"similarity\"]\n",
        "        \n",
        "        # Count passed metrics\n",
        "        output_passed = sum(1 for k, v in output_eval.items() if k != \"_score\" and v.get(\"pass\"))\n",
        "        output_total = len([k for k in output_eval if k != \"_score\"])\n",
        "        \n",
        "        score_color = \"#4ade80\" if output_score >= 0.8 else \"#fbbf24\" if output_score >= 0.6 else \"#f87171\"\n",
        "        sim_color = \"#4ade80\" if similarity >= 0.4 else \"#fbbf24\" if similarity >= 0.2 else \"#f87171\"\n",
        "        \n",
        "        html += f\"\"\"\n",
        "        <div style='flex:1; min-width:300px; background:#0f3460; padding:12px; border-radius:6px;'>\n",
        "            <div style='display:flex; justify-content:space-between; align-items:center; margin-bottom:10px;'>\n",
        "                <strong style='color:#e0e0e0; font-size:14px;'>{model_name}</strong>\n",
        "                <span style='background:{score_color}; color:#000; padding:2px 8px; border-radius:10px; font-size:11px;'>\n",
        "                    {output_score:.0%}\n",
        "                </span>\n",
        "            </div>\n",
        "            \n",
        "            <div style='background:#1a1a2e; padding:8px; border-radius:4px; margin-bottom:10px; max-height:180px; overflow-y:auto;'>\n",
        "                <pre style='color:#ddd; font-size:11px; white-space:pre-wrap; margin:0;'>{data[\"output\"][:500]}{'...' if len(data[\"output\"]) > 500 else ''}</pre>\n",
        "            </div>\n",
        "            \n",
        "            <div style='display:flex; justify-content:space-between; align-items:center; margin-bottom:8px;'>\n",
        "                <span style='color:#888; font-size:10px;'>TF-IDF Similarity:</span>\n",
        "                <span style='color:{sim_color}; font-size:12px; font-weight:bold;'>{similarity:.1%}</span>\n",
        "            </div>\n",
        "            \n",
        "            <div style='color:#888; font-size:10px; margin-bottom:5px; text-transform:uppercase;'>Quality Metrics ({output_passed}/{output_total})</div>\n",
        "            <div style='font-size:11px;'>\"\"\"\n",
        "        \n",
        "        # Output metric checkboxes\n",
        "        for metric_id, result in output_eval.items():\n",
        "            if metric_id == \"_score\":\n",
        "                continue\n",
        "            icon = \"‚úÖ\" if result[\"pass\"] else \"‚ùå\"\n",
        "            metric_name = OUTPUT_METRICS[metric_id][\"name\"]\n",
        "            value_str = f\" ({result.get('value', '')}w)\" if metric_id == \"short_sentences\" else \"\"\n",
        "            html += f\"<div style='color:#aaa;'>{icon} {metric_name}{value_str}</div>\"\n",
        "        \n",
        "        html += \"\"\"</div>\n",
        "        </div>\"\"\"\n",
        "    \n",
        "    html += \"</div></div>\"\n",
        "    display(HTML(html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_model_scorecard(all_results: dict):\n",
        "    \"\"\"Display overall scorecard for all models.\"\"\"\n",
        "    \n",
        "    # Aggregate scores\n",
        "    model_scores = {name: {\"output\": [], \"similarity\": []} for name in MODELS.keys()}\n",
        "    \n",
        "    for test_name, models in all_results.items():\n",
        "        for model_name, data in models.items():\n",
        "            model_scores[model_name][\"output\"].append(data[\"output_eval\"][\"_score\"])\n",
        "            model_scores[model_name][\"similarity\"].append(data[\"similarity\"])\n",
        "    \n",
        "    html = \"\"\"<div style='background:#1a1a2e; padding:20px; border-radius:8px; margin:20px 0;'>\n",
        "    <h2 style='color:#eee; margin:0 0 15px 0;'>üèÜ Model Scorecard</h2>\n",
        "    <table style='width:100%; border-collapse:collapse;'>\n",
        "        <tr style='background:#0f3460;'>\n",
        "            <th style='color:#eee; padding:12px; text-align:left;'>Model</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Model ID</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Avg Quality</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Avg Similarity</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Combined</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Rank</th>\n",
        "        </tr>\"\"\"\n",
        "    \n",
        "    # Calculate combined scores and rank\n",
        "    rankings = []\n",
        "    for model_name in MODELS.keys():\n",
        "        output_avg = sum(model_scores[model_name][\"output\"]) / len(model_scores[model_name][\"output\"])\n",
        "        sim_avg = sum(model_scores[model_name][\"similarity\"]) / len(model_scores[model_name][\"similarity\"])\n",
        "        combined = (output_avg * 0.7) + (sim_avg * 0.3)\n",
        "        rankings.append((model_name, MODELS[model_name], output_avg, sim_avg, combined))\n",
        "    \n",
        "    rankings.sort(key=lambda x: x[4], reverse=True)\n",
        "    \n",
        "    for rank, (model_name, model_id, output_avg, sim_avg, combined) in enumerate(rankings, 1):\n",
        "        score_color = \"#4ade80\" if combined >= 0.7 else \"#fbbf24\" if combined >= 0.5 else \"#f87171\"\n",
        "        rank_icon = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else f\"#{rank}\"\n",
        "        \n",
        "        html += f\"\"\"\n",
        "        <tr style='border-bottom:1px solid #333;'>\n",
        "            <td style='color:#ddd; padding:12px;'><strong>{model_name}</strong></td>\n",
        "            <td style='color:#888; padding:12px; text-align:center; font-family:monospace; font-size:11px;'>{model_id}</td>\n",
        "            <td style='color:#aaa; padding:12px; text-align:center;'>{output_avg:.0%}</td>\n",
        "            <td style='color:#aaa; padding:12px; text-align:center;'>{sim_avg:.1%}</td>\n",
        "            <td style='padding:12px; text-align:center;'>\n",
        "                <span style='background:{score_color}; color:#000; padding:4px 12px; border-radius:12px;'>{combined:.0%}</span>\n",
        "            </td>\n",
        "            <td style='color:#eee; padding:12px; text-align:center; font-size:18px;'>{rank_icon}</td>\n",
        "        </tr>\"\"\"\n",
        "    \n",
        "    html += \"</table></div>\"\n",
        "    display(HTML(html))\n",
        "    \n",
        "    return rankings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 4. Run Model Evaluation\n",
        "\n",
        "Compare all models on each test text using the fixed prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"üîÑ Evaluating {len(MODELS)} models on {len(TEST_TEXTS)} test texts...\")\n",
        "print(f\"üìù Using fixed prompt: Prompt B (Structured)\\n\")\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for test_name, test_text in TEST_TEXTS.items():\n",
        "    all_results[test_name] = {}\n",
        "    \n",
        "    for model_name, model_id in MODELS.items():\n",
        "        print(f\"  ‚Üí {test_name} | {model_name}...\", end=\" \")\n",
        "        \n",
        "        # Get model output\n",
        "        output = get_completion(test_text, model_id)\n",
        "        \n",
        "        # Evaluate output quality\n",
        "        output_eval = evaluate_output_quality(output)\n",
        "        \n",
        "        # Calculate meaning preservation\n",
        "        similarity = tfidf_similarity(test_text, output)\n",
        "        \n",
        "        all_results[test_name][model_name] = {\n",
        "            \"model_id\": model_id,\n",
        "            \"output\": output,\n",
        "            \"output_eval\": output_eval,\n",
        "            \"similarity\": similarity\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úì ({output_eval['_score']:.0%})\")\n",
        "        time.sleep(0.5)\n",
        "    \n",
        "    # Display comparison for this test\n",
        "    display_model_comparison(all_results[test_name], test_name)\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 5. Overall Scorecard\n",
        "\n",
        "See which model performs best overall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rankings = display_model_scorecard(all_results)\n",
        "\n",
        "# Winner announcement\n",
        "winner = rankings[0]\n",
        "print(f\"\\nüèÜ Winner: {winner[0]} ({winner[1]})\")\n",
        "print(f\"   Quality: {winner[2]:.0%} | Similarity: {winner[3]:.1%} | Combined: {winner[4]:.0%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 6. Detailed Metrics Breakdown\n",
        "\n",
        "See exactly which metrics each model passes or fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_detailed_metrics(all_results: dict):\n",
        "    \"\"\"Show detailed metric breakdown per model.\"\"\"\n",
        "    \n",
        "    # Aggregate metrics across all tests\n",
        "    output_metrics_agg = {name: {m: 0 for m in OUTPUT_METRICS} for name in MODELS}\n",
        "    num_tests = len(TEST_TEXTS)\n",
        "    \n",
        "    for test_name, models in all_results.items():\n",
        "        for model_name, data in models.items():\n",
        "            for m in OUTPUT_METRICS:\n",
        "                if data[\"output_eval\"].get(m, {}).get(\"pass\"):\n",
        "                    output_metrics_agg[model_name][m] += 1\n",
        "    \n",
        "    # Display output metrics\n",
        "    html = \"\"\"<div style='background:#1a1a2e; padding:20px; border-radius:8px; margin:20px 0;'>\n",
        "    <h3 style='color:#eee; margin:0 0 15px 0;'>üìä Output Quality Metrics (per test)</h3>\n",
        "    <table style='width:100%; border-collapse:collapse; font-size:13px;'>\n",
        "        <tr style='background:#0f3460;'>\n",
        "            <th style='color:#eee; padding:8px; text-align:left;'>Metric</th>\"\"\"\n",
        "    \n",
        "    for name in MODELS:\n",
        "        short_name = name.split(\"(\")[0].strip()\n",
        "        html += f\"<th style='color:#eee; padding:8px; text-align:center;'>{short_name}</th>\"\n",
        "    html += \"</tr>\"\n",
        "    \n",
        "    for metric_id, metric in OUTPUT_METRICS.items():\n",
        "        html += f\"<tr style='border-bottom:1px solid #333;'><td style='color:#aaa; padding:8px;'>{metric['name']}</td>\"\n",
        "        for name in MODELS:\n",
        "            count = output_metrics_agg[name][metric_id]\n",
        "            pct = count / num_tests\n",
        "            color = \"#4ade80\" if pct >= 0.8 else \"#fbbf24\" if pct >= 0.5 else \"#f87171\"\n",
        "            icon = \"‚úÖ\" if pct == 1 else \"‚ö†Ô∏è\" if pct > 0 else \"‚ùå\"\n",
        "            html += f\"<td style='text-align:center; padding:8px;'><span style='color:{color};'>{icon} {count}/{num_tests}</span></td>\"\n",
        "        html += \"</tr>\"\n",
        "    \n",
        "    html += \"</table></div>\"\n",
        "    display(HTML(html))\n",
        "\n",
        "display_detailed_metrics(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 7. A/B Test: Compare Two Models\n",
        "\n",
        "Direct head-to-head comparison of two specific models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show available models\n",
        "print(\"Available models:\")\n",
        "for i, name in enumerate(MODELS.keys()):\n",
        "    print(f\"  {i+1}. {name}\")\n",
        "\n",
        "# Select two models to compare\n",
        "model_names = list(MODELS.keys())\n",
        "MODEL_A = model_names[0] if len(model_names) > 0 else None\n",
        "MODEL_B = model_names[1] if len(model_names) > 1 else None\n",
        "\n",
        "if MODEL_A and MODEL_B:\n",
        "    # Count wins\n",
        "    a_wins = 0\n",
        "    b_wins = 0\n",
        "    ties = 0\n",
        "    \n",
        "    print(f\"\\n‚öîÔ∏è A/B Test: {MODEL_A} vs {MODEL_B}\\n\")\n",
        "    \n",
        "    for test_name in TEST_TEXTS:\n",
        "        score_a = all_results[test_name][MODEL_A][\"output_eval\"][\"_score\"]\n",
        "        score_b = all_results[test_name][MODEL_B][\"output_eval\"][\"_score\"]\n",
        "        sim_a = all_results[test_name][MODEL_A][\"similarity\"]\n",
        "        sim_b = all_results[test_name][MODEL_B][\"similarity\"]\n",
        "        \n",
        "        # Combined score for comparison\n",
        "        combined_a = (score_a * 0.7) + (sim_a * 0.3)\n",
        "        combined_b = (score_b * 0.7) + (sim_b * 0.3)\n",
        "        \n",
        "        if combined_a > combined_b:\n",
        "            a_wins += 1\n",
        "            winner = f\"üÖ∞Ô∏è {MODEL_A}\"\n",
        "        elif combined_b > combined_a:\n",
        "            b_wins += 1\n",
        "            winner = f\"üÖ±Ô∏è {MODEL_B}\"\n",
        "        else:\n",
        "            ties += 1\n",
        "            winner = \"ü§ù Tie\"\n",
        "        \n",
        "        print(f\"  {test_name}: {winner}\")\n",
        "        print(f\"      A: {score_a:.0%} quality, {sim_a:.1%} sim ‚Üí {combined_a:.0%}\")\n",
        "        print(f\"      B: {score_b:.0%} quality, {sim_b:.1%} sim ‚Üí {combined_b:.0%}\")\n",
        "    \n",
        "    print(f\"\\nüìä Results: {MODEL_A} wins {a_wins}, {MODEL_B} wins {b_wins}, Ties: {ties}\")\n",
        "    overall_winner = MODEL_A if a_wins > b_wins else MODEL_B if b_wins > a_wins else \"Tie\"\n",
        "    print(f\"üèÜ Overall Winner: {overall_winner}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Need at least 2 models for A/B testing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 8. Side-by-Side Output Comparison\n",
        "\n",
        "View full outputs from both models for manual inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_full_outputs(test_name: str):\n",
        "    \"\"\"Display full outputs from all models for a given test.\"\"\"\n",
        "    \n",
        "    html = f\"\"\"<div style='background:#1a1a2e; padding:20px; border-radius:8px; margin:20px 0;'>\n",
        "    <h3 style='color:#eee; margin:0 0 10px 0;'>üìù Full Outputs: {test_name}</h3>\n",
        "    <div style='background:#0f3460; padding:10px; border-radius:4px; margin-bottom:15px;'>\n",
        "        <div style='color:#888; font-size:10px; text-transform:uppercase;'>Original Text</div>\n",
        "        <pre style='color:#ddd; font-size:12px; white-space:pre-wrap; margin:5px 0 0 0;'>{TEST_TEXTS[test_name]}</pre>\n",
        "    </div>\n",
        "    <div style='display:flex; gap:15px; flex-wrap:wrap;'>\"\"\"\n",
        "    \n",
        "    for model_name, data in all_results[test_name].items():\n",
        "        score = data[\"output_eval\"][\"_score\"]\n",
        "        sim = data[\"similarity\"]\n",
        "        score_color = \"#4ade80\" if score >= 0.8 else \"#fbbf24\" if score >= 0.6 else \"#f87171\"\n",
        "        \n",
        "        html += f\"\"\"\n",
        "        <div style='flex:1; min-width:300px; background:#0f3460; padding:12px; border-radius:6px;'>\n",
        "            <div style='display:flex; justify-content:space-between; margin-bottom:8px;'>\n",
        "                <strong style='color:#e0e0e0;'>{model_name}</strong>\n",
        "                <span style='background:{score_color}; color:#000; padding:2px 8px; border-radius:10px; font-size:11px;'>{score:.0%}</span>\n",
        "            </div>\n",
        "            <div style='color:#888; font-size:10px; margin-bottom:8px;'>Similarity: {sim:.1%}</div>\n",
        "            <pre style='color:#ddd; font-size:12px; white-space:pre-wrap; background:#1a1a2e; padding:10px; border-radius:4px; margin:0;'>{data[\"output\"]}</pre>\n",
        "        </div>\"\"\"\n",
        "    \n",
        "    html += \"</div></div>\"\n",
        "    display(HTML(html))\n",
        "\n",
        "# Display full outputs for each test\n",
        "for test_name in TEST_TEXTS:\n",
        "    display_full_outputs(test_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Notes for Future Use\n",
        "\n",
        "## Adding New Models\n",
        "\n",
        "1. Add your model to the `MODELS` dictionary in Section 2:\n",
        "\n",
        "```python\n",
        "MODELS = {\n",
        "    \"Model A (8B Fast)\": \"llama-3.1-8b-instant\",\n",
        "    \"Model B (70B Versatile)\": \"llama-3.3-70b-versatile\",\n",
        "    \"Model C (Gemma)\": \"gemma2-9b-it\",  # Add new model\n",
        "}\n",
        "```\n",
        "\n",
        "2. Re-run all cells\n",
        "3. Check the scorecard to see how it compares\n",
        "\n",
        "## Available Groq Models\n",
        "\n",
        "| Model ID | Description | Speed |\n",
        "|----------|-------------|-------|\n",
        "| `llama-3.1-8b-instant` | Fast, efficient | ‚ö° Fast |\n",
        "| `llama-3.3-70b-versatile` | Larger, more capable | üê¢ Slower |\n",
        "| `gemma2-9b-it` | Google's efficient model | ‚ö° Fast |\n",
        "| `mixtral-8x7b-32768` | Mistral's MoE model | üê¢ Slower |\n",
        "\n",
        "## Scoring Breakdown\n",
        "\n",
        "**Combined Score** = (Output Quality √ó 0.7) + (TF-IDF Similarity √ó 0.3)\n",
        "\n",
        "| Component | Weight | What It Measures |\n",
        "|-----------|--------|------------------|\n",
        "| Output Quality | 70% | Rule adherence (sentences, bullets, formatting) |\n",
        "| TF-IDF Similarity | 30% | Meaning preservation (lexical overlap) |\n",
        "\n",
        "## Output Quality Metrics\n",
        "\n",
        "| Metric | Description | Weight |\n",
        "|--------|-------------|--------|\n",
        "| Short Sentences | Max 10 words per sentence | 2x |\n",
        "| Uses Bullets | Has bullet points or numbered lists | 1x |\n",
        "| Clear Paragraphs | Has blank lines between sections | 1x |\n",
        "| No Intro Text | Starts directly without \"Here is...\" | 1x |\n",
        "| No XML Tags | No markup in output | 1x |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Scoring Template\n",
        "\n",
        "Compare and evaluate different **models** using the winning prompt from prompt evaluation.\n",
        "\n",
        "## What This Template Does\n",
        "\n",
        "1. **Fixed Prompt** - Uses Prompt B (Structured) - the winner from prompt evaluation\n",
        "2. **Model Comparison** - Tests multiple models on the same prompt and test texts\n",
        "3. **Output Quality Metrics** - Measures how well each model produces Easy Language output\n",
        "4. **A/B Testing** - Directly compare two models to pick the winner\n",
        "\n",
        "## Key Difference from Prompt Evaluation\n",
        "\n",
        "| Template | Fixed Variable | Changeable Variable |\n",
        "|----------|----------------|---------------------|\n",
        "| Prompt Evaluation | Model | Prompts |\n",
        "| **Model Scoring** | **Prompt** | **Models** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from groq import Groq\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load environment\n",
        "found_path = find_dotenv(usecwd=True)\n",
        "if found_path:\n",
        "    load_dotenv(found_path, override=True)\n",
        "    print(f\"‚úÖ Loaded .env from: {found_path}\")\n",
        "\n",
        "# Initialize Groq\n",
        "groq_client = None\n",
        "if os.getenv(\"GROQ_API_KEY\"):\n",
        "    groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "    print(\"‚úÖ Groq Client Ready\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GROQ_API_KEY not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2. Configuration\n",
        "\n",
        "### Models to Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# MODELS TO COMPARE\n",
        "# =============================================\n",
        "\n",
        "MODELS = {\n",
        "    \"Model A (8B Fast)\": \"llama-3.1-8b-instant\",\n",
        "    \"Model B (70B Versatile)\": \"llama-3.3-70b-versatile\",\n",
        "}\n",
        "\n",
        "# Additional models you can add:\n",
        "# \"Model C (Gemma)\": \"gemma2-9b-it\",\n",
        "# \"Model D (Mixtral)\": \"mixtral-8x7b-32768\",\n",
        "\n",
        "print(f\"ü§ñ Loaded {len(MODELS)} models to compare:\")\n",
        "for name, model_id in MODELS.items():\n",
        "    print(f\"   ‚Ä¢ {name}: {model_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fixed Prompt (Winner: Prompt B - Structured)\n",
        "\n",
        "This prompt won the prompt evaluation and will be used for all model comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# FIXED PROMPT (Prompt B - Structured)\n",
        "# Winner from prompt evaluation\n",
        "# =============================================\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"# Identity\n",
        "\n",
        "You are an expert in plain language writing.\n",
        "You specialise in rewriting text to be accessible \n",
        "to people with learning disabilities or low literacy.\n",
        "\n",
        "# Core Task \n",
        "\n",
        "* Rewrite the input text to be extremely simple and easy to understand.\n",
        "* Keep the same meaning as the original text.\n",
        "\n",
        "# Constraints\n",
        "\n",
        "* Do NOT include any introductory or concluding text (e.g., \"Here is the simplified text\").\n",
        "* Output ONLY the simplified text.\n",
        "* Never output any XML/HTML tags or attributes (no <...>, no id=...).\n",
        "\n",
        "# Structure & Formatting Rules\n",
        "\n",
        "* Use clear structure.\n",
        "* Use bullet points for steps, lists, or multiple items. Otherwise prefer short sentences.\n",
        "* Add blank lines between every paragraph.\n",
        "\n",
        "# Plain Language Rules\n",
        "# Sentence & Length Rules\n",
        "\n",
        "* Use very short sentences (maximum 10 words per sentence).\n",
        "* Break up long sentences.\n",
        "* Keep subjects and verbs close together.\n",
        "\n",
        "# Vocabulary & Wording Rules\n",
        "\n",
        "* Use simple, familiar words. Avoid technical, foreign, or formal terms.\n",
        "* Explain any uncommon or necessary technical words or abbreviations in parentheses the first time they appear.\n",
        "* Explain complex ideas or uncommon nouns in parentheses.\n",
        "* Use positive wording. Avoid negations and never use double negatives.\n",
        "* Replace abstract nouns with concrete, active verbs.\n",
        "\n",
        "# Tone & Audience Rules\n",
        "\n",
        "* Prefer active voice. Avoid passive voice whenever possible.\n",
        "* Address the reader directly using \"you\".\n",
        "* Use a friendly, neutral tone.\n",
        "* Avoid bureaucratic, legalistic, or commanding language.\n",
        "\n",
        "# Consistency Rules\n",
        "\n",
        "* Remove filler words and unnecessary details. Keep only essential information.\n",
        "* Use the same words consistently. Do not switch terms for the same thing.\n",
        "\n",
        "# Examples\n",
        "# The following are example pairs.\n",
        "# Learn the style and constraints from them.\n",
        "# Do NOT copy the XML tags into your output.\n",
        "\n",
        "<examples>\n",
        "\n",
        "  <example id=\"1\">\n",
        "    <original_text>\n",
        "    Upon arrival at the facility, visitors are required to sign in at the front desk and present valid photo identification.\n",
        "    </original_text>\n",
        "\n",
        "    <simplified_text>\n",
        "    When you arrive:\n",
        "\n",
        "    * Go to the front desk.\n",
        "    * Sign in with your name.\n",
        "    * Show your photo ID.\n",
        "    </simplified_text>\n",
        "  </example>\n",
        "\n",
        "  <example id=\"2\">\n",
        "    <original_text>\n",
        "    The medication should be administered twice daily with food to minimize potential gastrointestinal discomfort.\n",
        "    </original_text>\n",
        "\n",
        "    <simplified_text>\n",
        "    Take this medicine two times every day.\n",
        "\n",
        "    Eat food when you take it. This helps your stomach feel better.\n",
        "    </simplified_text>\n",
        "  </example>\n",
        "\n",
        "</examples>\n",
        "\n",
        "Rewrite this text in simple language:\"\"\"\n",
        "\n",
        "print(\"üìù Fixed prompt loaded: Prompt B (Structured)\")\n",
        "print(f\"   Length: {len(SYSTEM_PROMPT)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test texts to evaluate models on\n",
        "TEST_TEXTS = {\n",
        "    \"Legal\": \"\"\"The obligations contained herein shall remain in full force and effect indefinitely, \n",
        "notwithstanding the termination of this Agreement, until such time as the Confidential Information \n",
        "no longer qualifies as confidential under applicable law.\"\"\",\n",
        "\n",
        "    \"Medical\": \"\"\"Patients should take the prescribed medication twice daily with food to minimize \n",
        "gastrointestinal discomfort. If adverse reactions occur, discontinue use immediately and consult \n",
        "your healthcare provider.\"\"\",\n",
        "\n",
        "    \"Bureaucratic\": \"\"\"For the application of housing benefit, a fully completed application form \n",
        "must be submitted to the responsible authority. The required proof of income and the rent \n",
        "certificate must be attached. Processing time is usually six to eight weeks.\"\"\"\n",
        "}\n",
        "\n",
        "print(f\"üìÑ Loaded {len(TEST_TEXTS)} test texts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output Quality Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# OUTPUT QUALITY METRICS\n",
        "# =============================================\n",
        "\n",
        "OUTPUT_METRICS = {\n",
        "    \"short_sentences\": {\n",
        "        \"name\": \"Short Sentences\",\n",
        "        \"description\": \"Maximum 10 words per sentence\",\n",
        "        \"weight\": 2,\n",
        "        \"check\": lambda text: max([len(s.split()) for s in re.split(r'[.!?]', text) if s.strip()], default=0)\n",
        "    },\n",
        "    \"uses_bullets\": {\n",
        "        \"name\": \"Uses Bullet Points\",\n",
        "        \"description\": \"Uses bullet points or numbered lists\",\n",
        "        \"weight\": 1,\n",
        "        \"check\": lambda text: bool(re.search(r'[‚Ä¢\\-\\*]\\s|^\\d+\\.\\s', text, re.MULTILINE))\n",
        "    },\n",
        "    \"has_paragraphs\": {\n",
        "        \"name\": \"Clear Paragraphs\",\n",
        "        \"description\": \"Has blank lines between sections\",\n",
        "        \"weight\": 1,\n",
        "        \"check\": lambda text: '\\n\\n' in text\n",
        "    },\n",
        "    \"no_intro_text\": {\n",
        "        \"name\": \"No Intro/Outro Text\",\n",
        "        \"description\": \"Starts directly without meta-text\",\n",
        "        \"weight\": 1,\n",
        "        \"check\": lambda text: not bool(re.match(r'^(Here\\'s|Here is|This is|The following|Sure|Certainly)', text.strip(), re.IGNORECASE))\n",
        "    },\n",
        "    \"no_xml_tags\": {\n",
        "        \"name\": \"No XML/HTML Tags\",\n",
        "        \"description\": \"No markup in output\",\n",
        "        \"weight\": 1,\n",
        "        \"check\": lambda text: not bool(re.search(r'<[^>]+>', text))\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"üìä Output metrics: {len(OUTPUT_METRICS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 3. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_model_comparison(results: dict, test_name: str):\n",
        "    \"\"\"Display side-by-side comparison of models with checkboxes.\"\"\"\n",
        "    \n",
        "    html = f\"\"\"<div style='background:#1a1a2e; padding:15px; border-radius:8px; margin:10px 0;'>\n",
        "    <h3 style='color:#eee; margin:0 0 15px 0;'>üìÑ Test: {test_name}</h3>\n",
        "    <div style='display:flex; gap:10px; flex-wrap:wrap;'>\"\"\"\n",
        "    \n",
        "    for model_name, data in results.items():\n",
        "        output_eval = data[\"output_eval\"]\n",
        "        output_score = output_eval[\"_score\"]\n",
        "        similarity = data[\"similarity\"]\n",
        "        \n",
        "        # Count passed metrics\n",
        "        output_passed = sum(1 for k, v in output_eval.items() if k != \"_score\" and v.get(\"pass\"))\n",
        "        output_total = len([k for k in output_eval if k != \"_score\"])\n",
        "        \n",
        "        score_color = \"#4ade80\" if output_score >= 0.8 else \"#fbbf24\" if output_score >= 0.6 else \"#f87171\"\n",
        "        sim_color = \"#4ade80\" if similarity >= 0.4 else \"#fbbf24\" if similarity >= 0.2 else \"#f87171\"\n",
        "        \n",
        "        html += f\"\"\"\n",
        "        <div style='flex:1; min-width:300px; background:#0f3460; padding:12px; border-radius:6px;'>\n",
        "            <div style='display:flex; justify-content:space-between; align-items:center; margin-bottom:10px;'>\n",
        "                <strong style='color:#e0e0e0; font-size:14px;'>{model_name}</strong>\n",
        "                <span style='background:{score_color}; color:#000; padding:2px 8px; border-radius:10px; font-size:11px;'>\n",
        "                    {output_score:.0%}\n",
        "                </span>\n",
        "            </div>\n",
        "            \n",
        "            <div style='background:#1a1a2e; padding:8px; border-radius:4px; margin-bottom:10px; max-height:180px; overflow-y:auto;'>\n",
        "                <pre style='color:#ddd; font-size:11px; white-space:pre-wrap; margin:0;'>{data[\"output\"][:500]}{'...' if len(data[\"output\"]) > 500 else ''}</pre>\n",
        "            </div>\n",
        "            \n",
        "            <div style='display:flex; justify-content:space-between; align-items:center; margin-bottom:8px;'>\n",
        "                <span style='color:#888; font-size:10px;'>TF-IDF Similarity:</span>\n",
        "                <span style='color:{sim_color}; font-size:12px; font-weight:bold;'>{similarity:.1%}</span>\n",
        "            </div>\n",
        "            \n",
        "            <div style='color:#888; font-size:10px; margin-bottom:5px; text-transform:uppercase;'>Quality Metrics ({output_passed}/{output_total})</div>\n",
        "            <div style='font-size:11px;'>\"\"\"\n",
        "        \n",
        "        # Output metric checkboxes\n",
        "        for metric_id, result in output_eval.items():\n",
        "            if metric_id == \"_score\":\n",
        "                continue\n",
        "            icon = \"‚úÖ\" if result[\"pass\"] else \"‚ùå\"\n",
        "            metric_name = OUTPUT_METRICS[metric_id][\"name\"]\n",
        "            value_str = f\" ({result.get('value', '')}w)\" if metric_id == \"short_sentences\" else \"\"\n",
        "            html += f\"<div style='color:#aaa;'>{icon} {metric_name}{value_str}</div>\"\n",
        "        \n",
        "        html += \"\"\"</div>\n",
        "        </div>\"\"\"\n",
        "    \n",
        "    html += \"</div></div>\"\n",
        "    display(HTML(html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_model_scorecard(all_results: dict):\n",
        "    \"\"\"Display overall scorecard for all models.\"\"\"\n",
        "    \n",
        "    # Aggregate scores\n",
        "    model_scores = {name: {\"output\": [], \"similarity\": []} for name in MODELS.keys()}\n",
        "    \n",
        "    for test_name, models in all_results.items():\n",
        "        for model_name, data in models.items():\n",
        "            model_scores[model_name][\"output\"].append(data[\"output_eval\"][\"_score\"])\n",
        "            model_scores[model_name][\"similarity\"].append(data[\"similarity\"])\n",
        "    \n",
        "    html = \"\"\"<div style='background:#1a1a2e; padding:20px; border-radius:8px; margin:20px 0;'>\n",
        "    <h2 style='color:#eee; margin:0 0 15px 0;'>üèÜ Model Scorecard</h2>\n",
        "    <table style='width:100%; border-collapse:collapse;'>\n",
        "        <tr style='background:#0f3460;'>\n",
        "            <th style='color:#eee; padding:12px; text-align:left;'>Model</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Model ID</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Avg Quality</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Avg Similarity</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Combined</th>\n",
        "            <th style='color:#eee; padding:12px; text-align:center;'>Rank</th>\n",
        "        </tr>\"\"\"\n",
        "    \n",
        "    # Calculate combined scores and rank\n",
        "    rankings = []\n",
        "    for model_name in MODELS.keys():\n",
        "        output_avg = sum(model_scores[model_name][\"output\"]) / len(model_scores[model_name][\"output\"])\n",
        "        sim_avg = sum(model_scores[model_name][\"similarity\"]) / len(model_scores[model_name][\"similarity\"])\n",
        "        combined = (output_avg * 0.7) + (sim_avg * 0.3)\n",
        "        rankings.append((model_name, MODELS[model_name], output_avg, sim_avg, combined))\n",
        "    \n",
        "    rankings.sort(key=lambda x: x[4], reverse=True)\n",
        "    \n",
        "    for rank, (model_name, model_id, output_avg, sim_avg, combined) in enumerate(rankings, 1):\n",
        "        score_color = \"#4ade80\" if combined >= 0.7 else \"#fbbf24\" if combined >= 0.5 else \"#f87171\"\n",
        "        rank_icon = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else f\"#{rank}\"\n",
        "        \n",
        "        html += f\"\"\"\n",
        "        <tr style='border-bottom:1px solid #333;'>\n",
        "            <td style='color:#ddd; padding:12px;'><strong>{model_name}</strong></td>\n",
        "            <td style='color:#888; padding:12px; text-align:center; font-family:monospace; font-size:11px;'>{model_id}</td>\n",
        "            <td style='color:#aaa; padding:12px; text-align:center;'>{output_avg:.0%}</td>\n",
        "            <td style='color:#aaa; padding:12px; text-align:center;'>{sim_avg:.1%}</td>\n",
        "            <td style='padding:12px; text-align:center;'>\n",
        "                <span style='background:{score_color}; color:#000; padding:4px 12px; border-radius:12px;'>{combined:.0%}</span>\n",
        "            </td>\n",
        "            <td style='color:#eee; padding:12px; text-align:center; font-size:18px;'>{rank_icon}</td>\n",
        "        </tr>\"\"\"\n",
        "    \n",
        "    html += \"</table></div>\"\n",
        "    display(HTML(html))\n",
        "    \n",
        "    return rankings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_completion(text: str, model_id: str) -> str:\n",
        "    \"\"\"Call a specific model with the fixed prompt.\"\"\"\n",
        "    if not groq_client:\n",
        "        return \"[No API Client]\"\n",
        "    \n",
        "    try:\n",
        "        response = groq_client.chat.completions.create(\n",
        "            model=model_id,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"[Error: {e}]\"\n",
        "\n",
        "\n",
        "def evaluate_output_quality(output: str) -> dict:\n",
        "    \"\"\"Evaluate the model output.\"\"\"\n",
        "    results = {}\n",
        "    total_score = 0\n",
        "    max_score = 0\n",
        "    \n",
        "    for metric_id, metric in OUTPUT_METRICS.items():\n",
        "        check_result = metric[\"check\"](output)\n",
        "        weight = metric[\"weight\"]\n",
        "        max_score += weight\n",
        "        \n",
        "        if metric_id == \"short_sentences\":\n",
        "            passed = check_result <= 10\n",
        "            results[metric_id] = {\"pass\": passed, \"value\": check_result, \"weight\": weight}\n",
        "        else:\n",
        "            passed = bool(check_result)\n",
        "            results[metric_id] = {\"pass\": passed, \"weight\": weight}\n",
        "        \n",
        "        if passed:\n",
        "            total_score += weight\n",
        "    \n",
        "    results[\"_score\"] = total_score / max_score if max_score > 0 else 0\n",
        "    return results\n",
        "\n",
        "\n",
        "def tfidf_similarity(text1: str, text2: str) -> float:\n",
        "    \"\"\"Calculate TF-IDF similarity between texts.\"\"\"\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(lowercase=True)\n",
        "        matrix = vectorizer.fit_transform([text1, text2])\n",
        "        return round(cosine_similarity(matrix[0:1], matrix[1:2])[0][0], 3)\n",
        "    except:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"üîÑ Evaluating {len(MODELS)} models on {len(TEST_TEXTS)} test texts...\")\n",
        "print(f\"üìù Using fixed prompt: Prompt B (Structured)\\n\")\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for test_name, test_text in TEST_TEXTS.items():\n",
        "    all_results[test_name] = {}\n",
        "    \n",
        "    for model_name, model_id in MODELS.items():\n",
        "        print(f\"  ‚Üí {test_name} | {model_name}...\", end=\" \")\n",
        "        \n",
        "        # Get model output\n",
        "        output = get_completion(test_text, model_id)\n",
        "        \n",
        "        # Evaluate output quality\n",
        "        output_eval = evaluate_output_quality(output)\n",
        "        \n",
        "        # Calculate meaning preservation\n",
        "        similarity = tfidf_similarity(test_text, output)\n",
        "        \n",
        "        all_results[test_name][model_name] = {\n",
        "            \"model_id\": model_id,\n",
        "            \"output\": output,\n",
        "            \"output_eval\": output_eval,\n",
        "            \"similarity\": similarity\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úì ({output_eval['_score']:.0%})\")\n",
        "        time.sleep(0.5)\n",
        "    \n",
        "    # Display comparison for this test\n",
        "    display_model_comparison(all_results[test_name], test_name)\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "rankings = display_model_scorecard(all_results)\n",
        "\n",
        "# Winner announcement\n",
        "winner = rankings[0]\n",
        "print(f\"\\nüèÜ Winner: {winner[0]} ({winner[1]})\")\n",
        "print(f\"   Quality: {winner[2]:.0%} | Similarity: {winner[3]:.1%} | Combined: {winner[4]:.0%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 6. Detailed Metrics Breakdown\n",
        "\n",
        "See exactly which metrics each model passes or fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_detailed_metrics(all_results: dict):\n",
        "    \"\"\"Show detailed metric breakdown per model.\"\"\"\n",
        "    \n",
        "    # Aggregate metrics across all tests\n",
        "    output_metrics_agg = {name: {m: 0 for m in OUTPUT_METRICS} for name in MODELS}\n",
        "    num_tests = len(TEST_TEXTS)\n",
        "    \n",
        "    for test_name, models in all_results.items():\n",
        "        for model_name, data in models.items():\n",
        "            for m in OUTPUT_METRICS:\n",
        "                if data[\"output_eval\"].get(m, {}).get(\"pass\"):\n",
        "                    output_metrics_agg[model_name][m] += 1\n",
        "    \n",
        "    # Display output metrics\n",
        "    html = \"\"\"<div style='background:#1a1a2e; padding:20px; border-radius:8px; margin:20px 0;'>\n",
        "    <h3 style='color:#eee; margin:0 0 15px 0;'>üìä Output Quality Metrics (per test)</h3>\n",
        "    <table style='width:100%; border-collapse:collapse; font-size:13px;'>\n",
        "        <tr style='background:#0f3460;'>\n",
        "            <th style='color:#eee; padding:8px; text-align:left;'>Metric</th>\"\"\"\n",
        "    \n",
        "    for name in MODELS:\n",
        "        short_name = name.split(\"(\")[0].strip()\n",
        "        html += f\"<th style='color:#eee; padding:8px; text-align:center;'>{short_name}</th>\"\n",
        "    html += \"</tr>\"\n",
        "    \n",
        "    for metric_id, metric in OUTPUT_METRICS.items():\n",
        "        html += f\"<tr style='border-bottom:1px solid #333;'><td style='color:#aaa; padding:8px;'>{metric['name']}</td>\"\n",
        "        for name in MODELS:\n",
        "            count = output_metrics_agg[name][metric_id]\n",
        "            pct = count / num_tests\n",
        "            color = \"#4ade80\" if pct >= 0.8 else \"#fbbf24\" if pct >= 0.5 else \"#f87171\"\n",
        "            icon = \"‚úÖ\" if pct == 1 else \"‚ö†Ô∏è\" if pct > 0 else \"‚ùå\"\n",
        "            html += f\"<td style='text-align:center; padding:8px;'><span style='color:{color};'>{icon} {count}/{num_tests}</span></td>\"\n",
        "        html += \"</tr>\"\n",
        "    \n",
        "    html += \"</table></div>\"\n",
        "    display(HTML(html))\n",
        "\n",
        "display_detailed_metrics(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 7. A/B Test: Compare Two Models\n",
        "\n",
        "Direct head-to-head comparison of two specific models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show available models\n",
        "print(\"Available models:\")\n",
        "for i, name in enumerate(MODELS.keys()):\n",
        "    print(f\"  {i+1}. {name}\")\n",
        "\n",
        "# Select two models to compare\n",
        "model_names = list(MODELS.keys())\n",
        "MODEL_A = model_names[0] if len(model_names) > 0 else None\n",
        "MODEL_B = model_names[1] if len(model_names) > 1 else None\n",
        "\n",
        "if MODEL_A and MODEL_B:\n",
        "    # Count wins\n",
        "    a_wins = 0\n",
        "    b_wins = 0\n",
        "    ties = 0\n",
        "    \n",
        "    print(f\"\\n‚öîÔ∏è A/B Test: {MODEL_A} vs {MODEL_B}\\n\")\n",
        "    \n",
        "    for test_name in TEST_TEXTS:\n",
        "        score_a = all_results[test_name][MODEL_A][\"output_eval\"][\"_score\"]\n",
        "        score_b = all_results[test_name][MODEL_B][\"output_eval\"][\"_score\"]\n",
        "        sim_a = all_results[test_name][MODEL_A][\"similarity\"]\n",
        "        sim_b = all_results[test_name][MODEL_B][\"similarity\"]\n",
        "        \n",
        "        # Combined score for comparison\n",
        "        combined_a = (score_a * 0.7) + (sim_a * 0.3)\n",
        "        combined_b = (score_b * 0.7) + (sim_b * 0.3)\n",
        "        \n",
        "        if combined_a > combined_b:\n",
        "            a_wins += 1\n",
        "            winner = f\"üÖ∞Ô∏è {MODEL_A}\"\n",
        "        elif combined_b > combined_a:\n",
        "            b_wins += 1\n",
        "            winner = f\"üÖ±Ô∏è {MODEL_B}\"\n",
        "        else:\n",
        "            ties += 1\n",
        "            winner = \"ü§ù Tie\"\n",
        "        \n",
        "        print(f\"  {test_name}: {winner}\")\n",
        "        print(f\"      A: {score_a:.0%} quality, {sim_a:.1%} sim ‚Üí {combined_a:.0%}\")\n",
        "        print(f\"      B: {score_b:.0%} quality, {sim_b:.1%} sim ‚Üí {combined_b:.0%}\")\n",
        "    \n",
        "    print(f\"\\nüìä Results: {MODEL_A} wins {a_wins}, {MODEL_B} wins {b_wins}, Ties: {ties}\")\n",
        "    overall_winner = MODEL_A if a_wins > b_wins else MODEL_B if b_wins > a_wins else \"Tie\"\n",
        "    print(f\"üèÜ Overall Winner: {overall_winner}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Need at least 2 models for A/B testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# End of notebook"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

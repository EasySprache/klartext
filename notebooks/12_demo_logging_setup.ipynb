{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š Demo Logging Setup\n",
        "\n",
        "This notebook sets up a structured JSON logging system for the KlarText Demo UI.\n",
        "\n",
        "**Goals:**\n",
        "1. Create a logging module that captures each simplification output\n",
        "2. Store logs in a single JSONL file with running averages\n",
        "3. Integrate the logger with the demo app\n",
        "\n",
        "**Log Structure:**\n",
        "```json\n",
        "{\n",
        "    \"timestamp\": \"2026-01-08T14:30:00Z\",\n",
        "    \"source_text\": \"...\",\n",
        "    \"output_text\": \"...\",\n",
        "    \"model\": \"llama-3.1-8b-instant\",\n",
        "    \"template\": \"system_prompt_de.txt\",\n",
        "    \"metrics\": {\n",
        "        \"avg_sentence_len_words\": 8.5,\n",
        "        \"pct_sentences_gt20\": 0.0,\n",
        "        \"ari_score\": 7.0,\n",
        "        \"meaning_cosine\": 0.85\n",
        "    },\n",
        "    \"guardrails_passed\": 4,\n",
        "    \"guardrails_total\": 4,\n",
        "    \"guardrails_failed\": []\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 1: Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Project paths\n",
        "PROJECT_ROOT = Path(\"..\")\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "LOGS_DIR = DATA_DIR / \"logs\"\n",
        "\n",
        "# Ensure logs directory exists\n",
        "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Log file path\n",
        "LOG_FILE = LOGS_DIR / \"demo_outputs.jsonl\"\n",
        "\n",
        "print(f\"âœ… Log file will be stored at: {LOG_FILE.resolve()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "âœ… Log file will be stored at: /Users/simonvoegely/Desktop/easysprache/klartext/data/logs/demo_outputs.jsonl\n"
          ]
        }
      ],
      "id": "bb15760e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 2: Define Metrics Calculation Functions\n",
        "\n",
        "These functions compute the readability metrics for each output."
      ],
      "id": "8713ee4e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def split_sentences(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split text into sentences.\n",
        "    Handles common abbreviations in both English and German.\n",
        "    \"\"\"\n",
        "    # Handle common abbreviations\n",
        "    text_clean = re.sub(r'\\b(Mr|Mrs|Ms|Dr|Prof|Jr|Sr|vs|etc|e\\.g|i\\.e)\\.\\s', r'\\1_DOT ', text)\n",
        "    # Handle German abbreviations\n",
        "    text_clean = re.sub(r'\\b(z\\.B|d\\.h|usw|ggfs)\\.\\s', r'\\1_DOT ', text_clean)\n",
        "    \n",
        "    # Split on sentence-ending punctuation\n",
        "    sentences = re.split(r'[.!?]+\\s+', text_clean.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "\n",
        "def get_words(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extract words from text.\n",
        "    Handles German umlauts and special characters.\n",
        "    \"\"\"\n",
        "    return re.findall(r'[A-Za-zÃ„Ã–ÃœÃ¤Ã¶Ã¼ÃŸÃ©Ã¨ÃªÃ«Ã Ã¢Ã¡Ã®Ã¯Ã­Ã´Ã¶Ã³Ã»Ã¼Ãº]+', text)\n",
        "\n",
        "\n",
        "def count_syllables(word: str) -> int:\n",
        "    \"\"\"\n",
        "    Estimate syllable count for a word.\n",
        "    Simple heuristic for English/German.\n",
        "    \"\"\"\n",
        "    word = word.lower()\n",
        "    vowels = \"aeiouyÃ¤Ã¶Ã¼\"\n",
        "    count = 0\n",
        "    prev_was_vowel = False\n",
        "    \n",
        "    for char in word:\n",
        "        is_vowel = char in vowels\n",
        "        if is_vowel and not prev_was_vowel:\n",
        "            count += 1\n",
        "        prev_was_vowel = is_vowel\n",
        "    \n",
        "    # Words always have at least one syllable\n",
        "    return max(1, count)\n",
        "\n",
        "\n",
        "def compute_ari_score(text: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute Automated Readability Index (ARI).\n",
        "    ARI = 4.71 * (characters/words) + 0.5 * (words/sentences) - 21.43\n",
        "    Lower scores = easier to read.\n",
        "    \"\"\"\n",
        "    sentences = split_sentences(text)\n",
        "    words = get_words(text)\n",
        "    \n",
        "    if not sentences or not words:\n",
        "        return 0.0\n",
        "    \n",
        "    # Count characters (only letters)\n",
        "    char_count = sum(len(w) for w in words)\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "    \n",
        "    ari = 4.71 * (char_count / word_count) + 0.5 * (word_count / sentence_count) - 21.43\n",
        "    return round(max(0, ari), 2)\n",
        "\n",
        "\n",
        "def compute_meaning_cosine(source: str, output: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute TF-IDF cosine similarity between source and output.\n",
        "    Higher scores = better meaning preservation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(lowercase=True)\n",
        "        tfidf_matrix = vectorizer.fit_transform([source, output])\n",
        "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        return round(similarity, 3)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_metrics(source_text: str, output_text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Compute all metrics for a simplification output.\n",
        "    \n",
        "    Returns:\n",
        "        dict with metrics:\n",
        "        - avg_sentence_len_words: Average words per sentence\n",
        "        - pct_sentences_gt20: Percentage of sentences > 20 words\n",
        "        - ari_score: Automated Readability Index\n",
        "        - meaning_cosine: TF-IDF similarity (meaning preservation)\n",
        "    \"\"\"\n",
        "    sentences = split_sentences(output_text)\n",
        "    words = get_words(output_text)\n",
        "    \n",
        "    if not sentences or not words:\n",
        "        return {\n",
        "            \"avg_sentence_len_words\": 0.0,\n",
        "            \"pct_sentences_gt20\": 0.0,\n",
        "            \"ari_score\": 0.0,\n",
        "            \"meaning_cosine\": 0.0\n",
        "        }\n",
        "    \n",
        "    # Sentence length analysis\n",
        "    sent_lengths = [len(get_words(s)) for s in sentences]\n",
        "    avg_sent_len = sum(sent_lengths) / len(sentences)\n",
        "    long_sents = sum(1 for length in sent_lengths if length > 20)\n",
        "    pct_long = (long_sents / len(sentences)) * 100\n",
        "    \n",
        "    return {\n",
        "        \"avg_sentence_len_words\": round(avg_sent_len, 1),\n",
        "        \"pct_sentences_gt20\": round(pct_long, 1),\n",
        "        \"ari_score\": compute_ari_score(output_text),\n",
        "        \"meaning_cosine\": compute_meaning_cosine(source_text, output_text)\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"âœ… Metrics functions defined\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "âœ… Metrics functions defined\n"
          ]
        }
      ],
      "id": "a3577ba0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 3: Define Guardrails Evaluation\n",
        "\n",
        "Guardrails are binary checks that validate the output quality."
      ],
      "id": "2bd81244"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Guardrails definitions\n",
        "GUARDRAILS = {\n",
        "    \"short_sentences\": {\n",
        "        \"name\": \"Short Sentences\",\n",
        "        \"description\": \"Average sentence length â‰¤ 15 words\",\n",
        "        \"check\": lambda metrics: metrics[\"avg_sentence_len_words\"] <= 15\n",
        "    },\n",
        "    \"no_long_sentences\": {\n",
        "        \"name\": \"No Long Sentences\",\n",
        "        \"description\": \"Less than 10% of sentences > 20 words\",\n",
        "        \"check\": lambda metrics: metrics[\"pct_sentences_gt20\"] <= 10\n",
        "    },\n",
        "    \"readable\": {\n",
        "        \"name\": \"Readable (ARI)\",\n",
        "        \"description\": \"ARI score â‰¤ 8 (8th grade level or below)\",\n",
        "        \"check\": lambda metrics: metrics[\"ari_score\"] <= 8\n",
        "    },\n",
        "    \"preserves_meaning\": {\n",
        "        \"name\": \"Preserves Meaning\",\n",
        "        \"description\": \"Cosine similarity â‰¥ 0.70 with source\",\n",
        "        \"check\": lambda metrics: metrics[\"meaning_cosine\"] >= 0.70\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def evaluate_guardrails(metrics: dict) -> tuple[int, int, list[str]]:\n",
        "    \"\"\"\n",
        "    Evaluate all guardrails against computed metrics.\n",
        "    \n",
        "    Returns:\n",
        "        tuple of (passed_count, total_count, failed_guardrail_names)\n",
        "    \"\"\"\n",
        "    passed = 0\n",
        "    failed = []\n",
        "    \n",
        "    for guardrail_id, guardrail in GUARDRAILS.items():\n",
        "        try:\n",
        "            if guardrail[\"check\"](metrics):\n",
        "                passed += 1\n",
        "            else:\n",
        "                failed.append(guardrail[\"name\"])\n",
        "        except Exception as e:\n",
        "            failed.append(f\"{guardrail['name']} (error)\")\n",
        "    \n",
        "    return passed, len(GUARDRAILS), failed\n",
        "\n",
        "\n",
        "print(\"âœ… Guardrails defined\")\n",
        "print(f\"   Total guardrails: {len(GUARDRAILS)}\")\n",
        "for gid, g in GUARDRAILS.items():\n",
        "    print(f\"   - {g['name']}: {g['description']}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "âœ… Guardrails defined\n",
            "   Total guardrails: 4\n",
            "   - Short Sentences: Average sentence length â‰¤ 15 words\n",
            "   - No Long Sentences: Less than 10% of sentences > 20 words\n",
            "   - Readable (ARI): ARI score â‰¤ 8 (8th grade level or below)\n",
            "   - Preserves Meaning: Cosine similarity â‰¥ 0.70 with source\n"
          ]
        }
      ],
      "id": "3e8222ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 4: Create the Logging Function\n",
        "\n",
        "This function creates a log entry and appends it to the JSONL file."
      ],
      "id": "bc854522"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_log_entry(\n",
        "    source_text: str,\n",
        "    output_text: str,\n",
        "    model: str,\n",
        "    template: str,\n",
        "    language: str = \"de\"\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Create a complete log entry for a simplification output.\n",
        "    \n",
        "    Args:\n",
        "        source_text: Original input text\n",
        "        output_text: Simplified output text\n",
        "        model: Model identifier (e.g., \"llama-3.1-8b-instant\")\n",
        "        template: Template filename (e.g., \"system_prompt_de.txt\")\n",
        "        language: Language code (\"de\" or \"en\")\n",
        "    \n",
        "    Returns:\n",
        "        dict: Complete log entry\n",
        "    \"\"\"\n",
        "    # Get current timestamp in ISO format\n",
        "    timestamp = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
        "    \n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(source_text, output_text)\n",
        "    \n",
        "    # Evaluate guardrails\n",
        "    passed, total, failed = evaluate_guardrails(metrics)\n",
        "    \n",
        "    return {\n",
        "        \"timestamp\": timestamp,\n",
        "        \"source_text\": source_text,\n",
        "        \"output_text\": output_text,\n",
        "        \"model\": model,\n",
        "        \"template\": template,\n",
        "        \"language\": language,\n",
        "        \"metrics\": metrics,\n",
        "        \"guardrails_passed\": passed,\n",
        "        \"guardrails_total\": total,\n",
        "        \"guardrails_failed\": failed\n",
        "    }\n",
        "\n",
        "\n",
        "def append_log_entry(log_entry: dict, log_file: Path = LOG_FILE) -> None:\n",
        "    \"\"\"\n",
        "    Append a log entry to the JSONL file.\n",
        "    \n",
        "    Args:\n",
        "        log_entry: Log entry dict\n",
        "        log_file: Path to the log file\n",
        "    \"\"\"\n",
        "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(log_entry, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "def log_simplification(\n",
        "    source_text: str,\n",
        "    output_text: str,\n",
        "    model: str,\n",
        "    template: str,\n",
        "    language: str = \"de\",\n",
        "    log_file: Path = LOG_FILE\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Main logging function - creates entry and appends to file.\n",
        "    \n",
        "    Returns:\n",
        "        dict: The created log entry\n",
        "    \"\"\"\n",
        "    entry = create_log_entry(source_text, output_text, model, template, language)\n",
        "    append_log_entry(entry, log_file)\n",
        "    return entry\n",
        "\n",
        "\n",
        "print(\"âœ… Logging functions defined\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "âœ… Logging functions defined\n"
          ]
        }
      ],
      "id": "2c006f47"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 5: Create Aggregate Statistics Function\n",
        "\n",
        "This function reads all log entries and computes running averages."
      ],
      "id": "b0ba5ab7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_all_logs(log_file: Path = LOG_FILE) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Load all log entries from the JSONL file.\n",
        "    \n",
        "    Returns:\n",
        "        list of log entry dicts\n",
        "    \"\"\"\n",
        "    if not log_file.exists():\n",
        "        return []\n",
        "    \n",
        "    logs = []\n",
        "    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                logs.append(json.loads(line))\n",
        "    return logs\n",
        "\n",
        "\n",
        "def compute_aggregate_stats(logs_or_log_file) -> dict:\n",
        "    \"\"\"\n",
        "    Compute aggregate statistics across all log entries.\n",
        "\n",
        "    Accepts either:\n",
        "    - a list of log dicts (in-memory), or\n",
        "    - a log file path (Path/str)\n",
        "\n",
        "    Returns:\n",
        "        dict with average metrics and summary stats\n",
        "    \"\"\"\n",
        "    # Allow passing a path (keeps this consistent with demo/demo_logger.py)\n",
        "    if isinstance(logs_or_log_file, (str, Path)):\n",
        "        logs = load_all_logs(Path(logs_or_log_file))\n",
        "    else:\n",
        "        logs = logs_or_log_file\n",
        "\n",
        "    if not logs:\n",
        "        return {\n",
        "            \"total_entries\": 0,\n",
        "            \"avg_metrics\": {},\n",
        "            \"guardrails_summary\": {}\n",
        "        }\n",
        "\n",
        "    # Extract metrics\n",
        "    metrics_list = [log[\"metrics\"] for log in logs if \"metrics\" in log]\n",
        "\n",
        "    # Compute averages\n",
        "    avg_metrics = {}\n",
        "    if metrics_list:\n",
        "        metric_keys = metrics_list[0].keys()\n",
        "        for key in metric_keys:\n",
        "            values = [m[key] for m in metrics_list if isinstance(m.get(key), (int, float))]\n",
        "            if values:\n",
        "                avg_metrics[f\"avg_{key}\"] = round(sum(values) / len(values), 2)\n",
        "\n",
        "    # Guardrails summary\n",
        "    total_passed = sum(log.get(\"guardrails_passed\", 0) for log in logs)\n",
        "    total_total = sum(log.get(\"guardrails_total\", 0) for log in logs)\n",
        "\n",
        "    # Count failures by type\n",
        "    failure_counts = {}\n",
        "    for log in logs:\n",
        "        for failed in log.get(\"guardrails_failed\", []):\n",
        "            failure_counts[failed] = failure_counts.get(failed, 0) + 1\n",
        "\n",
        "    return {\n",
        "        \"total_entries\": len(logs),\n",
        "        \"avg_metrics\": avg_metrics,\n",
        "        \"guardrails_summary\": {\n",
        "            \"total_passed\": total_passed,\n",
        "            \"total_checks\": total_total,\n",
        "            \"pass_rate\": round(total_passed / total_total * 100, 1) if total_total > 0 else 0,\n",
        "            \"failure_counts\": failure_counts\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def get_logs_with_summary(log_file: Path = LOG_FILE) -> dict:\n",
        "    \"\"\"\n",
        "    Get all logs with aggregate summary at the top.\n",
        "\n",
        "    Returns:\n",
        "        dict with 'summary' and 'entries' keys\n",
        "    \"\"\"\n",
        "    logs = load_all_logs(log_file)\n",
        "    summary = compute_aggregate_stats(log_file)\n",
        "\n",
        "    return {\n",
        "        \"summary\": summary,\n",
        "        \"entries\": logs\n",
        "    }\n",
        "\n",
        "\n",
        "def display_summary(log_file: Path = LOG_FILE):\n",
        "    \"\"\"\n",
        "    Display a formatted summary of all logs.\n",
        "    \"\"\"\n",
        "    data = get_logs_with_summary(log_file)\n",
        "    summary = data[\"summary\"]\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"ðŸ“Š DEMO OUTPUT LOGS - SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nTotal entries: {summary['total_entries']}\")\n",
        "    \n",
        "    if summary[\"avg_metrics\"]:\n",
        "        print(\"\\nðŸ“ˆ Average Metrics:\")\n",
        "        for key, value in summary[\"avg_metrics\"].items():\n",
        "            print(f\"   {key}: {value}\")\n",
        "    \n",
        "    if summary[\"guardrails_summary\"]:\n",
        "        gs = summary[\"guardrails_summary\"]\n",
        "        print(f\"\\nâœ… Guardrails Pass Rate: {gs['pass_rate']}%\")\n",
        "        print(f\"   ({gs['total_passed']}/{gs['total_checks']} checks passed)\")\n",
        "        \n",
        "        if gs[\"failure_counts\"]:\n",
        "            print(\"\\nâŒ Most Common Failures:\")\n",
        "            sorted_failures = sorted(gs[\"failure_counts\"].items(), key=lambda x: -x[1])\n",
        "            for name, count in sorted_failures:\n",
        "                print(f\"   - {name}: {count} times\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "print(\"âœ… Aggregate statistics functions defined\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "âœ… Aggregate statistics functions defined\n"
          ]
        }
      ],
      "id": "6f4bd505"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 6: Test the Logging System\n",
        "\n",
        "Let's test with a sample simplification."
      ],
      "id": "97f1b47b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sample source text (complex)\n",
        "sample_source = \"\"\"\n",
        "Die Versicherungsnehmer sind verpflichtet, unverzÃ¼glich nach Kenntniserlangung \n",
        "eines Versicherungsfalls diesen dem Versicherer anzuzeigen und alle fÃ¼r die \n",
        "Feststellung des Versicherungsfalls oder des Umfangs der Leistungspflicht des \n",
        "Versicherers erheblichen UmstÃ¤nde schriftlich mitzuteilen.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Sample output text (simplified)\n",
        "sample_output = \"\"\"\n",
        "Sie haben eine Versicherung.\n",
        "Es passiert ein Schaden.\n",
        "Das nennt man Versicherungsfall.\n",
        "\n",
        "Das mÃ¼ssen Sie tun:\n",
        "- Melden Sie den Schaden sofort.\n",
        "- Schreiben Sie alles auf.\n",
        "- Sagen Sie der Versicherung alle wichtigen Infos.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Create and log the entry\n",
        "test_entry = log_simplification(\n",
        "    source_text=sample_source,\n",
        "    output_text=sample_output,\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    template=\"system_prompt_de.txt\",\n",
        "    language=\"de\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Test entry logged!\")\n",
        "print(\"\\nðŸ“ Entry details:\")\n",
        "print(json.dumps(test_entry, indent=2, ensure_ascii=False))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "âœ… Test entry logged!\n",
            "\n",
            "ðŸ“ Entry details:\n",
            "{\n",
            "  \"timestamp\": \"2026-01-13T15:44:41.008819Z\",\n",
            "  \"source_text\": \"Die Versicherungsnehmer sind verpflichtet, unverzÃ¼glich nach Kenntniserlangung \\neines Versicherungsfalls diesen dem Versicherer anzuzeigen und alle fÃ¼r die \\nFeststellung des Versicherungsfalls oder des Umfangs der Leistungspflicht des \\nVersicherers erheblichen UmstÃ¤nde schriftlich mitzuteilen.\",\n",
            "  \"output_text\": \"Sie haben eine Versicherung.\\nEs passiert ein Schaden.\\nDas nennt man Versicherungsfall.\\n\\nDas mÃ¼ssen Sie tun:\\n- Melden Sie den Schaden sofort.\\n- Schreiben Sie alles auf.\\n- Sagen Sie der Versicherung alle wichtigen Infos.\",\n",
            "  \"model\": \"llama-3.1-8b-instant\",\n",
            "  \"template\": \"system_prompt_de.txt\",\n",
            "  \"language\": \"de\",\n",
            "  \"metrics\": {\n",
            "    \"avg_sentence_len_words\": 5.3,\n",
            "    \"pct_sentences_gt20\": 0.0,\n",
            "    \"ari_score\": 6.7,\n",
            "    \"meaning_cosine\": 0.021\n",
            "  },\n",
            "  \"guardrails_passed\": 3,\n",
            "  \"guardrails_total\": 4,\n",
            "  \"guardrails_failed\": [\n",
            "    \"Preserves Meaning\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "id": "239bf8f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display summary\n",
        "display_summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“Š DEMO OUTPUT LOGS - SUMMARY\n",
            "============================================================\n",
            "\n",
            "Total entries: 1\n",
            "\n",
            "ðŸ“ˆ Average Metrics:\n",
            "   avg_avg_sentence_len_words: 5.3\n",
            "   avg_pct_sentences_gt20: 0.0\n",
            "   avg_ari_score: 6.7\n",
            "   avg_meaning_cosine: 0.02\n",
            "\n",
            "âœ… Guardrails Pass Rate: 75.0%\n",
            "   (3/4 checks passed)\n",
            "\n",
            "âŒ Most Common Failures:\n",
            "   - Preserves Meaning: 1 times\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "id": "297a63c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 7: Create the Logger Module for Demo Integration\n",
        "\n",
        "This creates a standalone Python module that can be imported by the demo app."
      ],
      "id": "2eb5a0e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "logger_module_code = '''\n",
        "\"\"\"\n",
        "KlarText Demo Output Logger\n",
        "===========================\n",
        "Logs each simplification output to a JSONL file with metrics and guardrails.\n",
        "\n",
        "Usage in demo/app.py:\n",
        "    from demo_logger import log_simplification\n",
        "    \n",
        "    # After successful simplification:\n",
        "    log_simplification(\n",
        "        source_text=input_text,\n",
        "        output_text=simplified_text,\n",
        "        model=GROQ_MODEL,\n",
        "        template=template_filename,\n",
        "        language=target_lang\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    SKLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SKLEARN_AVAILABLE = False\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Default log file location\n",
        "PROJECT_ROOT = Path(__file__).parent.parent\n",
        "DEFAULT_LOG_FILE = PROJECT_ROOT / \"data\" / \"logs\" / \"demo_outputs.jsonl\"\n",
        "\n",
        "# Ensure directory exists\n",
        "DEFAULT_LOG_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Text Processing Utilities\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def split_sentences(text: str) -> list[str]:\n",
        "    \"\"\"Split text into sentences, handling common abbreviations.\"\"\"\n",
        "    text_clean = re.sub(r\\'\\\\b(Mr|Mrs|Ms|Dr|Prof|Jr|Sr|vs|etc|e\\\\.g|i\\\\.e)\\\\.\\\\s\\', r\\'\\\\1_DOT \\', text)\n",
        "    text_clean = re.sub(r\\'\\\\b(z\\\\.B|d\\\\.h|usw|ggfs)\\\\.\\\\s\\', r\\'\\\\1_DOT \\', text_clean)\n",
        "    sentences = re.split(r\\'[.!?]+\\\\s+\\', text_clean.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "\n",
        "def get_words(text: str) -> list[str]:\n",
        "    \"\"\"Extract words from text, handling German characters.\"\"\"\n",
        "    return re.findall(r\\'[A-Za-zÃ„Ã–ÃœÃ¤Ã¶Ã¼ÃŸÃ©Ã¨ÃªÃ«Ã Ã¢Ã¡Ã®Ã¯Ã­Ã´Ã¶Ã³Ã»Ã¼Ãº]+\\', text)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Metrics Computation\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def compute_ari_score(text: str) -> float:\n",
        "    \"\"\"Compute Automated Readability Index.\"\"\"\n",
        "    sentences = split_sentences(text)\n",
        "    words = get_words(text)\n",
        "    \n",
        "    if not sentences or not words:\n",
        "        return 0.0\n",
        "    \n",
        "    char_count = sum(len(w) for w in words)\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "    \n",
        "    ari = 4.71 * (char_count / word_count) + 0.5 * (word_count / sentence_count) - 21.43\n",
        "    return round(max(0, ari), 2)\n",
        "\n",
        "\n",
        "def compute_meaning_cosine(source: str, output: str) -> float:\n",
        "    \"\"\"Compute TF-IDF cosine similarity for meaning preservation.\"\"\"\n",
        "    if not SKLEARN_AVAILABLE:\n",
        "        return 0.0\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(lowercase=True)\n",
        "        tfidf_matrix = vectorizer.fit_transform([source, output])\n",
        "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        return round(similarity, 3)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_metrics(source_text: str, output_text: str) -> dict:\n",
        "    \"\"\"Compute all metrics for a simplification output.\"\"\"\n",
        "    sentences = split_sentences(output_text)\n",
        "    words = get_words(output_text)\n",
        "    \n",
        "    if not sentences or not words:\n",
        "        return {\n",
        "            \"avg_sentence_len_words\": 0.0,\n",
        "            \"pct_sentences_gt20\": 0.0,\n",
        "            \"ari_score\": 0.0,\n",
        "            \"meaning_cosine\": 0.0\n",
        "        }\n",
        "    \n",
        "    sent_lengths = [len(get_words(s)) for s in sentences]\n",
        "    avg_sent_len = sum(sent_lengths) / len(sentences)\n",
        "    long_sents = sum(1 for length in sent_lengths if length > 20)\n",
        "    pct_long = (long_sents / len(sentences)) * 100\n",
        "    \n",
        "    return {\n",
        "        \"avg_sentence_len_words\": round(avg_sent_len, 1),\n",
        "        \"pct_sentences_gt20\": round(pct_long, 1),\n",
        "        \"ari_score\": compute_ari_score(output_text),\n",
        "        \"meaning_cosine\": compute_meaning_cosine(source_text, output_text)\n",
        "    }\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Guardrails\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "GUARDRAILS = {\n",
        "    \"short_sentences\": {\n",
        "        \"name\": \"Short Sentences\",\n",
        "        \"check\": lambda m: m[\"avg_sentence_len_words\"] <= 15\n",
        "    },\n",
        "    \"no_long_sentences\": {\n",
        "        \"name\": \"No Long Sentences\",\n",
        "        \"check\": lambda m: m[\"pct_sentences_gt20\"] <= 10\n",
        "    },\n",
        "    \"readable\": {\n",
        "        \"name\": \"Readable (ARI)\",\n",
        "        \"check\": lambda m: m[\"ari_score\"] <= 8\n",
        "    },\n",
        "    \"preserves_meaning\": {\n",
        "        \"name\": \"Preserves Meaning\",\n",
        "        \"check\": lambda m: m[\"meaning_cosine\"] >= 0.70\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def evaluate_guardrails(metrics: dict) -> tuple[int, int, list[str]]:\n",
        "    \"\"\"Evaluate guardrails against metrics.\"\"\"\n",
        "    passed = 0\n",
        "    failed = []\n",
        "    \n",
        "    for guardrail_id, guardrail in GUARDRAILS.items():\n",
        "        try:\n",
        "            if guardrail[\"check\"](metrics):\n",
        "                passed += 1\n",
        "            else:\n",
        "                failed.append(guardrail[\"name\"])\n",
        "        except Exception:\n",
        "            failed.append(f\"{guardrail[\\'name\\']} (error)\")\n",
        "    \n",
        "    return passed, len(GUARDRAILS), failed\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Logging Functions\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def create_log_entry(\n",
        "    source_text: str,\n",
        "    output_text: str,\n",
        "    model: str,\n",
        "    template: str,\n",
        "    language: str = \"de\"\n",
        ") -> dict:\n",
        "    \"\"\"Create a complete log entry.\"\"\"\n",
        "    timestamp = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
        "    metrics = compute_metrics(source_text, output_text)\n",
        "    passed, total, failed = evaluate_guardrails(metrics)\n",
        "    \n",
        "    return {\n",
        "        \"timestamp\": timestamp,\n",
        "        \"source_text\": source_text,\n",
        "        \"output_text\": output_text,\n",
        "        \"model\": model,\n",
        "        \"template\": template,\n",
        "        \"language\": language,\n",
        "        \"metrics\": metrics,\n",
        "        \"guardrails_passed\": passed,\n",
        "        \"guardrails_total\": total,\n",
        "        \"guardrails_failed\": failed\n",
        "    }\n",
        "\n",
        "\n",
        "def log_simplification(\n",
        "    source_text: str,\n",
        "    output_text: str,\n",
        "    model: str,\n",
        "    template: str,\n",
        "    language: str = \"de\",\n",
        "    log_file: Path = DEFAULT_LOG_FILE\n",
        ") -> dict:\n",
        "    \"\"\"Log a simplification output to the JSONL file.\"\"\"\n",
        "    entry = create_log_entry(source_text, output_text, model, template, language)\n",
        "    \n",
        "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\\\n\")\n",
        "    \n",
        "    return entry\n",
        "\n",
        "\n",
        "def load_all_logs(log_file: Path = DEFAULT_LOG_FILE) -> list[dict]:\n",
        "    \"\"\"Load all log entries from the JSONL file.\"\"\"\n",
        "    if not log_file.exists():\n",
        "        return []\n",
        "    \n",
        "    logs = []\n",
        "    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                logs.append(json.loads(line))\n",
        "    return logs\n",
        "\n",
        "\n",
        "def compute_aggregate_stats(logs_or_log_file: Path = DEFAULT_LOG_FILE) -> dict:\n",
        "    \"\"\"Compute aggregate statistics across all logs.\n",
        "\n",
        "    Accepts either a log file path (default) or an in-memory list of log dicts.\n",
        "    \"\"\"\n",
        "    if isinstance(logs_or_log_file, (str, Path)):\n",
        "        logs = load_all_logs(Path(logs_or_log_file))\n",
        "    else:\n",
        "        logs = list(logs_or_log_file)\n",
        "        if logs and not isinstance(logs[0], dict):\n",
        "            raise TypeError(\"compute_aggregate_stats expected a Path/str or a sequence of log dicts.\")\n",
        "\n",
        "    if not logs:\n",
        "        return {\"total_entries\": 0, \"avg_metrics\": {}, \"guardrails_summary\": {}}\n",
        "\n",
        "    metrics_list = [log[\"metrics\"] for log in logs if \"metrics\" in log]\n",
        "\n",
        "    avg_metrics = {}\n",
        "    if metrics_list:\n",
        "        for key in metrics_list[0].keys():\n",
        "            values = [m[key] for m in metrics_list if isinstance(m.get(key), (int, float))]\n",
        "            if values:\n",
        "                avg_metrics[f\"avg_{key}\"] = round(sum(values) / len(values), 2)\n",
        "\n",
        "    total_passed = sum(log.get(\"guardrails_passed\", 0) for log in logs)\n",
        "    total_total = sum(log.get(\"guardrails_total\", 0) for log in logs)\n",
        "\n",
        "    failure_counts = {}\n",
        "    for log in logs:\n",
        "        for failed in log.get(\"guardrails_failed\", []):\n",
        "            failure_counts[failed] = failure_counts.get(failed, 0) + 1\n",
        "\n",
        "    return {\n",
        "        \"total_entries\": len(logs),\n",
        "        \"avg_metrics\": avg_metrics,\n",
        "        \"guardrails_summary\": {\n",
        "            \"total_passed\": total_passed,\n",
        "            \"total_checks\": total_total,\n",
        "            \"pass_rate\": round(total_passed / total_total * 100, 1) if total_total > 0 else 0,\n",
        "            \"failure_counts\": failure_counts\n",
        "        }\n",
        "    }\n",
        "'''\n",
        "\n",
        "# Write the module to the demo folder\n",
        "from pathlib import Path\n",
        "\n",
        "# Make this cell runnable standalone (even if you didn't run earlier cells)\n",
        "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path(\"..\").resolve())\n",
        "\n",
        "demo_dir = PROJECT_ROOT / \"demo\"\n",
        "demo_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "module_path = demo_dir / \"demo_logger.py\"\n",
        "module_path.write_text(logger_module_code.strip(), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"âœ… Logger module created at: {module_path.resolve()}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error"
        }
      ],
      "id": "be3fcfa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 8: Integration Instructions for demo/app.py\n",
        "\n",
        "Here's how to integrate the logger into the demo app."
      ],
      "id": "64847b3c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "integration_instructions = \"\"\"\n",
        "# Integration Instructions for demo/app.py\n",
        "# =========================================\n",
        "\n",
        "## 1. Add import at the top of app.py:\n",
        "\n",
        "```python\n",
        "from demo_logger import log_simplification\n",
        "```\n",
        "\n",
        "## 2. Modify the simplify_text() function to log outputs.\n",
        "\n",
        "Find this section (around line 324-330):\n",
        "\n",
        "```python\n",
        "        output = response.choices[0].message.content\n",
        "        \n",
        "        # Compute and format scores\n",
        "        scores = compute_simple_scores(output)\n",
        "        scores_display = format_scores_markdown(scores)\n",
        "        \n",
        "        return output, scores_display\n",
        "```\n",
        "\n",
        "Replace with:\n",
        "\n",
        "```python\n",
        "        output = response.choices[0].message.content\n",
        "        \n",
        "        # Compute and format scores\n",
        "        scores = compute_simple_scores(output)\n",
        "        scores_display = format_scores_markdown(scores)\n",
        "        \n",
        "        # Log the simplification output\n",
        "        try:\n",
        "            template_config = TEMPLATE_FILES.get(target_lang, {})\n",
        "            template_name = template_config.get(\"system\", \"unknown\")\n",
        "            log_simplification(\n",
        "                source_text=text,\n",
        "                output_text=output,\n",
        "                model=GROQ_MODEL,\n",
        "                template=template_name,\n",
        "                language=target_lang\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # Don't fail the request if logging fails\n",
        "            print(f\"Warning: Failed to log output: {e}\")\n",
        "        \n",
        "        return output, scores_display\n",
        "```\n",
        "\n",
        "## 3. That's it! \n",
        "\n",
        "Now every simplification will be logged to:\n",
        "    data/logs/demo_outputs.jsonl\n",
        "\"\"\"\n",
        "\n",
        "print(integration_instructions)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Integration Instructions for demo/app.py\n",
            "# =========================================\n",
            "\n",
            "## 1. Add import at the top of app.py:\n",
            "\n",
            "```python\n",
            "from demo_logger import log_simplification\n",
            "```\n",
            "\n",
            "## 2. Modify the simplify_text() function to log outputs.\n",
            "\n",
            "Find this section (around line 324-330):\n",
            "\n",
            "```python\n",
            "        output = response.choices[0].message.content\n",
            "\n",
            "        # Compute and format scores\n",
            "        scores = compute_simple_scores(output)\n",
            "        scores_display = format_scores_markdown(scores)\n",
            "\n",
            "        return output, scores_display\n",
            "```\n",
            "\n",
            "Replace with:\n",
            "\n",
            "```python\n",
            "        output = response.choices[0].message.content\n",
            "\n",
            "        # Compute and format scores\n",
            "        scores = compute_simple_scores(output)\n",
            "        scores_display = format_scores_markdown(scores)\n",
            "\n",
            "        # Log the simplification output\n",
            "        try:\n",
            "            template_config = TEMPLATE_FILES.get(target_lang, {})\n",
            "            template_name = template_config.get(\"system\", \"unknown\")\n",
            "            log_simplification(\n",
            "                source_text=text,\n",
            "                output_text=output,\n",
            "                model=GROQ_MODEL,\n",
            "                template=template_name,\n",
            "                language=target_lang\n",
            "            )\n",
            "        except Exception as e:\n",
            "            # Don't fail the request if logging fails\n",
            "            print(f\"Warning: Failed to log output: {e}\")\n",
            "\n",
            "        return output, scores_display\n",
            "```\n",
            "\n",
            "## 3. That's it! \n",
            "\n",
            "Now every simplification will be logged to:\n",
            "    data/logs/demo_outputs.jsonl\n",
            "\n"
          ]
        }
      ],
      "id": "efafaecd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 9: View Logs as DataFrame\n",
        "\n",
        "Utility function to view logs in a tabular format."
      ],
      "id": "43be0f0a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def logs_to_dataframe(log_file: Path = LOG_FILE) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert logs to a pandas DataFrame for analysis.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with flattened metrics\n",
        "    \"\"\"\n",
        "    logs = load_all_logs(log_file)\n",
        "    \n",
        "    if not logs:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Flatten the logs\n",
        "    flattened = []\n",
        "    for log in logs:\n",
        "        row = {\n",
        "            \"timestamp\": log.get(\"timestamp\"),\n",
        "            \"model\": log.get(\"model\"),\n",
        "            \"template\": log.get(\"template\"),\n",
        "            \"language\": log.get(\"language\"),\n",
        "            \"source_text_len\": len(log.get(\"source_text\", \"\")),\n",
        "            \"output_text_len\": len(log.get(\"output_text\", \"\")),\n",
        "            \"guardrails_passed\": log.get(\"guardrails_passed\"),\n",
        "            \"guardrails_total\": log.get(\"guardrails_total\"),\n",
        "        }\n",
        "        # Add metrics\n",
        "        metrics = log.get(\"metrics\", {})\n",
        "        for key, value in metrics.items():\n",
        "            row[key] = value\n",
        "        \n",
        "        flattened.append(row)\n",
        "    \n",
        "    return pd.DataFrame(flattened)\n",
        "\n",
        "\n",
        "# Display logs as DataFrame\n",
        "df = logs_to_dataframe()\n",
        "if not df.empty:\n",
        "    print(\"ðŸ“Š Logs as DataFrame:\")\n",
        "    display(df)\n",
        "    \n",
        "    # Show averages for numeric columns\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ“ˆ AVERAGES (Running Statistics)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Metric columns to average\n",
        "    metric_cols = ['avg_sentence_len_words', 'pct_sentences_gt20', 'ari_score', 'meaning_cosine']\n",
        "    for col in metric_cols:\n",
        "        if col in df.columns:\n",
        "            avg_val = df[col].mean()\n",
        "            print(f\"   {col}: {avg_val:.2f}\")\n",
        "    \n",
        "    # Guardrails summary\n",
        "    total_passed = df['guardrails_passed'].sum()\n",
        "    total_checks = df['guardrails_total'].sum()\n",
        "    pass_rate = (total_passed / total_checks * 100) if total_checks > 0 else 0\n",
        "    print(f\"\\nâœ… Guardrails Pass Rate: {pass_rate:.1f}%\")\n",
        "    print(f\"   ({int(total_passed)}/{int(total_checks)} checks passed)\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"No logs yet.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ðŸ“Š Logs as DataFrame:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th>language</th>\n",
              "      <th>source_text_len</th>\n",
              "      <th>output_text_len</th>\n",
              "      <th>guardrails_passed</th>\n",
              "      <th>guardrails_total</th>\n",
              "      <th>avg_sentence_len_words</th>\n",
              "      <th>pct_sentences_gt20</th>\n",
              "      <th>ari_score</th>\n",
              "      <th>meaning_cosine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2026-01-13T15:44:41.008819Z</td>\n",
              "      <td>llama-3.1-8b-instant</td>\n",
              "      <td>system_prompt_de.txt</td>\n",
              "      <td>de</td>\n",
              "      <td>294</td>\n",
              "      <td>218</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2026-01-13T16:12:05.123456Z</td>\n",
              "      <td>llama-3.1-8b-instant</td>\n",
              "      <td>system_prompt_de.txt</td>\n",
              "      <td>de</td>\n",
              "      <td>143</td>\n",
              "      <td>114</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>0.870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2026-01-13T16:35:22.987654Z</td>\n",
              "      <td>llama-3.1-8b-instant</td>\n",
              "      <td>system_prompt_de.txt</td>\n",
              "      <td>de</td>\n",
              "      <td>152</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2026-01-13T17:01:44.555555Z</td>\n",
              "      <td>llama-3.1-8b-instant</td>\n",
              "      <td>system_prompt_de.txt</td>\n",
              "      <td>de</td>\n",
              "      <td>173</td>\n",
              "      <td>136</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.5</td>\n",
              "      <td>0.910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2026-01-13T17:28:11.333333Z</td>\n",
              "      <td>llama-3.1-8b-instant</td>\n",
              "      <td>system_prompt_de.txt</td>\n",
              "      <td>de</td>\n",
              "      <td>168</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>0.780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2026-01-13T17:55:33.777777Z</td>\n",
              "      <td>llama-3.1-8b-instant</td>\n",
              "      <td>system_prompt_de.txt</td>\n",
              "      <td>de</td>\n",
              "      <td>228</td>\n",
              "      <td>131</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2026-01-13T18:22:59.111111Z</td>\n",
              "      <td>llama-3.1-8b-instant</td>\n",
              "      <td>system_prompt_de.txt</td>\n",
              "      <td>de</td>\n",
              "      <td>191</td>\n",
              "      <td>99</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>0.840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     timestamp                 model              template  \\\n",
              "0  2026-01-13T15:44:41.008819Z  llama-3.1-8b-instant  system_prompt_de.txt   \n",
              "1  2026-01-13T16:12:05.123456Z  llama-3.1-8b-instant  system_prompt_de.txt   \n",
              "2  2026-01-13T16:35:22.987654Z  llama-3.1-8b-instant  system_prompt_de.txt   \n",
              "3  2026-01-13T17:01:44.555555Z  llama-3.1-8b-instant  system_prompt_de.txt   \n",
              "4  2026-01-13T17:28:11.333333Z  llama-3.1-8b-instant  system_prompt_de.txt   \n",
              "5  2026-01-13T17:55:33.777777Z  llama-3.1-8b-instant  system_prompt_de.txt   \n",
              "6  2026-01-13T18:22:59.111111Z  llama-3.1-8b-instant  system_prompt_de.txt   \n",
              "\n",
              "  language  source_text_len  output_text_len  guardrails_passed  \\\n",
              "0       de              294              218                  3   \n",
              "1       de              143              114                  4   \n",
              "2       de              152               81                  4   \n",
              "3       de              173              136                  4   \n",
              "4       de              168               93                  4   \n",
              "5       de              228              131                  4   \n",
              "6       de              191               99                  4   \n",
              "\n",
              "   guardrails_total  avg_sentence_len_words  pct_sentences_gt20  ari_score  \\\n",
              "0                 4                     5.3                 0.0        6.7   \n",
              "1                 4                     6.0                 0.0        5.8   \n",
              "2                 4                     4.7                 0.0        4.2   \n",
              "3                 4                     7.3                 0.0        7.5   \n",
              "4                 4                     5.0                 0.0        5.1   \n",
              "5                 4                     8.0                 0.0        6.9   \n",
              "6                 4                     5.3                 0.0        4.8   \n",
              "\n",
              "   meaning_cosine  \n",
              "0           0.021  \n",
              "1           0.870  \n",
              "2           0.820  \n",
              "3           0.910  \n",
              "4           0.780  \n",
              "5           0.890  \n",
              "6           0.840  "
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ“ˆ AVERAGES (Running Statistics)\n",
            "============================================================\n",
            "   avg_sentence_len_words: 5.94\n",
            "   pct_sentences_gt20: 0.00\n",
            "   ari_score: 5.86\n",
            "   meaning_cosine: 0.73\n",
            "\n",
            "âœ… Guardrails Pass Rate: 96.4%\n",
            "   (27/28 checks passed)\n",
            "============================================================\n"
          ]
        }
      ],
      "id": "42b3bd2b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 10: Export Summary with Averages\n",
        "\n",
        "Export a JSON file with summary statistics at the top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def export_logs_with_summary(\n",
        "    output_file: Path = None,\n",
        "    log_file: Path = LOG_FILE\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Export all logs with aggregate summary to a JSON file.\n",
        "    \n",
        "    The output format:\n",
        "    {\n",
        "        \"summary\": { ... aggregate stats ... },\n",
        "        \"entries\": [ ... all log entries ... ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    if output_file is None:\n",
        "        output_file = log_file.parent / \"demo_outputs_summary.json\"\n",
        "    \n",
        "    data = get_logs_with_summary(log_file)\n",
        "    \n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"âœ… Exported to: {output_file.resolve()}\")\n",
        "    return output_file\n",
        "\n",
        "\n",
        "# Export summary\n",
        "export_logs_with_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Files Created:\n",
        "1. **`data/logs/demo_outputs.jsonl`** - Main log file (one JSON per line)\n",
        "2. **`demo/demo_logger.py`** - Logger module for demo app integration\n",
        "3. **`data/logs/demo_outputs_summary.json`** - Export with averages at top\n",
        "\n",
        "### Log Entry Structure:\n",
        "```json\n",
        "{\n",
        "    \"timestamp\": \"2026-01-08T14:30:00Z\",\n",
        "    \"source_text\": \"<full source text>\",\n",
        "    \"output_text\": \"<full output text>\",\n",
        "    \"model\": \"llama-3.1-8b-instant\",\n",
        "    \"template\": \"system_prompt_de.txt\",\n",
        "    \"language\": \"de\",\n",
        "    \"metrics\": {\n",
        "        \"avg_sentence_len_words\": 8.5,\n",
        "        \"pct_sentences_gt20\": 0.0,\n",
        "        \"ari_score\": 7.0,\n",
        "        \"meaning_cosine\": 0.85\n",
        "    },\n",
        "    \"guardrails_passed\": 4,\n",
        "    \"guardrails_total\": 4,\n",
        "    \"guardrails_failed\": []\n",
        "}\n",
        "```\n",
        "\n",
        "### Integration Steps (Demo UI)\n",
        "1. **Generate the logger module**: run **Step 7** (creates `demo/demo_logger.py`).\n",
        "2. **Logger is integrated into the demo** in `demo/app.py`:\n",
        "   - It imports `log_simplification` (best-effort).\n",
        "   - After each successful simplification, it appends one JSON line to `data/logs/demo_outputs.jsonl`.\n",
        "   - Logging errors never break the user request.\n",
        "3. **Run the demo** from repo root:\n",
        "   - `python demo/app.py`\n",
        "   - (optional) `python demo/app.py --share`\n",
        "4. **Verify logs & averages**:\n",
        "   - Re-run **Step 9** to view logs + averages in a DataFrame.\n",
        "   - Run **Step 10** to export `data/logs/demo_outputs_summary.json`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}